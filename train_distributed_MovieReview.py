"""
åˆ†å¸ƒå¼è®­ç»ƒè„šæœ¬ - è±†ç“£å½±è¯„æ¨¡åž‹ï¼ˆFlashAttention 2 + åŠ¨æ€Batch Paddingï¼‰

ç”¨äºŽè®­ç»ƒç”¨æˆ·å½±è¯„é£Žæ ¼æ¨¡æ‹Ÿæ¨¡åž‹

ä½¿ç”¨æ–¹æ³•ï¼š
# å•å¡è®­ç»ƒ
python train_distributed_MovieReview.py \
    --data_file movie_review_data.json \
    --output_dir outputs/movie_review_model

# å¤šå¡è®­ç»ƒ
torchrun --nproc_per_node=4 train_distributed_MovieReview.py \
    --data_file movie_review_data.json \
    --output_dir outputs/movie_review_model_4gpu
"""
import json
import argparse
import os
import sys
from pathlib import Path
import random
import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler

# å¯¼å…¥å½±è¯„æ•°æ®åŠ è½½å™¨
from data_loader_movie_review import (
    load_movie_review_data, 
    extract_movie_review_samples,
    split_movie_reviews_by_time,
    format_movie_review_prompt
)

# å¤ç”¨åŠ¨æ€Paddingæ•°æ®é›†
from train_with_dynamic_padding_Lovink import DynamicPaddingDataset, dynamic_padding_collate_fn

from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    EarlyStoppingCallback,
    TrainingArguments,
    Trainer
)
from typing import List, Dict, Any, Optional
import torch.nn as nn


def check_flash_attention_support():
    """æ£€æŸ¥ç³»ç»Ÿæ˜¯å¦æ”¯æŒ FlashAttention 2"""
    try:
        import flash_attn
        flash_version = getattr(flash_attn, '__version__', 'unknown')
        print(f"FlashAttention å·²å®‰è£…ï¼Œç‰ˆæœ¬: {flash_version}")
        return True
    except ImportError:
        print("è­¦å‘Š: FlashAttention æœªå®‰è£…")
        return False


def setup_distributed():
    """åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒçŽ¯å¢ƒ"""
    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:
        rank = int(os.environ["RANK"])
        world_size = int(os.environ['WORLD_SIZE'])
        local_rank = int(os.environ['LOCAL_RANK'])
    elif 'SLURM_PROCID' in os.environ:
        rank = int(os.environ['SLURM_PROCID'])
        world_size = int(os.environ['SLURM_NTASKS'])
        local_rank = rank % torch.cuda.device_count()
    else:
        print('æœªæ£€æµ‹åˆ°åˆ†å¸ƒå¼è®­ç»ƒçŽ¯å¢ƒï¼Œä½¿ç”¨å•å¡è®­ç»ƒ')
        rank = 0
        world_size = 1
        local_rank = 0
    
    torch.cuda.set_device(local_rank)
    
    if world_size > 1:
        dist.init_process_group(
            backend='nccl',
            init_method='env://',
            world_size=world_size,
            rank=rank
        )
    
    return rank, world_size, local_rank


def cleanup_distributed():
    """æ¸…ç†åˆ†å¸ƒå¼è®­ç»ƒçŽ¯å¢ƒ"""
    if dist.is_initialized():
        dist.destroy_process_group()


class MovieReviewDataset(DynamicPaddingDataset):
    """
    å½±è¯„æ•°æ®é›†ï¼ˆç»§æ‰¿è‡ªDynamicPaddingDatasetï¼‰
    """
    
    def format_prompt(self, sample: Dict[str, Any]) -> str:
        """
        æ ¼å¼åŒ–å½±è¯„æ ·æœ¬ä¸ºè®­ç»ƒprompt
        
        è¦†ç›–çˆ¶ç±»æ–¹æ³•ï¼Œä½¿ç”¨å½±è¯„ä¸“ç”¨æ ¼å¼
        """
        parts = []
        
        # 1. ç”¨æˆ·Profile
        if self.use_profile and sample.get('user_profile'):
            profile = sample['user_profile']
            parts.append(f"ç”¨æˆ·: {profile.get('name', 'Unknown')}")
            if sample.get('task_description'):
                parts.append(f"ä»»åŠ¡: {sample['task_description']}")
            parts.append("")
        
        # 2. åŽ†å²å½±è¯„ï¼ˆå¦‚æžœå¯ç”¨ï¼‰
        if self.use_history and sample.get('history'):
            history = sample['history']
            parts.append(f"åŽ†å²å½±è¯„è®°å½• ({len(history)}æ¡):")
            
            # åªä½¿ç”¨æœ€è¿‘çš„Næ¡åŽ†å²
            # max_history = 15
            for h in history:
                parts.append(f"  ç”µå½±ã€Š{h['movie']}ã€‹: {h['review']}")
            
            # if len(history) > max_history:
            #     parts.append(f"  ...ï¼ˆè¿˜æœ‰{len(history) - max_history}æ¡æ›´æ—©çš„è¯„è®ºï¼‰")
            parts.append("")
        
        # 3. å½“å‰ç”µå½±
        movie_name = sample.get('movie_name', '')
        parts.append(f"æ¨¡ä»¿ç”¨æˆ·é£Žæ ¼ä¸ºç”µå½±ã€Š{movie_name}ã€‹å†™ä¸€æ¡å½±è¯„ï¼š")
        
        return "\n".join(parts)


def main():
    parser = argparse.ArgumentParser(description='è±†ç“£å½±è¯„æ¨¡åž‹ - åˆ†å¸ƒå¼è®­ç»ƒ')
    
    # é…ç½®æ–‡ä»¶
    parser.add_argument('--config', type=str,
                       default='config_MovieReview.json',
                       help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--ablation_config', type=str, required=True,
                       choices=['profile_and_history', 'profile_only', 'history_only', 'baseline'],
                       help='æ¶ˆèžå®žéªŒé…ç½®')
    
    # æ•°æ®ç›¸å…³ï¼ˆå¯é€‰ï¼Œè¦†ç›–é…ç½®æ–‡ä»¶ï¼‰
    parser.add_argument('--data_file', type=str, default=None,
                       help='å½±è¯„æ•°æ®JSONæ–‡ä»¶è·¯å¾„ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶ï¼‰')
    parser.add_argument('--val_ratio', type=float, default=None,
                       help='éªŒè¯é›†æ¯”ä¾‹ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶ï¼‰')
    
    # è¾“å‡ºç›®å½•
    parser.add_argument('--output_dir', type=str, default=None,
                       help='æ¨¡åž‹è¾“å‡ºç›®å½•')
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--max_epochs', type=int, default=50,
                       help='æœ€å¤§è®­ç»ƒè½®æ¬¡ï¼ˆé»˜è®¤50ï¼‰')
    parser.add_argument('--early_stopping_patience', type=int, default=3,
                       help='æ—©åœè€å¿ƒå€¼ï¼ˆé»˜è®¤3ï¼‰')
    parser.add_argument('--early_stopping_threshold', type=float, default=0.001,
                       help='æ—©åœé˜ˆå€¼ï¼ˆé»˜è®¤0.001ï¼‰')
    
    # DeepSpeedå’Œå…¶ä»–
    parser.add_argument('--deepspeed', type=str, default=None,
                       help='DeepSpeedé…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--disable_flash_attn', action='store_true',
                       help='ç¦ç”¨FlashAttention 2')
    parser.add_argument('--wandb_project', type=str, default='MovieReview',
                       help='Weights & Biasesé¡¹ç›®åç§°')
    parser.add_argument('--wandb_run_name', type=str, default=None,
                       help='Weights & Biasesè¿è¡Œåç§°')
    parser.add_argument('--local_rank', type=int, default=-1,
                       help='æœ¬åœ°è¿›ç¨‹rank')
    parser.add_argument('--prompt_style', type=str, default='simple',
                       choices=['simple', 'detailed'],
                       help='Prompté£Žæ ¼ï¼šsimple=ç®€æ´ï¼Œdetailed=è¯¦ç»†')
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®æ–‡ä»¶
    if os.path.isabs(args.config):
        config_path = args.config
    else:
        local_config = Path(__file__).parent / args.config
        if local_config.exists():
            config_path = str(local_config)
        else:
            config_path = args.config
    
    with open(config_path, 'r', encoding='utf-8') as f:
        config = json.load(f)
    
    # èŽ·å–æ¶ˆèžé…ç½®
    ablation_config = config['ablation_configs'][args.ablation_config]
    use_profile = ablation_config.get('use_profile', True)
    use_history = ablation_config.get('use_history', True)
    config_name = ablation_config['name']
    
    # åˆå§‹åŒ–åˆ†å¸ƒå¼çŽ¯å¢ƒ
    rank, world_size, local_rank = setup_distributed()
    is_main_process = (rank == 0)
    
    # é…ç½®W&B
    if args.wandb_project and is_main_process:
        try:
            import wandb
            os.environ['WANDB_PROJECT'] = args.wandb_project
            if args.wandb_run_name:
                os.environ['WANDB_NAME'] = args.wandb_run_name
            print(f"âœ“ å·²å¯ç”¨ W&B ç›‘æŽ§: {args.wandb_project}")
        except ImportError:
            print("è­¦å‘Š: wandb æœªå®‰è£…")
            args.wandb_project = None
    
    if is_main_process:
        print("=" * 80)
        print("è±†ç“£å½±è¯„æ¨¡åž‹ - åˆ†å¸ƒå¼è®­ç»ƒ")
        print("=" * 80)
        print(f"World Size: {world_size}")
        print(f"Rank: {rank}")
        print(f"Local Rank: {local_rank}")
        print(f"æ¶ˆèžå®žéªŒ: {config_name}")
        print(f"ä½¿ç”¨é…ç½®:")
        print(f"  Profile: {use_profile}")
        print(f"  History: {use_history}")
        print(f"  Prompt Style: {args.prompt_style}")
        if args.deepspeed:
            print(f"  DeepSpeed: {args.deepspeed}")
        print("=" * 80)
    
    # æ£€æŸ¥FlashAttentionï¼ˆæ‰€æœ‰è¿›ç¨‹ç‹¬ç«‹æ£€æŸ¥ï¼Œé¿å…CUDA broadcasté—®é¢˜ï¼‰
    use_flash_attn = False
    if not args.disable_flash_attn:
        # æ‰€æœ‰è¿›ç¨‹éƒ½æ£€æŸ¥FlashAttentionæ”¯æŒ
        try:
            import flash_attn
            use_flash_attn = True
            if is_main_process:
                flash_version = getattr(flash_attn, '__version__', 'unknown')
                print(f"FlashAttention å·²å®‰è£…ï¼Œç‰ˆæœ¬: {flash_version}")
        except ImportError:
            if is_main_process:
                print("FlashAttention æœªå®‰è£…ï¼Œä½¿ç”¨æ ‡å‡†attention")
    
    # éªŒè¯GPUæ˜¯å¦å¯ç”¨
    if not torch.cuda.is_available():
        print(f"[Rank {rank}] é”™è¯¯: CUDA ä¸å¯ç”¨")
        cleanup_distributed()
        return
    
    if is_main_process:
        print(f"CUDA å¯ç”¨ï¼ŒGPUæ•°é‡: {torch.cuda.device_count()}")
        print(f"å½“å‰GPU: {local_rank} - {torch.cuda.get_device_name(local_rank)}")
        compute_cap = torch.cuda.get_device_capability(local_rank)
        print(f"è®¡ç®—èƒ½åŠ›: {compute_cap[0]}.{compute_cap[1]}")
        print(f"FlashAttention 2: {'å¯ç”¨' if use_flash_attn else 'ç¦ç”¨'}")
    
    # åŠ è½½æ•°æ®
    if is_main_process:
        print("\n" + "=" * 80)
        print("åŠ è½½å½±è¯„æ•°æ®...")
    
    # æ•°æ®è·¯å¾„ï¼ˆä¼˜å…ˆä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°ï¼Œå¦åˆ™ä½¿ç”¨é…ç½®æ–‡ä»¶ï¼‰
    data_file = args.data_file if args.data_file else config['data']['train_path']
    if not os.path.isabs(data_file):
        data_file = str(Path(__file__).parent / data_file)
    
    raw_data = load_movie_review_data(data_file)
    all_samples = extract_movie_review_samples(raw_data, debug=is_main_process)
    
    if is_main_process:
        print(f"æ•°æ®æ–‡ä»¶: {data_file}")
        print(f"æå–äº† {len(all_samples)} ä¸ªæ ·æœ¬")
    
    # èŽ·å–æ•°æ®åˆ’åˆ†æ¯”ä¾‹
    data_split = config.get('data_split', {})
    train_ratio = data_split.get('train_ratio', 0.7)
    val_ratio_config = data_split.get('val_ratio', 0.15)
    test_ratio = data_split.get('test_ratio', 0.15)
    
    # å¦‚æžœæŒ‡å®šäº†val_ratioï¼Œéœ€è¦é‡æ–°è®¡ç®—test_ratio
    if args.val_ratio is not None:
        val_ratio_config = args.val_ratio
        test_ratio = 1.0 - train_ratio - val_ratio_config
    
    # æŒ‰æ—¶é—´é¡ºåºåˆ’åˆ†æ•°æ®é›†
    train_samples, val_samples, test_samples = split_movie_reviews_by_time(
        all_samples,
        train_ratio=train_ratio,
        val_ratio=val_ratio_config,
        test_ratio=test_ratio,
        debug=is_main_process
    )
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    if args.output_dir:
        output_dir = args.output_dir
    else:
        checkpoint_dir = config['model']['checkpoint_dir']
        flash_suffix = "flashattn2" if use_flash_attn else "standard"
        output_dir = os.path.join(checkpoint_dir, f"MovieReview_{config_name}_{flash_suffix}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    if is_main_process:
        os.makedirs(output_dir, exist_ok=True)
        print(f"è¾“å‡ºç›®å½•: {output_dir}")
        
        # ä¿å­˜æµ‹è¯•é›†ï¼ˆç”¨äºŽåŽç»­è¯„ä¼°ï¼‰
        test_file = os.path.join(output_dir, 'test_samples.json')
        with open(test_file, 'w', encoding='utf-8') as f:
            json.dump(test_samples, f, ensure_ascii=False, indent=2)
        print(f"æµ‹è¯•é›†å·²ä¿å­˜åˆ°: {test_file}")
    
    if world_size > 1:
        dist.barrier()
    
    # åŠ è½½tokenizerå’Œæ¨¡åž‹
    model_path = config['model']['path']
    if is_main_process:
        print(f"\nåŠ è½½æ¨¡åž‹: {model_path}")
    
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # åŠ è½½æ¨¡åž‹
    model_kwargs = {
        'torch_dtype': torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,
        'trust_remote_code': True,
    }
    
    if use_flash_attn:
        model_kwargs['attn_implementation'] = 'flash_attention_2'
    
    try:
        model = AutoModelForCausalLM.from_pretrained(model_path, **model_kwargs)
        if is_main_process:
            print(f"âœ“ æ¨¡åž‹å·²åŠ è½½ ({'FlashAttention 2' if use_flash_attn else 'æ ‡å‡†Attention'})")
    except Exception as e:
        if is_main_process:
            print(f"åŠ è½½å¤±è´¥: {e}")
            print("å›žé€€åˆ°æ ‡å‡†attention...")
        model_kwargs.pop('attn_implementation', None)
        model = AutoModelForCausalLM.from_pretrained(model_path, **model_kwargs)
        use_flash_attn = False
    
    # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
    if hasattr(model, 'gradient_checkpointing_enable'):
        model.gradient_checkpointing_enable()
        if is_main_process:
            print("âœ“ æ¢¯åº¦æ£€æŸ¥ç‚¹å·²å¯ç”¨")
    
    model = model.to(local_rank)
    
    # åˆ›å»ºæ•°æ®é›†
    train_config = config.get('training', {})
    if is_main_process:
        print("\nåˆ›å»ºè®­ç»ƒæ•°æ®é›†...")
    
    train_dataset = MovieReviewDataset(
        samples=train_samples,
        tokenizer=tokenizer,
        max_length=train_config.get('max_length', 4096),
        use_profile=use_profile,
        use_history=use_history,
        use_context=False,  # å½±è¯„æ•°æ®ä¸ä½¿ç”¨context
        verbose=is_main_process
    )
    
    val_dataset = None
    if val_samples:
        val_dataset = MovieReviewDataset(
            samples=val_samples,
            tokenizer=tokenizer,
            max_length=train_config.get('max_length', 4096),
            use_profile=use_profile,
            use_history=use_history,
            use_context=False,
            verbose=False
        )
    
    # æ•°æ®æ•´ç†å™¨
    def collate_fn(examples):
        return dynamic_padding_collate_fn(examples, tokenizer)
    
    # æ‰“å°æ ·æœ¬ç¤ºä¾‹
    if is_main_process:
        print("\n" + "=" * 80)
        print("ðŸ“ è®­ç»ƒæ ·æœ¬ç¤ºä¾‹ï¼ˆå‰3ä¸ªï¼‰")
        print("=" * 80)
        
        sample_log_file = os.path.join(output_dir, "training_samples_preview.txt")
        with open(sample_log_file, 'w', encoding='utf-8') as log_file:
            for i in range(min(3, len(train_samples))):
                sample = train_samples[i]
                
                print(f"\n--- æ ·æœ¬ {i+1} ---")
                log_file.write(f"\n{'='*80}\næ ·æœ¬ {i+1}\n{'='*80}\n\n")
                
                # ç”µå½±ä¿¡æ¯
                movie_name = sample.get('movie_name', 'N/A')
                timestamp = sample.get('timestamp', 'N/A')
                print(f"ç”µå½±: {movie_name}")
                print(f"æ—¶é—´: {timestamp}")
                log_file.write(f"ç”µå½±: {movie_name}\n")
                log_file.write(f"æ—¶é—´: {timestamp}\n\n")
                
                # åŽ†å²å½±è¯„æ•°é‡
                history_count = len(sample.get('history', []))
                print(f"åŽ†å²å½±è¯„: {history_count}æ¡")
                log_file.write(f"åŽ†å²å½±è¯„: {history_count}æ¡\n")
                
                if history_count > 0:
                    log_file.write("æœ€è¿‘3æ¡:\n")
                    for h in sample['history'][-3:]:
                        log_file.write(f"  - {h['movie']}: {h['review']}\n")
                log_file.write("\n")
                
                # ç›®æ ‡å½±è¯„
                target = sample.get('next_question', '')
                print(f"ç›®æ ‡å½±è¯„: {target[:80]}...")
                log_file.write(f"ç›®æ ‡å½±è¯„:\n{target}\n\n")
                
                # ç¼–ç ä¿¡æ¯
                encoded = train_dataset[i]
                input_len = len(encoded['input_ids'])
                valid_labels = (encoded['labels'] != -100).sum().item()
                print(f"ç¼–ç é•¿åº¦: {input_len} tokens, æœ‰æ•ˆæ ‡ç­¾: {valid_labels}")
                log_file.write(f"ç¼–ç é•¿åº¦: {input_len} tokens\n")
                log_file.write(f"æœ‰æ•ˆæ ‡ç­¾: {valid_labels} tokens\n")
        
        print(f"\nâœ“ æ ·æœ¬è¯¦æƒ…å·²ä¿å­˜åˆ°: {sample_log_file}")
        print("=" * 80)
    
    # è®¡ç®—è®­ç»ƒæ­¥æ•°
    batch_size = train_config.get('batch_size', 2)
    gradient_accumulation_steps = train_config.get('gradient_accumulation_steps', 8)
    steps_per_epoch = len(train_dataset) // (world_size * batch_size * gradient_accumulation_steps)
    eval_steps = max(1, steps_per_epoch // 2) if val_dataset else None
    save_steps = train_config.get('save_steps', 500)
    
    # è°ƒæ•´save_stepsä¸ºeval_stepsçš„æ•´æ•°å€
    if val_dataset and eval_steps and save_steps % eval_steps != 0:
        save_steps = ((save_steps + eval_steps - 1) // eval_steps) * eval_steps
        if is_main_process:
            print(f"è°ƒæ•´ save_steps ä¸º {save_steps}ï¼ˆeval_steps={eval_steps} çš„æ•´æ•°å€ï¼‰")
    
    # è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=args.max_epochs,
        per_device_train_batch_size=batch_size,
        per_device_eval_batch_size=train_config.get('eval_batch_size', 2),
        gradient_accumulation_steps=gradient_accumulation_steps,
        learning_rate=train_config.get('learning_rate', 1e-5),
        weight_decay=train_config.get('weight_decay', 0.01),
        warmup_steps=train_config.get('warmup_steps', 100),
        logging_steps=train_config.get('logging_steps', 10),
        save_steps=save_steps,
        eval_steps=eval_steps,
        eval_strategy="steps" if val_dataset else "no",
        save_total_limit=train_config.get('save_total_limit', 3),
        load_best_model_at_end=True if val_dataset else False,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        fp16=False,
        bf16=True,
        dataloader_pin_memory=False,
        gradient_checkpointing=True,
        optim="adamw_torch",
        max_grad_norm=0.5,
        report_to="wandb" if args.wandb_project else "none",
        local_rank=local_rank,
        ddp_find_unused_parameters=False,
        ddp_backend="nccl",
        dataloader_num_workers=2,
        save_on_each_node=False,
        logging_first_step=True,
        deepspeed=args.deepspeed,
    )
    
    # æ—©åœå›žè°ƒ
    callbacks = []
    if val_dataset:
        early_stopping = EarlyStoppingCallback(
            early_stopping_patience=args.early_stopping_patience,
            early_stopping_threshold=args.early_stopping_threshold
        )
        callbacks.append(early_stopping)
    
    # åˆ›å»ºTrainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        data_collator=collate_fn,
        processing_class=tokenizer,
        callbacks=callbacks,
    )
    
    # å¼€å§‹è®­ç»ƒ
    if is_main_process:
        print("\n" + "=" * 80)
        print("å¼€å§‹è®­ç»ƒ")
        print("=" * 80)
        print(f"è®­ç»ƒæ ·æœ¬: {len(train_dataset)}")
        print(f"éªŒè¯æ ·æœ¬: {len(val_dataset) if val_dataset else 0}")
        print(f"æµ‹è¯•æ ·æœ¬: {len(test_samples)}")
        print(f"æ¯ä¸ªGPU batch size: {batch_size}")
        print(f"æ¢¯åº¦ç´¯ç§¯: {gradient_accumulation_steps}")
        print(f"æœ‰æ•ˆbatch size: {batch_size * gradient_accumulation_steps * world_size}")
        print(f"é¢„è®¡æ¯epochæ­¥æ•°: {steps_per_epoch}")
        print(f"Max Length: {train_config.get('max_length', 4096)}")
        print(f"Learning Rate: {train_config.get('learning_rate', 1e-5)}")
        if args.deepspeed:
            print(f"DeepSpeed: {args.deepspeed}")
        print("=" * 80)
    
    trainer.train()
    
    # ä¿å­˜æ¨¡åž‹
    if is_main_process:
        print(f"\nä¿å­˜æ¨¡åž‹åˆ° {output_dir}")
        trainer.save_model()
        tokenizer.save_pretrained(output_dir)
        
        # ä¿å­˜é…ç½®
        config_info = {
            'data_file': data_file,
            'ablation_config': args.ablation_config,
            'config_name': config_name,
            'use_profile': use_profile,
            'use_history': use_history,
            'flash_attention_2': use_flash_attn,
            'max_length': train_config.get('max_length', 4096),
            'train_samples': len(train_samples),
            'val_samples': len(val_samples),
            'test_samples': len(test_samples),
            'prompt_style': args.prompt_style,
        }
        with open(os.path.join(output_dir, 'training_config.json'), 'w', encoding='utf-8') as f:
            json.dump(config_info, f, indent=2, ensure_ascii=False)
        
        print("âœ“ è®­ç»ƒå®Œæˆï¼")
        print(f"âœ“ æ¨¡åž‹å·²ä¿å­˜åˆ°: {output_dir}")
    
    # ç­‰å¾…æ‰€æœ‰è¿›ç¨‹å®Œæˆ
    if world_size > 1:
        dist.barrier()
    
    cleanup_distributed()


if __name__ == '__main__':
    main()
