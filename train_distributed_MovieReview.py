"""
åˆ†å¸ƒå¼è®­ç»ƒè„šæœ¬ - è±†ç“£å½±è¯„æ¨¡å‹ï¼ˆFlashAttention 2 + åŠ¨æ€Batch Paddingï¼‰
è‡ªåŒ…å«ç‰ˆæœ¬ - æ— éœ€å¤–éƒ¨ä¾èµ–

ç”¨äºè®­ç»ƒç”¨æˆ·å½±è¯„é£æ ¼æ¨¡æ‹Ÿæ¨¡å‹

ä½¿ç”¨æ–¹æ³•:
torchrun \
    --nproc_per_node=8 \
    --master_port=29505 \
    train_distributed_MovieReview.py \
    --config config_DMSC_30B.json \
    --deepspeed ds_config_zero3_optimized.json \
    --ablation_config profile_and_history \
    --output_dir outputs/DMSC_one_per_user_0213 \
    --max_epochs 50 \
    --val_ratio 0.1 \
    --wandb_project Qwen3_30B-DMSC \
    --wandb_run_name one_per_user_0213 \
    --prompt_style simple \
    --one_sample_per_user
"""

import json
import argparse
import os
import sys
from pathlib import Path
import random
import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler
from torch.utils.data import Dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    EarlyStoppingCallback,
    TrainingArguments,
    Trainer
)
from typing import List, Dict, Any, Optional, Tuple
import torch.nn as nn
from datetime import datetime

# LoRA æ”¯æŒ
try:
    from peft import LoraConfig, get_peft_model, TaskType
    PEFT_AVAILABLE = True
except ImportError:
    PEFT_AVAILABLE = False


# ================================
# æ•°æ®åŠ è½½å‡½æ•°
# ================================

def load_movie_review_data(file_path: str) -> List[Dict[str, Any]]:
    """
    åŠ è½½è±†ç“£å½±è¯„æ•°æ®
    
    Args:
        file_path: JSONæ–‡ä»¶è·¯å¾„
        
    Returns:
        è§£æåçš„æ•°æ®åˆ—è¡¨
    """
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # æ”¯æŒå•ä¸ªç”¨æˆ·æˆ–å¤šç”¨æˆ·æ•°æ®
    if isinstance(data, dict):
        data = [data]
    
    return data


def extract_movie_review_samples(
    raw_data: List[Dict[str, Any]], 
    max_samples_per_user: Optional[int] = None,
    debug: bool = False
) -> List[Dict[str, Any]]:
    """
    å°†åŸå§‹å½±è¯„æ•°æ®è½¬æ¢ä¸ºè®­ç»ƒæ ·æœ¬æ ¼å¼
    
    ä¸¤ç§æ¨¡å¼ï¼š
    1. max_samples_per_user=Noneï¼ˆé»˜è®¤ï¼‰ï¼šæ¯æ¡å½±è¯„è½¬æ¢ä¸ºä¸€ä¸ªæ ·æœ¬
       - ç”¨æˆ·æœ‰100æ¡å½±è¯„ â†’ ç”Ÿæˆ100ä¸ªæ ·æœ¬
       - æ ·æœ¬1: [] â†’ r1, æ ·æœ¬2: [r1] â†’ r2, ..., æ ·æœ¬100: [r1..r99] â†’ r100
    
    2. max_samples_per_user=Nï¼šæ¯ä¸ªç”¨æˆ·åªç”Ÿæˆæœ€åNä¸ªæ ·æœ¬
       - ç”¨æˆ·æœ‰100æ¡å½±è¯„ï¼Œmax_samples_per_user=2 â†’ ç”Ÿæˆ2ä¸ªæ ·æœ¬
       - æ ·æœ¬1: [r1..r98] â†’ r99ï¼ˆç”¨å‰98æ¡é¢„æµ‹ç¬¬99æ¡ï¼‰
       - æ ·æœ¬2: [r1..r99] â†’ r100ï¼ˆç”¨å‰99æ¡é¢„æµ‹ç¬¬100æ¡ï¼‰
       - **å¤§å¹…å‡å°‘è®­ç»ƒæ•°æ®é‡ï¼ŒåŒæ—¶æœ€å¤§åŒ–å†å²ä¿¡æ¯åˆ©ç”¨**
    
    Args:
        raw_data: åŸå§‹æ•°æ®
        max_samples_per_user: æ¯ä¸ªç”¨æˆ·æœ€å¤šç”Ÿæˆçš„æ ·æœ¬æ•°ï¼ˆNoneè¡¨ç¤ºå…¨éƒ¨ï¼Œé»˜è®¤Noneï¼‰
        debug: æ˜¯å¦è¾“å‡ºè°ƒè¯•ä¿¡æ¯
        
    Returns:
        è®­ç»ƒæ ·æœ¬åˆ—è¡¨ï¼ˆæŒ‰æ—¶é—´é¡ºåºï¼‰
    """
    all_samples = []
    
    for user_data in raw_data:
        user_profile = user_data.get('user', {}).get('profile', {})
        task_desc = user_data.get('task', {}).get('description', '')
        
        # è·å–å½±è¯„æ•°æ®ï¼ˆå·²æŒ‰æ—¶é—´æ’åºï¼‰
        task_collections = user_data.get('task', {}).get('task_behavior_collections', [])
        
        for collection in task_collections:
            if collection.get('type') != 'movie_review':
                continue
            
            reviews = collection.get('data', [])
            
            if debug:
                print(f"å¤„ç†ç”¨æˆ·: {user_profile.get('name', 'Unknown')}")
                print(f"ä»»åŠ¡æè¿°: {task_desc}")
                print(f"å½±è¯„æ€»æ•°: {len(reviews)}")
            
            if max_samples_per_user is not None and max_samples_per_user > 0:
                # ğŸ”¥ æ–°æ¨¡å¼ï¼šæ¯ä¸ªç”¨æˆ·é€‰æ‹©æœ€åNæ¡ä½œä¸ºé¢„æµ‹ç›®æ ‡
                # ä½¿ç”¨å‰ n-N æ¡ä½œä¸ºå†å²ï¼Œé¢„æµ‹æœ€å N æ¡
                if len(reviews) < max_samples_per_user + 1:
                    if debug:
                        print(f"  âš ï¸ è·³è¿‡è¯¥ç”¨æˆ·ï¼ˆå½±è¯„æ•° < {max_samples_per_user + 1}ï¼‰")
                    continue
                
                # å‰ n-N æ¡ä½œä¸ºå…±äº«å†å²
                history_reviews = reviews[:-max_samples_per_user]
                # æœ€åNæ¡ä½œä¸ºé¢„æµ‹ç›®æ ‡
                last_n_reviews = reviews[-max_samples_per_user:]
                
                # ä¸ºæœ€åNæ¡å½±è¯„åˆ†åˆ«åˆ›å»ºæ ·æœ¬ï¼Œä½¿ç”¨ç´¯ç§¯çš„å†å²
                for idx, target_review in enumerate(last_n_reviews):
                    # å†å²åŒ…æ‹¬ï¼šå‰ n-N æ¡ + å½“å‰ç›®æ ‡ä¹‹å‰çš„æ‰€æœ‰æœ€åNæ¡ä¸­çš„å½±è¯„
                    current_history = history_reviews + last_n_reviews[:idx]
                    
                    sample = {
                        'user_profile': user_profile,
                        'user_hash': user_profile.get('name', 'unknown'),
                        'task_description': task_desc,
                        
                        # å†å²å½±è¯„
                        'history': [
                            {
                                'movie': h.get('continuation_prefix', '').rstrip(': '),
                                'review': h.get('continuation', ''),
                                'timestamp': h.get('timestamp', '')
                            }
                            for h in current_history
                        ],
                        
                        # å½“å‰ç”µå½±ä¿¡æ¯
                        'movie_name': target_review.get('continuation_prefix', '').rstrip(': '),
                        'timestamp': target_review.get('timestamp', ''),
                        
                        # ç›®æ ‡ï¼šè¦é¢„æµ‹çš„å½±è¯„
                        'next_question': target_review.get('continuation', ''),
                        
                        # contextä¿æŒç©ºåˆ—è¡¨ï¼ˆå…¼å®¹ç°æœ‰æ¡†æ¶ï¼‰
                        'context': target_review.get('context', []),
                        
                        # å…ƒæ•°æ®
                        'total_reviews': len(reviews),
                        'history_count': len(current_history),
                        'target_index': len(reviews) - max_samples_per_user + idx,
                        'raw_review': target_review
                    }
                    
                    all_samples.append(sample)
                
                if debug:
                    print(f"  ç”Ÿæˆ{max_samples_per_user}ä¸ªæ ·æœ¬: {len(history_reviews)}æ¡å…±äº«å†å² â†’ é¢„æµ‹æœ€å{max_samples_per_user}æ¡")
            
            else:
                # åŸæ¨¡å¼ï¼šä¸ºæ¯æ¡å½±è¯„åˆ›å»ºä¸€ä¸ªè®­ç»ƒæ ·æœ¬
                for i, review in enumerate(reviews):
                    # ä¹‹å‰çš„æ‰€æœ‰å½±è¯„ä½œä¸ºå†å²ä¸Šä¸‹æ–‡
                    history_reviews = reviews[:i] if i > 0 else []
                    
                    sample = {
                        'user_profile': user_profile,
                        'user_hash': user_profile.get('name', 'unknown'),
                        'task_description': task_desc,
                        
                        # å†å²å½±è¯„ï¼ˆä½œä¸ºä¸Šä¸‹æ–‡ï¼‰
                        'history': [
                            {
                                'movie': h.get('continuation_prefix', '').rstrip(': '),
                                'review': h.get('continuation', ''),
                                'timestamp': h.get('timestamp', '')
                            }
                            for h in history_reviews
                        ],
                        
                        # å½“å‰ç”µå½±ä¿¡æ¯
                        'movie_name': review.get('continuation_prefix', '').rstrip(': '),
                        'timestamp': review.get('timestamp', ''),
                        
                        # ç›®æ ‡ï¼šè¦é¢„æµ‹çš„å½±è¯„
                        'next_question': review.get('continuation', ''),
                        
                        # contextä¿æŒç©ºåˆ—è¡¨ï¼ˆå…¼å®¹ç°æœ‰æ¡†æ¶ï¼‰
                        'context': review.get('context', []),
                        
                        # åŸå§‹æ•°æ®ï¼ˆç”¨äºè°ƒè¯•ï¼‰
                        'raw_review': review
                    }
                    
                    all_samples.append(sample)
            
            if debug:
                print(f"ç”Ÿæˆæ ·æœ¬æ•°: {len(all_samples)}")
    
    return all_samples


def split_movie_reviews_by_time(
    samples: List[Dict[str, Any]],
    train_ratio: float = 0.7,
    val_ratio: float = 0.15,
    test_ratio: float = 0.15,
    debug: bool = False
) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]], List[Dict[str, Any]]]:
    """
    æŒ‰æ—¶é—´é¡ºåºåˆ’åˆ†è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†
    
    é‡è¦ï¼šä¿æŒæ—¶é—´é¡ºåºï¼Œç”¨æ—©æœŸæ•°æ®è®­ç»ƒï¼ŒåæœŸæ•°æ®æµ‹è¯•
    
    Args:
        samples: æ ·æœ¬åˆ—è¡¨ï¼ˆå·²æŒ‰æ—¶é—´æ’åºï¼‰
        train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
        val_ratio: éªŒè¯é›†æ¯”ä¾‹
        test_ratio: æµ‹è¯•é›†æ¯”ä¾‹
        debug: æ˜¯å¦è¾“å‡ºè°ƒè¯•ä¿¡æ¯
        
    Returns:
        (train_samples, val_samples, test_samples)
    """
    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6, \
        f"æ¯”ä¾‹ä¹‹å’Œå¿…é¡»ä¸º1.0ï¼Œå½“å‰ä¸º {train_ratio + val_ratio + test_ratio}"
    
    total = len(samples)
    train_end = int(total * train_ratio)
    val_end = train_end + int(total * val_ratio)
    
    train_samples = samples[:train_end]
    val_samples = samples[train_end:val_end]
    test_samples = samples[val_end:]
    
    if debug:
        print("=" * 80)
        print("æŒ‰æ—¶é—´é¡ºåºåˆ’åˆ†æ•°æ®é›†:")
        print(f"  æ€»æ ·æœ¬æ•°: {total}")
        print(f"  è®­ç»ƒé›†: {len(train_samples)} ({len(train_samples)/total*100:.1f}%)")
        if train_samples:
            print(f"    æ—¶é—´èŒƒå›´: {train_samples[0].get('timestamp', 'N/A')} -> {train_samples[-1].get('timestamp', 'N/A')}")
        
        print(f"  éªŒè¯é›†: {len(val_samples)} ({len(val_samples)/total*100:.1f}%)")
        if val_samples:
            print(f"    æ—¶é—´èŒƒå›´: {val_samples[0].get('timestamp', 'N/A')} -> {val_samples[-1].get('timestamp', 'N/A')}")
        
        print(f"  æµ‹è¯•é›†: {len(test_samples)} ({len(test_samples)/total*100:.1f}%)")
        if test_samples:
            print(f"    æ—¶é—´èŒƒå›´: {test_samples[0].get('timestamp', 'N/A')} -> {test_samples[-1].get('timestamp', 'N/A')}")
        print("=" * 80)
    
    return train_samples, val_samples, test_samples


# ================================
# æ•°æ®é›†ç±»
# ================================

class MovieReviewDataset(Dataset):
    """
    å½±è¯„æ•°æ®é›†ï¼ˆåŠ¨æ€Paddingç‰ˆæœ¬ï¼‰
    """
    def __init__(self, samples, tokenizer, max_length=4096, use_profile=True, use_history=True, verbose=False):
        self.samples = samples
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.use_profile = use_profile
        self.use_history = use_history
        self.verbose = verbose
        
        # æˆªæ–­ç»Ÿè®¡
        self.truncation_stats = {
            'total_samples': 0,
            'truncated_samples': 0,
            'truncated_history': 0,
        }
        self.first_truncation_logged = False
    
    def build_prompt(self, sample: Dict[str, Any]) -> Tuple[List[Dict[str, str]], str]:
        """
        æ„å»ºå½±è¯„è®­ç»ƒprompt
        
        è¿”å›:
            messages: èŠå¤©æ¶ˆæ¯åˆ—è¡¨
            target_answer: ç›®æ ‡ç­”æ¡ˆï¼ˆè¦é¢„æµ‹çš„å½±è¯„ï¼‰
        """
        parts = []
        
        # 1. ç”¨æˆ·Profile
        if self.use_profile and sample.get('user_profile'):
            profile = sample['user_profile']
            parts.append(f"ç”¨æˆ·: {profile.get('name', 'Unknown')}")
        
        # 2. å†å²å½±è¯„
        if self.use_history and sample.get('history'):
            history = sample['history']
            if history:
                parts.append(f"\nå†å²å½±è¯„è®°å½• ({len(history)}æ¡):")
                for h in history:
                    parts.append(f"  ç”µå½±ã€Š{h['movie']}ã€‹: {h['review']}")
        
        # 3. å½“å‰ç”µå½±ï¼ˆä¸­æ–‡æç¤ºï¼‰
        movie_name = sample.get('movie_name', '')
        parts.append(f"\né¢„æµ‹ç”¨æˆ·å¯¹è¯¥ç”µå½±çš„è¯„ä»·ï¼š")
        parts.append("æ³¨æ„ï¼šè¯·ç›´æ¥ç»™å‡ºç”¨æˆ·å¯¹è¯¥ç”µå½±çš„è¯„ä»·ï¼Œç”¨ [ANSWER] å’Œ [/ANSWER] æ ‡ç­¾åŒ…è£¹ç­”æ¡ˆå†…å®¹ï¼Œä¸éœ€è¦è§£é‡Šæˆ–æ€è€ƒè¿‡ç¨‹ã€‚")
        
        system_content = "\n".join(parts)
        
        messages = [
            {'role': 'system', 'content': system_content}
        ]
        
        # target_answer ç”¨ [ANSWER] å’Œ [/ANSWER] åŒ…è£¹ next_questionï¼ˆä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰
        next_question = sample.get('next_question', '')
        target_answer = f"[ANSWER]\n{next_question}\n[/ANSWER]"
        
        return messages, target_answer
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        sample = self.samples[idx]
        
        # 1. æ„å»ºæ¶ˆæ¯
        messages, target_answer = self.build_prompt(sample)
        
        # 2. ç”Ÿæˆå®Œæ•´æ–‡æœ¬
        full_prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)
        generation_suffix = "<|im_start|>assistant\n"
        full_prompt = full_prompt.strip() + generation_suffix
        im_end_token = "<|im_end|>"
        full_text = full_prompt + target_answer + im_end_token
        
        # 3. å¤„ç†è¶…é•¿æ–‡æœ¬ï¼šåˆ é™¤å†å²è®°å½•
        full_length = len(self.tokenizer.encode(full_text, add_special_tokens=False))
        is_truncated = False
        removed_history = 0
        
        if full_length > self.max_length:
            is_truncated = True
            history = sample.get('history', [])
            
            if history and len(history) > 0:
                reduced_history = history[:]
                while full_length > self.max_length and len(reduced_history) > 0:
                    reduced_history.pop(0)  # åˆ é™¤æœ€æ—§çš„å†å²è®°å½•
                    removed_history += 1
                    
                    # é‡å»ºæ ·æœ¬
                    temp_sample = sample.copy()
                    temp_sample['history'] = reduced_history
                    messages, target_answer = self.build_prompt(temp_sample)
                    
                    # é‡æ–°ç”Ÿæˆ
                    full_prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)
                    full_prompt = full_prompt.strip() + generation_suffix
                    full_text = full_prompt + target_answer + im_end_token
                    full_length = len(self.tokenizer.encode(full_text, add_special_tokens=False))
        
        # æ›´æ–°ç»Ÿè®¡
        self.truncation_stats['total_samples'] += 1
        if is_truncated:
            self.truncation_stats['truncated_samples'] += 1
            self.truncation_stats['truncated_history'] += removed_history
            
            if not self.first_truncation_logged and self.verbose:
                self.first_truncation_logged = True
                print(f"\nâš ï¸  æ ·æœ¬#{idx} è¶…é•¿ï¼Œåˆ é™¤äº† {removed_history} æ¡å†å²è®°å½•")
                print(f"  è°ƒæ•´åé•¿åº¦: {full_length} tokens (max: {self.max_length})\n")
        
        # 4. ç¼–ç ï¼ˆä¸åšpaddingï¼‰
        encoded = self.tokenizer(
            full_text,
            truncation=True,
            max_length=self.max_length,
            padding=False,
            return_tensors='pt'
        )
        
        input_ids = encoded['input_ids'].squeeze()
        attention_mask = encoded['attention_mask'].squeeze()
        
        # 5. è®¡ç®—labels
        prompt_ids = self.tokenizer.encode(full_prompt, add_special_tokens=False)
        actual_prompt_len = len(prompt_ids)
        
        labels = input_ids.clone()
        safe_prompt_len = min(actual_prompt_len, len(input_ids) - 1)
        labels[:safe_prompt_len] = -100
        labels[input_ids == self.tokenizer.pad_token_id] = -100
        
        return {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'labels': labels,
            'actual_length': len(input_ids)
        }


def dynamic_padding_collate_fn(examples, tokenizer):
    """
    åŠ¨æ€Paddingçš„collateå‡½æ•°
    åªpaddingåˆ°batchå†…æœ€é•¿æ ·æœ¬çš„é•¿åº¦
    """
    # æ‰¾åˆ°batchä¸­æœ€é•¿çš„åºåˆ—é•¿åº¦
    max_length_in_batch = max(ex['input_ids'].shape[0] for ex in examples)
    
    # 5%æ¦‚ç‡æ‰“å°batchä¿¡æ¯
    if random.random() < 0.05:
        lengths = [ex['input_ids'].shape[0] for ex in examples]
        print(f"[Batch] Lengths: min={min(lengths)}, max={max_length_in_batch}, avg={sum(lengths)/len(lengths):.0f}")
    
    batch = {}
    padded_input_ids = []
    padded_attention_mask = []
    padded_labels = []
    
    for ex in examples:
        seq_len = ex['input_ids'].shape[0]
        pad_len = max_length_in_batch - seq_len
        
        # Padding
        padded_input_ids.append(
            torch.cat([
                ex['input_ids'],
                torch.full((pad_len,), tokenizer.pad_token_id, dtype=torch.long)
            ])
        )
        
        padded_attention_mask.append(
            torch.cat([
                ex['attention_mask'],
                torch.zeros(pad_len, dtype=torch.long)
            ])
        )
        
        padded_labels.append(
            torch.cat([
                ex['labels'],
                torch.full((pad_len,), -100, dtype=torch.long)
            ])
        )
    
    batch['input_ids'] = torch.stack(padded_input_ids)
    batch['attention_mask'] = torch.stack(padded_attention_mask)
    batch['labels'] = torch.stack(padded_labels)
    
    return batch


# ================================
# åˆ†å¸ƒå¼è®­ç»ƒè¾…åŠ©å‡½æ•°
# ================================

def setup_distributed():
    """åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ"""
    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:
        rank = int(os.environ["RANK"])
        world_size = int(os.environ['WORLD_SIZE'])
        local_rank = int(os.environ['LOCAL_RANK'])
    elif 'SLURM_PROCID' in os.environ:
        rank = int(os.environ['SLURM_PROCID'])
        world_size = int(os.environ['SLURM_NTASKS'])
        local_rank = rank % torch.cuda.device_count()
    else:
        print('æœªæ£€æµ‹åˆ°åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒï¼Œä½¿ç”¨å•å¡è®­ç»ƒ')
        rank = 0
        world_size = 1
        local_rank = 0
    
    torch.cuda.set_device(local_rank)
    
    if world_size > 1:
        dist.init_process_group(
            backend='nccl',
            init_method='env://',
            world_size=world_size,
            rank=rank
        )
    
    return rank, world_size, local_rank


def cleanup_distributed():
    """æ¸…ç†åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ"""
    if dist.is_initialized():
        dist.destroy_process_group()


# ================================
# ä¸»å‡½æ•°
# ================================

def main():
    parser = argparse.ArgumentParser(description='è±†ç“£å½±è¯„æ¨¡å‹ - åˆ†å¸ƒå¼è®­ç»ƒï¼ˆè‡ªåŒ…å«ç‰ˆæœ¬ï¼‰')
    
    # é…ç½®æ–‡ä»¶
    parser.add_argument('--config', type=str, required=True, help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--ablation_config', type=str, required=True,
                       choices=['profile_and_history', 'profile_only', 'history_only', 'baseline'],
                       help='æ¶ˆèå®éªŒé…ç½®')
    
    # æ•°æ®ç›¸å…³
    parser.add_argument('--data_file', type=str, default=None, help='å½±è¯„æ•°æ®JSONæ–‡ä»¶è·¯å¾„ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶ï¼‰')
    parser.add_argument('--val_ratio', type=float, default=None, help='éªŒè¯é›†æ¯”ä¾‹ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶ï¼‰')
    parser.add_argument('--max_samples_per_user', type=int, default=None, 
                       help='æ¯ä¸ªç”¨æˆ·æœ€å¤šç”Ÿæˆçš„æ ·æœ¬æ•°ï¼ˆNoneè¡¨ç¤ºå…¨éƒ¨ï¼Œé»˜è®¤ä»é…ç½®æ–‡ä»¶è¯»å–ï¼‰')
    parser.add_argument('--one_sample_per_user', action='store_true',
                       help='[å·²åºŸå¼ƒ] ä½¿ç”¨ --max_samples_per_user=2 æ›¿ä»£')
    
    # è¾“å‡ºç›®å½•
    parser.add_argument('--output_dir', type=str, default=None, help='æ¨¡å‹è¾“å‡ºç›®å½•')
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--max_epochs', type=int, default=50, help='æœ€å¤§è®­ç»ƒè½®æ¬¡')
    parser.add_argument('--early_stopping_patience', type=int, default=3, help='æ—©åœè€å¿ƒå€¼')
    parser.add_argument('--early_stopping_threshold', type=float, default=0.001, help='æ—©åœé˜ˆå€¼')
    
    # DeepSpeed
    parser.add_argument('--deepspeed', type=str, default=None, help='DeepSpeedé…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--disable_flash_attn', action='store_true', help='ç¦ç”¨FlashAttention 2')
    
    # W&B
    parser.add_argument('--wandb_project', type=str, default='MovieReview', help='Weights & Biasesé¡¹ç›®åç§°')
    parser.add_argument('--wandb_run_name', type=str, default=None, help='Weights & Biasesè¿è¡Œåç§°')
    
    # å…¶ä»–
    parser.add_argument('--local_rank', type=int, default=-1, help='æœ¬åœ°è¿›ç¨‹rank')
    parser.add_argument('--prompt_style', type=str, default='simple', choices=['simple', 'detailed'],
                       help='Prompté£æ ¼ï¼ˆæœ¬ç‰ˆæœ¬åªæ”¯æŒsimpleï¼‰')
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®æ–‡ä»¶
    with open(args.config, 'r', encoding='utf-8') as f:
        config = json.load(f)
    
    # è·å–æ¶ˆèé…ç½®
    ablation_config = config['ablation_configs'][args.ablation_config]
    use_profile = ablation_config.get('use_profile', True)
    use_history = ablation_config.get('use_history', True)
    config_name = ablation_config['name']
    
    # åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ
    rank, world_size, local_rank = setup_distributed()
    is_main_process = (rank == 0)
    
    # é…ç½®W&B
    if args.wandb_project and is_main_process:
        try:
            import wandb
            os.environ['WANDB_PROJECT'] = args.wandb_project
            if args.wandb_run_name:
                os.environ['WANDB_NAME'] = args.wandb_run_name
            print(f"âœ“ å·²å¯ç”¨ W&B ç›‘æ§: {args.wandb_project}")
        except ImportError:
            print("è­¦å‘Š: wandb æœªå®‰è£…")
            args.wandb_project = None
    
    if is_main_process:
        print("=" * 80)
        print("è±†ç“£å½±è¯„æ¨¡å‹ - åˆ†å¸ƒå¼è®­ç»ƒï¼ˆè‡ªåŒ…å«ç‰ˆæœ¬ï¼‰")
        print("=" * 80)
        print(f"World Size: {world_size}, Rank: {rank}, Local Rank: {local_rank}")
        print(f"æ¶ˆèå®éªŒ: {config_name}")
        print(f"ä½¿ç”¨é…ç½®: Profile={use_profile}, History={use_history}")
        # å¤„ç† max_samples_per_user
        max_samples = args.max_samples_per_user
        if max_samples is None:
            # ä»é…ç½®æ–‡ä»¶è¯»å–
            max_samples = config.get('training', {}).get('max_samples_per_user', None)
        # å…¼å®¹æ—§çš„ one_sample_per_user å‚æ•°
        if args.one_sample_per_user:
            max_samples = 2
            if is_main_process:
                print(f"âš ï¸  --one_sample_per_user å·²åºŸå¼ƒï¼Œè¯·ä½¿ç”¨ --max_samples_per_user=2")
        
        if max_samples is not None:
            print(f"é‡‡æ ·æ¨¡å¼: æ¯ç”¨æˆ·æœ€å¤š {max_samples} ä¸ªæ ·æœ¬")
        else:
            print(f"é‡‡æ ·æ¨¡å¼: æ¯æ¡å½±è¯„ä¸€ä¸ªæ ·æœ¬ï¼ˆå…¨éƒ¨ï¼‰")
        if args.deepspeed:
            print(f"DeepSpeed: {args.deepspeed}")
        print("=" * 80)
    
    # æ£€æŸ¥FlashAttention
    use_flash_attn = False
    if not args.disable_flash_attn:
        try:
            import flash_attn
            use_flash_attn = True
            if is_main_process:
                print(f"FlashAttention å·²å®‰è£…")
        except ImportError:
            if is_main_process:
                print("FlashAttention æœªå®‰è£…ï¼Œä½¿ç”¨æ ‡å‡†attention")
    
    # éªŒè¯GPU
    if not torch.cuda.is_available():
        print(f"[Rank {rank}] é”™è¯¯: CUDA ä¸å¯ç”¨")
        cleanup_distributed()
        return
    
    if is_main_process:
        print(f"CUDA å¯ç”¨ï¼ŒGPUæ•°é‡: {torch.cuda.device_count()}")
        print(f"å½“å‰GPU: {local_rank} - {torch.cuda.get_device_name(local_rank)}")
    
    # åŠ è½½æ•°æ®
    if is_main_process:
        print("\n" + "=" * 80)
        print("åŠ è½½å½±è¯„æ•°æ®...")
    
    data_file = args.data_file if args.data_file else config['data']['train_path']
    raw_data = load_movie_review_data(data_file)
    
    # ç¡®å®š max_samples_per_user
    max_samples = args.max_samples_per_user
    if max_samples is None:
        # ä»é…ç½®æ–‡ä»¶è¯»å–
        max_samples = config.get('training', {}).get('max_samples_per_user', None)
    # å…¼å®¹æ—§çš„ one_sample_per_user å‚æ•°
    if args.one_sample_per_user:
        max_samples = 2
        if is_main_process:
            print(f"âš ï¸  --one_sample_per_user å·²åºŸå¼ƒï¼Œè¯·ä½¿ç”¨ --max_samples_per_user=2")
    
    all_samples = extract_movie_review_samples(
        raw_data, 
        max_samples_per_user=max_samples,
        debug=is_main_process
    )
    
    if is_main_process:
        print(f"æ•°æ®æ–‡ä»¶: {data_file}")
        print(f"æå–äº† {len(all_samples)} ä¸ªæ ·æœ¬")
    
    # è·å–æ•°æ®åˆ’åˆ†æ¯”ä¾‹
    data_split = config.get('data_split', {})
    train_ratio = data_split.get('train_ratio', 0.7)
    val_ratio_config = data_split.get('val_ratio', 0.15)
    test_ratio = data_split.get('test_ratio', 0.15)
    
    if args.val_ratio is not None:
        val_ratio_config = args.val_ratio
        test_ratio = 1.0 - train_ratio - val_ratio_config
    
    # æŒ‰æ—¶é—´åˆ’åˆ†æ•°æ®é›†
    train_samples, val_samples, test_samples = split_movie_reviews_by_time(
        all_samples,
        train_ratio=train_ratio,
        val_ratio=val_ratio_config,
        test_ratio=test_ratio,
        debug=is_main_process
    )
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    if args.output_dir:
        output_dir = args.output_dir
    else:
        checkpoint_dir = config['model']['checkpoint_dir']
        output_dir = os.path.join(checkpoint_dir, f"MovieReview_{config_name}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    if is_main_process:
        os.makedirs(output_dir, exist_ok=True)
        print(f"è¾“å‡ºç›®å½•: {output_dir}")
        
        # ä¿å­˜æµ‹è¯•é›†
        test_file = os.path.join(output_dir, 'test_samples.json')
        with open(test_file, 'w', encoding='utf-8') as f:
            json.dump(test_samples, f, ensure_ascii=False, indent=2)
        print(f"æµ‹è¯•é›†å·²ä¿å­˜åˆ°: {test_file}")
    
    if world_size > 1:
        dist.barrier()
    
    # åŠ è½½tokenizerå’Œæ¨¡å‹
    model_path = config['model']['path']
    if is_main_process:
        print(f"\nåŠ è½½æ¨¡å‹: {model_path}")
    
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # è·å–è®­ç»ƒé…ç½®å’Œæ¨¡å‹é…ç½®
    train_config = config.get('training', {})
    model_config = config.get('model', {})
    
    # åŠ è½½æ¨¡å‹
    model_kwargs = {
        'torch_dtype': torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,
        'trust_remote_code': True,
    }
    
    if use_flash_attn:
        model_kwargs['attn_implementation'] = 'flash_attention_2'
    
    try:
        model = AutoModelForCausalLM.from_pretrained(model_path, **model_kwargs)
        if is_main_process:
            if use_flash_attn:
                print("âœ“ æ¨¡å‹å·²åŠ è½½ï¼ˆFlashAttention 2ï¼‰")
            else:
                print("âœ“ æ¨¡å‹å·²åŠ è½½ï¼ˆæ ‡å‡†Attentionï¼‰")
    except Exception as e:
        if is_main_process:
            print(f"åŠ è½½å¤±è´¥: {e}")
            print("å›é€€åˆ°æ ‡å‡†attention...")
        model_kwargs.pop('attn_implementation', None)
        model = AutoModelForCausalLM.from_pretrained(model_path, **model_kwargs)
        use_flash_attn = False
    
    # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
    if hasattr(model, 'gradient_checkpointing_enable'):
        model.gradient_checkpointing_enable()
        if is_main_process:
            print("âœ“ æ¢¯åº¦æ£€æŸ¥ç‚¹å·²å¯ç”¨")
    
    # LoRA é…ç½®ï¼ˆå¦‚æœé…ç½®æ–‡ä»¶ä¸­å¯ç”¨ï¼‰
    use_lora = model_config.get('use_lora', False)
    if use_lora:
        if not PEFT_AVAILABLE:
            raise ImportError("LoRA å·²å¯ç”¨ä½† peft åº“æœªå®‰è£…ã€‚è¯·è¿è¡Œ: pip install peft")
        
        lora_config_dict = model_config.get('lora_config', {})
        if is_main_process:
            print("\n" + "="*80)
            print("âš¡ LoRA é…ç½®:")
            print(f"   - rank (r): {lora_config_dict.get('r', 64)}")
            print(f"   - alpha: {lora_config_dict.get('lora_alpha', 128)}")
            print(f"   - dropout: {lora_config_dict.get('lora_dropout', 0.05)}")
            print(f"   - target modules: {lora_config_dict.get('target_modules', [])}")
            print("="*80 + "\n")
        
        # åˆ›å»º LoRA é…ç½®
        lora_config = LoraConfig(
            r=lora_config_dict.get('r', 64),
            lora_alpha=lora_config_dict.get('lora_alpha', 128),
            lora_dropout=lora_config_dict.get('lora_dropout', 0.05),
            target_modules=lora_config_dict.get('target_modules', [
                "q_proj", "k_proj", "v_proj", "o_proj",
                "gate_proj", "up_proj", "down_proj"
            ]),
            bias=lora_config_dict.get('bias', 'none'),
            task_type=TaskType.CAUSAL_LM,
        )
        
        # åº”ç”¨ LoRA
        model = get_peft_model(model, lora_config)
        
        if is_main_process:
            print("âœ“ LoRA å·²åº”ç”¨")
            model.print_trainable_parameters()
    
    model = model.to(local_rank)
    
    # åˆ›å»ºæ•°æ®é›†
    if is_main_process:
        print("\nåˆ›å»ºè®­ç»ƒæ•°æ®é›†...")
    
    train_dataset = MovieReviewDataset(
        samples=train_samples,
        tokenizer=tokenizer,
        max_length=train_config.get('max_length', 4096),
        use_profile=use_profile,
        use_history=use_history,
        verbose=is_main_process
    )
    
    val_dataset = None
    if val_samples:
        val_dataset = MovieReviewDataset(
            samples=val_samples,
            tokenizer=tokenizer,
            max_length=train_config.get('max_length', 4096),
            use_profile=use_profile,
            use_history=use_history,
            verbose=False
        )
    
    # Tokené•¿åº¦ç»Ÿè®¡
    if is_main_process:
        print("\n" + "=" * 80)
        print("ğŸ“Š Token é•¿åº¦ç»Ÿè®¡ï¼ˆè®­ç»ƒé›†ï¼‰")
        print("=" * 80)
        
        token_lengths = []
        for i in range(min(len(train_dataset), 1000)):  # åªç»Ÿè®¡å‰1000ä¸ª
            sample = train_dataset[i]
            token_lengths.append(len(sample['input_ids']))
        
        if token_lengths:
            import numpy as np
            max_length_config = train_config.get('max_length', 4096)
            
            print(f"æ ·æœ¬æ€»æ•°: {len(train_dataset)}")
            print(f"é…ç½®çš„ max_length: {max_length_config}")
            print(f"\nToken é•¿åº¦åˆ†å¸ƒï¼ˆå‰1000ä¸ªæ ·æœ¬ï¼‰:")
            print(f"  æœ€å°: {min(token_lengths)}, æœ€å¤§: {max(token_lengths)}, å¹³å‡: {np.mean(token_lengths):.1f}")
            print(f"  ä¸­ä½æ•°: {np.median(token_lengths):.0f}")
            print(f"  75%: {np.percentile(token_lengths, 75):.0f}")
            print(f"  95%: {np.percentile(token_lengths, 95):.0f}")
            
            exceed_count = sum(1 for l in token_lengths if l > max_length_config)
            if exceed_count > 0:
                print(f"\nâš ï¸ {exceed_count} ä¸ªæ ·æœ¬è¶…è¿‡ max_length")
            else:
                print(f"\nâœ… æ‰€æœ‰æ ·æœ¬éƒ½åœ¨ max_length èŒƒå›´å†…")
        
        print("=" * 80)
    
    # æ•°æ®æ•´ç†å™¨
    def collate_fn(examples):
        return dynamic_padding_collate_fn(examples, tokenizer)
    
    # è®¡ç®—è®­ç»ƒæ­¥æ•°
    batch_size = train_config.get('batch_size', 2)
    gradient_accumulation_steps = train_config.get('gradient_accumulation_steps', 8)
    steps_per_epoch = len(train_dataset) // (world_size * batch_size * gradient_accumulation_steps)
    eval_steps = max(1, steps_per_epoch // 2) if val_dataset else None
    save_steps = train_config.get('save_steps', 500)
    
    # è°ƒæ•´save_steps
    if val_dataset and eval_steps and save_steps % eval_steps != 0:
        save_steps = ((save_steps + eval_steps - 1) // eval_steps) * eval_steps
        if is_main_process:
            print(f"è°ƒæ•´ save_steps ä¸º {save_steps}")
    
    # è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=args.max_epochs,
        per_device_train_batch_size=batch_size,
        per_device_eval_batch_size=train_config.get('eval_batch_size', 2),
        gradient_accumulation_steps=gradient_accumulation_steps,
        learning_rate=train_config.get('learning_rate', 1e-5),
        weight_decay=train_config.get('weight_decay', 0.01),
        warmup_steps=train_config.get('warmup_steps', 100),
        logging_steps=train_config.get('logging_steps', 10),
        save_steps=save_steps,
        eval_steps=eval_steps,
        eval_strategy="steps" if val_dataset else "no",
        save_total_limit=train_config.get('save_total_limit', 3),
        load_best_model_at_end=True if val_dataset else False,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        fp16=False,
        bf16=True,
        dataloader_pin_memory=False,
        gradient_checkpointing=True,
        optim="adamw_torch",
        max_grad_norm=0.5,
        report_to="wandb" if args.wandb_project else "none",
        local_rank=local_rank,
        ddp_find_unused_parameters=False,
        ddp_backend="nccl",
        dataloader_num_workers=train_config.get('dataloader_num_workers', 4),
        save_on_each_node=False,
        logging_first_step=True,
        deepspeed=args.deepspeed,
    )
    
    # æ—©åœå›è°ƒ
    callbacks = []
    if val_dataset:
        early_stopping = EarlyStoppingCallback(
            early_stopping_patience=args.early_stopping_patience,
            early_stopping_threshold=args.early_stopping_threshold
        )
        callbacks.append(early_stopping)
    
    # åˆ›å»ºè‡ªå®šä¹‰Trainerï¼ˆå¸¦æƒé‡å¤„ç†ï¼‰
    class CustomTrainer(Trainer):
        def __init__(self, *args, **kwargs):
            super().__init__(*args, **kwargs)
            # ä¿å­˜ tokenizer å¼•ç”¨ï¼ˆç”¨äºæŸå¤±æƒé‡è®¡ç®—ï¼‰
            self.tokenizer = tokenizer
        
        def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
            """è®¡ç®—æŸå¤±ï¼ˆå¯¹ [ANSWER] å’Œ [/ANSWER] token å¢åŠ æƒé‡ï¼‰"""
            outputs = model(**inputs)
            logits = outputs.get("logits")
            labels = inputs.get("labels")
            
            if hasattr(outputs, 'loss') and outputs.loss is not None:
                loss = outputs.loss
            elif labels is not None:
                valid_labels_count = (labels != -100).sum().item()
                
                if valid_labels_count == 0:
                    if rank == 0:
                        print(f"è­¦å‘Š: [GPU {rank}] Step {self.state.global_step} æ²¡æœ‰æœ‰æ•ˆçš„labels")
                    loss = torch.tensor(2.0, device=logits.device, requires_grad=True)
                else:
                    shift_logits = logits[..., :-1, :].contiguous()
                    shift_labels = labels[..., 1:].contiguous()
                    
                    # åˆ›å»ºæŸå¤±æƒé‡ï¼šå¯¹ [ANSWER] å’Œ [/ANSWER] token å¢åŠ æƒé‡
                    # è·å– tokenizer ä¸­çš„ [ANSWER] å’Œ [/ANSWER] çš„æ‰€æœ‰ token IDs
                    answer_start_token_ids = set()
                    answer_end_token_ids = set()
                    
                    try:
                        # å°è¯•è·å– [ANSWER] å’Œ [/ANSWER] çš„æ‰€æœ‰ token IDs
                        if hasattr(self.tokenizer, 'encode'):
                            # ç¼–ç æ ‡ç­¾ï¼ˆå¯èƒ½è¢«ç¼–ç ä¸ºå¤šä¸ª tokenï¼‰
                            answer_start_tokens = self.tokenizer.encode("[ANSWER]", add_special_tokens=False)
                            answer_end_tokens = self.tokenizer.encode("[/ANSWER]", add_special_tokens=False)
                            
                            # ä¿å­˜æ‰€æœ‰ç›¸å…³çš„ token IDsï¼ˆä¸ä»…ä»…æ˜¯ç¬¬ä¸€ä¸ªï¼‰
                            if answer_start_tokens:
                                answer_start_token_ids = set(answer_start_tokens)
                            if answer_end_tokens:
                                answer_end_token_ids = set(answer_end_tokens)
                    except:
                        pass
                    
                    # åˆ›å»ºæƒé‡å¼ é‡ï¼ˆé»˜è®¤æƒé‡ä¸º 1.0ï¼‰
                    batch_size, seq_len = shift_labels.shape
                    loss_weights = torch.ones_like(shift_labels, dtype=torch.float32)
                    
                    # å¯¹ [ANSWER] å’Œ [/ANSWER] çš„æ‰€æœ‰ token å¢åŠ æƒé‡ï¼ˆæƒé‡è®¾ä¸º 3.0ï¼‰
                    if answer_start_token_ids:
                        for token_id in answer_start_token_ids:
                            loss_weights[shift_labels == token_id] = 3.0
                    if answer_end_token_ids:
                        for token_id in answer_end_token_ids:
                            loss_weights[shift_labels == token_id] = 3.0
                    
                    # ä½¿ç”¨åŠ æƒæŸå¤±
                    loss_fct = nn.CrossEntropyLoss(ignore_index=-100, reduction='none')
                    per_token_loss = loss_fct(
                        shift_logits.view(-1, shift_logits.size(-1)),
                        shift_labels.view(-1)
                    )
                    
                    # åº”ç”¨æƒé‡å¹¶è®¡ç®—å¹³å‡æŸå¤±
                    per_token_loss = per_token_loss.view(batch_size, seq_len)
                    valid_mask = (shift_labels != -100)
                    weighted_loss = (per_token_loss * loss_weights * valid_mask.float()).sum()
                    valid_count = (valid_mask.float() * loss_weights).sum()
                    
                    if valid_count > 0:
                        loss = weighted_loss / valid_count
                    else:
                        loss = torch.tensor(2.0, device=logits.device, requires_grad=True)
            else:
                loss = torch.tensor(2.0, device=logits.device, requires_grad=True)
            
            # æ£€æŸ¥æŸå¤±å€¼
            if loss is not None and torch.is_tensor(loss):
                if loss.dim() > 0:
                    loss = loss.mean()
                
                if torch.isnan(loss) or torch.isinf(loss):
                    if rank == 0:
                        print(f"è­¦å‘Š: [GPU {rank}] Step {self.state.global_step} lossä¸ºnan/inf")
                    loss = torch.tensor(2.0, device=logits.device, requires_grad=True)
                elif loss.item() > 1e6:
                    if rank == 0:
                        print(f"è­¦å‘Š: [GPU {rank}] Step {self.state.global_step} lossè¿‡å¤§")
                    loss = torch.clamp(loss, max=100.0)
            
            if return_outputs:
                return loss, outputs
            return loss
    
    # åˆ›å»ºTrainer
    trainer = CustomTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        data_collator=collate_fn,
        processing_class=tokenizer,
        callbacks=callbacks,
    )
    
    # å¼€å§‹è®­ç»ƒ
    if is_main_process:
        print("\n" + "=" * 80)
        print("å¼€å§‹è®­ç»ƒ")
        print("=" * 80)
        print(f"è®­ç»ƒæ ·æœ¬: {len(train_dataset)}")
        print(f"éªŒè¯æ ·æœ¬: {len(val_dataset) if val_dataset else 0}")
        print(f"æµ‹è¯•æ ·æœ¬: {len(test_samples)}")
        print(f"æœ‰æ•ˆbatch size: {batch_size * gradient_accumulation_steps * world_size}")
        print(f"é¢„è®¡æ¯epochæ­¥æ•°: {steps_per_epoch}")
        print("=" * 80)
    
    trainer.train()
    
    # ä¿å­˜æ¨¡å‹
    if is_main_process:
        print(f"\nä¿å­˜æ¨¡å‹åˆ° {output_dir}")
        trainer.save_model()
        tokenizer.save_pretrained(output_dir)
        
        # ä¿å­˜é…ç½®
        config_info = {
            'data_file': data_file,
            'ablation_config': args.ablation_config,
            'config_name': config_name,
            'use_profile': use_profile,
            'use_history': use_history,
            'max_samples_per_user': max_samples,
            'one_sample_per_user': args.one_sample_per_user,  # ä¿ç•™ä»¥å…¼å®¹
            'train_samples': len(train_samples),
            'val_samples': len(val_samples),
            'test_samples': len(test_samples),
        }
        with open(os.path.join(output_dir, 'training_config.json'), 'w', encoding='utf-8') as f:
            json.dump(config_info, f, indent=2, ensure_ascii=False)
        
        print("âœ“ è®­ç»ƒå®Œæˆï¼")
        print(f"âœ“ æ¨¡å‹å·²ä¿å­˜åˆ°: {output_dir}")
    
    # ç­‰å¾…æ‰€æœ‰è¿›ç¨‹å®Œæˆ
    if world_size > 1:
        dist.barrier()
    
    cleanup_distributed()


if __name__ == '__main__':
    main()
