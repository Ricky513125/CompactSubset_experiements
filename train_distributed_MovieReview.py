"""
åˆ†å¸ƒå¼è®­ç»ƒè„šæœ¬ - è±†ç“£å½±è¯„æ¨¡å‹ï¼ˆFlashAttention 2 + åŠ¨æ€Batch Paddingï¼‰
è‡ªåŒ…å«ç‰ˆæœ¬ - æ— éœ€å¤–éƒ¨ä¾èµ–

ç”¨äºè®­ç»ƒç”¨æˆ·å½±è¯„é£æ ¼æ¨¡æ‹Ÿæ¨¡å‹

ä½¿ç”¨æ–¹æ³•:
torchrun \
    --nproc_per_node=8 \
    --master_port=29505 \
    train_distributed_MovieReview.py \
    --config config_DMSC_30B.json \
    --deepspeed ds_config_zero3_optimized.json \
    --ablation_config profile_and_history \
    --output_dir outputs/DMSC_one_per_user_0213 \
    --max_epochs 50 \
    --val_ratio 0.1 \
    --wandb_project Qwen3_30B-DMSC \
    --wandb_run_name one_per_user_0213 \
    --prompt_style simple \
    --one_sample_per_user
"""

import json
import argparse
import os
import sys
from pathlib import Path
import random
import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler
from torch.utils.data import Dataset
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    EarlyStoppingCallback,
    TrainingArguments,
    Trainer
)
from typing import List, Dict, Any, Optional, Tuple
import torch.nn as nn
from datetime import datetime


# ================================
# æ•°æ®åŠ è½½å‡½æ•°
# ================================

def load_movie_review_data(file_path: str) -> List[Dict[str, Any]]:
    """
    åŠ è½½è±†ç“£å½±è¯„æ•°æ®
    
    Args:
        file_path: JSONæ–‡ä»¶è·¯å¾„
        
    Returns:
        è§£æåçš„æ•°æ®åˆ—è¡¨
    """
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # æ”¯æŒå•ä¸ªç”¨æˆ·æˆ–å¤šç”¨æˆ·æ•°æ®
    if isinstance(data, dict):
        data = [data]
    
    return data


def extract_movie_review_samples(
    raw_data: List[Dict[str, Any]], 
    one_sample_per_user: bool = False,
    debug: bool = False
) -> List[Dict[str, Any]]:
    """
    å°†åŸå§‹å½±è¯„æ•°æ®è½¬æ¢ä¸ºè®­ç»ƒæ ·æœ¬æ ¼å¼
    
    ä¸¤ç§æ¨¡å¼ï¼š
    1. one_sample_per_user=Falseï¼ˆé»˜è®¤ï¼‰ï¼šæ¯æ¡å½±è¯„è½¬æ¢ä¸ºä¸€ä¸ªæ ·æœ¬
       - ç”¨æˆ·æœ‰100æ¡å½±è¯„ â†’ ç”Ÿæˆ100ä¸ªæ ·æœ¬
       - æ ·æœ¬1: [] â†’ r1, æ ·æœ¬2: [r1] â†’ r2, ..., æ ·æœ¬100: [r1..r99] â†’ r100
    
    2. one_sample_per_user=Trueï¼šæ¯ä¸ªç”¨æˆ·åªç”Ÿæˆ2ä¸ªæ ·æœ¬ï¼ˆæœ€å2æ¡ï¼‰
       - ç”¨æˆ·æœ‰100æ¡å½±è¯„ â†’ ç”Ÿæˆ2ä¸ªæ ·æœ¬
       - æ ·æœ¬1: [r1..r98] â†’ r99ï¼ˆç”¨å‰98æ¡é¢„æµ‹ç¬¬99æ¡ï¼‰
       - æ ·æœ¬2: [r1..r99] â†’ r100ï¼ˆç”¨å‰99æ¡é¢„æµ‹ç¬¬100æ¡ï¼‰
       - **å¤§å¹…å‡å°‘è®­ç»ƒæ•°æ®é‡ï¼ŒåŒæ—¶æœ€å¤§åŒ–å†å²ä¿¡æ¯åˆ©ç”¨**
    
    Args:
        raw_data: åŸå§‹æ•°æ®
        one_sample_per_user: æ˜¯å¦æ¯ä¸ªç”¨æˆ·åªç”Ÿæˆæœ€å2ä¸ªæ ·æœ¬ï¼ˆé»˜è®¤Falseï¼‰
        debug: æ˜¯å¦è¾“å‡ºè°ƒè¯•ä¿¡æ¯
        
    Returns:
        è®­ç»ƒæ ·æœ¬åˆ—è¡¨ï¼ˆæŒ‰æ—¶é—´é¡ºåºï¼‰
    """
    all_samples = []
    
    for user_data in raw_data:
        user_profile = user_data.get('user', {}).get('profile', {})
        task_desc = user_data.get('task', {}).get('description', '')
        
        # è·å–å½±è¯„æ•°æ®ï¼ˆå·²æŒ‰æ—¶é—´æ’åºï¼‰
        task_collections = user_data.get('task', {}).get('task_behavior_collections', [])
        
        for collection in task_collections:
            if collection.get('type') != 'movie_review':
                continue
            
            reviews = collection.get('data', [])
            
            if debug:
                print(f"å¤„ç†ç”¨æˆ·: {user_profile.get('name', 'Unknown')}")
                print(f"ä»»åŠ¡æè¿°: {task_desc}")
                print(f"å½±è¯„æ€»æ•°: {len(reviews)}")
            
            if one_sample_per_user:
                # ğŸ”¥ æ–°æ¨¡å¼ï¼šæ¯ä¸ªç”¨æˆ·é€‰æ‹©æœ€å2æ¡ä½œä¸ºé¢„æµ‹ç›®æ ‡
                # ä½¿ç”¨å‰ n-2 æ¡ä½œä¸ºå†å²ï¼Œé¢„æµ‹æœ€å 2 æ¡
                if len(reviews) < 3:
                    if debug:
                        print(f"  âš ï¸ è·³è¿‡è¯¥ç”¨æˆ·ï¼ˆå½±è¯„æ•° < 3ï¼‰")
                    continue
                
                # å‰ n-2 æ¡ä½œä¸ºå…±äº«å†å²
                history_reviews = reviews[:-2]
                # æœ€å2æ¡ä½œä¸ºé¢„æµ‹ç›®æ ‡
                last_two_reviews = reviews[-2:]
                
                # ä¸ºæœ€å2æ¡å½±è¯„åˆ†åˆ«åˆ›å»ºæ ·æœ¬ï¼Œä½†éƒ½ä½¿ç”¨ç›¸åŒçš„å†å²
                for idx, target_review in enumerate(last_two_reviews):
                    # å¯¹äºç¬¬äºŒä¸ªæ ·æœ¬ï¼ˆreviews[-1]ï¼‰ï¼Œå¯ä»¥é¢å¤–åŒ…å«reviews[-2]ä½œä¸ºå†å²
                    if idx == 0:
                        # ç¬¬ä¸€ä¸ªæ ·æœ¬ï¼šåªç”¨å‰ n-2 æ¡ä½œä¸ºå†å²
                        current_history = history_reviews
                    else:
                        # ç¬¬äºŒä¸ªæ ·æœ¬ï¼šç”¨å‰ n-2 æ¡ + reviews[-2] ä½œä¸ºå†å²
                        current_history = history_reviews + [last_two_reviews[0]]
                    
                    sample = {
                        'user_profile': user_profile,
                        'user_hash': user_profile.get('name', 'unknown'),
                        'task_description': task_desc,
                        
                        # å†å²å½±è¯„
                        'history': [
                            {
                                'movie': h.get('continuation_prefix', '').rstrip(': '),
                                'review': h.get('continuation', ''),
                                'timestamp': h.get('timestamp', '')
                            }
                            for h in current_history
                        ],
                        
                        # å½“å‰ç”µå½±ä¿¡æ¯
                        'movie_name': target_review.get('continuation_prefix', '').rstrip(': '),
                        'timestamp': target_review.get('timestamp', ''),
                        
                        # ç›®æ ‡ï¼šè¦é¢„æµ‹çš„å½±è¯„
                        'next_question': target_review.get('continuation', ''),
                        
                        # contextä¿æŒç©ºåˆ—è¡¨ï¼ˆå…¼å®¹ç°æœ‰æ¡†æ¶ï¼‰
                        'context': target_review.get('context', []),
                        
                        # å…ƒæ•°æ®
                        'total_reviews': len(reviews),
                        'history_count': len(current_history),
                        'target_index': len(reviews) - 2 + idx,  # å€’æ•°ç¬¬2ä¸ªæˆ–æœ€å1ä¸ª
                        'raw_review': target_review
                    }
                    
                    all_samples.append(sample)
                
                if debug:
                    print(f"  ç”Ÿæˆ2ä¸ªæ ·æœ¬: {len(history_reviews)}æ¡å…±äº«å†å² â†’ é¢„æµ‹æœ€å2æ¡")
            
            else:
                # åŸæ¨¡å¼ï¼šä¸ºæ¯æ¡å½±è¯„åˆ›å»ºä¸€ä¸ªè®­ç»ƒæ ·æœ¬
                for i, review in enumerate(reviews):
                    # ä¹‹å‰çš„æ‰€æœ‰å½±è¯„ä½œä¸ºå†å²ä¸Šä¸‹æ–‡
                    history_reviews = reviews[:i] if i > 0 else []
                    
                    sample = {
                        'user_profile': user_profile,
                        'user_hash': user_profile.get('name', 'unknown'),
                        'task_description': task_desc,
                        
                        # å†å²å½±è¯„ï¼ˆä½œä¸ºä¸Šä¸‹æ–‡ï¼‰
                        'history': [
                            {
                                'movie': h.get('continuation_prefix', '').rstrip(': '),
                                'review': h.get('continuation', ''),
                                'timestamp': h.get('timestamp', '')
                            }
                            for h in history_reviews
                        ],
                        
                        # å½“å‰ç”µå½±ä¿¡æ¯
                        'movie_name': review.get('continuation_prefix', '').rstrip(': '),
                        'timestamp': review.get('timestamp', ''),
                        
                        # ç›®æ ‡ï¼šè¦é¢„æµ‹çš„å½±è¯„
                        'next_question': review.get('continuation', ''),
                        
                        # contextä¿æŒç©ºåˆ—è¡¨ï¼ˆå…¼å®¹ç°æœ‰æ¡†æ¶ï¼‰
                        'context': review.get('context', []),
                        
                        # åŸå§‹æ•°æ®ï¼ˆç”¨äºè°ƒè¯•ï¼‰
                        'raw_review': review
                    }
                    
                    all_samples.append(sample)
            
            if debug:
                print(f"ç”Ÿæˆæ ·æœ¬æ•°: {len(all_samples)}")
    
    return all_samples


def split_movie_reviews_by_time(
    samples: List[Dict[str, Any]],
    train_ratio: float = 0.7,
    val_ratio: float = 0.15,
    test_ratio: float = 0.15,
    debug: bool = False
) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]], List[Dict[str, Any]]]:
    """
    æŒ‰æ—¶é—´é¡ºåºåˆ’åˆ†è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†
    
    é‡è¦ï¼šä¿æŒæ—¶é—´é¡ºåºï¼Œç”¨æ—©æœŸæ•°æ®è®­ç»ƒï¼ŒåæœŸæ•°æ®æµ‹è¯•
    
    Args:
        samples: æ ·æœ¬åˆ—è¡¨ï¼ˆå·²æŒ‰æ—¶é—´æ’åºï¼‰
        train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
        val_ratio: éªŒè¯é›†æ¯”ä¾‹
        test_ratio: æµ‹è¯•é›†æ¯”ä¾‹
        debug: æ˜¯å¦è¾“å‡ºè°ƒè¯•ä¿¡æ¯
        
    Returns:
        (train_samples, val_samples, test_samples)
    """
    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6, \
        f"æ¯”ä¾‹ä¹‹å’Œå¿…é¡»ä¸º1.0ï¼Œå½“å‰ä¸º {train_ratio + val_ratio + test_ratio}"
    
    total = len(samples)
    train_end = int(total * train_ratio)
    val_end = train_end + int(total * val_ratio)
    
    train_samples = samples[:train_end]
    val_samples = samples[train_end:val_end]
    test_samples = samples[val_end:]
    
    if debug:
        print("=" * 80)
        print("æŒ‰æ—¶é—´é¡ºåºåˆ’åˆ†æ•°æ®é›†:")
        print(f"  æ€»æ ·æœ¬æ•°: {total}")
        print(f"  è®­ç»ƒé›†: {len(train_samples)} ({len(train_samples)/total*100:.1f}%)")
        if train_samples:
            print(f"    æ—¶é—´èŒƒå›´: {train_samples[0].get('timestamp', 'N/A')} -> {train_samples[-1].get('timestamp', 'N/A')}")
        
        print(f"  éªŒè¯é›†: {len(val_samples)} ({len(val_samples)/total*100:.1f}%)")
        if val_samples:
            print(f"    æ—¶é—´èŒƒå›´: {val_samples[0].get('timestamp', 'N/A')} -> {val_samples[-1].get('timestamp', 'N/A')}")
        
        print(f"  æµ‹è¯•é›†: {len(test_samples)} ({len(test_samples)/total*100:.1f}%)")
        if test_samples:
            print(f"    æ—¶é—´èŒƒå›´: {test_samples[0].get('timestamp', 'N/A')} -> {test_samples[-1].get('timestamp', 'N/A')}")
        print("=" * 80)
    
    return train_samples, val_samples, test_samples


# ================================
# æ•°æ®é›†ç±»
# ================================

class MovieReviewDataset(Dataset):
    """
    å½±è¯„æ•°æ®é›†ï¼ˆåŠ¨æ€Paddingç‰ˆæœ¬ï¼‰
    """
    def __init__(self, samples, tokenizer, max_length=4096, use_profile=True, use_history=True, verbose=False):
        self.samples = samples
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.use_profile = use_profile
        self.use_history = use_history
        self.verbose = verbose
        
        # æˆªæ–­ç»Ÿè®¡
        self.truncation_stats = {
            'total_samples': 0,
            'truncated_samples': 0,
            'truncated_history': 0,
        }
        self.first_truncation_logged = False
    
    def build_prompt(self, sample: Dict[str, Any]) -> Tuple[List[Dict[str, str]], str]:
        """
        æ„å»ºå½±è¯„è®­ç»ƒprompt
        
        è¿”å›:
            messages: èŠå¤©æ¶ˆæ¯åˆ—è¡¨
            target_answer: ç›®æ ‡ç­”æ¡ˆï¼ˆè¦é¢„æµ‹çš„å½±è¯„ï¼‰
        """
        parts = []
        
        # 1. ç”¨æˆ·Profile
        if self.use_profile and sample.get('user_profile'):
            profile = sample['user_profile']
            parts.append(f"ç”¨æˆ·: {profile.get('name', 'Unknown')}")
        
        # 2. å†å²å½±è¯„
        if self.use_history and sample.get('history'):
            history = sample['history']
            if history:
                parts.append(f"\nå†å²å½±è¯„è®°å½• ({len(history)}æ¡):")
                for h in history:
                    parts.append(f"  ç”µå½±ã€Š{h['movie']}ã€‹: {h['review']}")
        
        # 3. å½“å‰ç”µå½±
        movie_name = sample.get('movie_name', '')
        parts.append(f"\næ¨¡ä»¿ç”¨æˆ·é£æ ¼ä¸ºç”µå½±ã€Š{movie_name}ã€‹å†™ä¸€æ¡å½±è¯„ï¼š")
        
        system_content = "\n".join(parts)
        
        messages = [
            {'role': 'system', 'content': system_content}
        ]
        
        target_answer = sample.get('next_question', '')
        
        return messages, target_answer
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx):
        sample = self.samples[idx]
        
        # 1. æ„å»ºæ¶ˆæ¯
        messages, target_answer = self.build_prompt(sample)
        
        # 2. ç”Ÿæˆå®Œæ•´æ–‡æœ¬
        full_prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)
        generation_suffix = "<|im_start|>assistant\n"
        full_prompt = full_prompt.strip() + generation_suffix
        im_end_token = "<|im_end|>"
        full_text = full_prompt + target_answer + im_end_token
        
        # 3. å¤„ç†è¶…é•¿æ–‡æœ¬ï¼šåˆ é™¤å†å²è®°å½•
        full_length = len(self.tokenizer.encode(full_text, add_special_tokens=False))
        is_truncated = False
        removed_history = 0
        
        if full_length > self.max_length:
            is_truncated = True
            history = sample.get('history', [])
            
            if history and len(history) > 0:
                reduced_history = history[:]
                while full_length > self.max_length and len(reduced_history) > 0:
                    reduced_history.pop(0)  # åˆ é™¤æœ€æ—§çš„å†å²è®°å½•
                    removed_history += 1
                    
                    # é‡å»ºæ ·æœ¬
                    temp_sample = sample.copy()
                    temp_sample['history'] = reduced_history
                    messages, target_answer = self.build_prompt(temp_sample)
                    
                    # é‡æ–°ç”Ÿæˆ
                    full_prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)
                    full_prompt = full_prompt.strip() + generation_suffix
                    full_text = full_prompt + target_answer + im_end_token
                    full_length = len(self.tokenizer.encode(full_text, add_special_tokens=False))
        
        # æ›´æ–°ç»Ÿè®¡
        self.truncation_stats['total_samples'] += 1
        if is_truncated:
            self.truncation_stats['truncated_samples'] += 1
            self.truncation_stats['truncated_history'] += removed_history
            
            if not self.first_truncation_logged and self.verbose:
                self.first_truncation_logged = True
                print(f"\nâš ï¸  æ ·æœ¬#{idx} è¶…é•¿ï¼Œåˆ é™¤äº† {removed_history} æ¡å†å²è®°å½•")
                print(f"  è°ƒæ•´åé•¿åº¦: {full_length} tokens (max: {self.max_length})\n")
        
        # 4. ç¼–ç ï¼ˆä¸åšpaddingï¼‰
        encoded = self.tokenizer(
            full_text,
            truncation=True,
            max_length=self.max_length,
            padding=False,
            return_tensors='pt'
        )
        
        input_ids = encoded['input_ids'].squeeze()
        attention_mask = encoded['attention_mask'].squeeze()
        
        # 5. è®¡ç®—labels
        prompt_ids = self.tokenizer.encode(full_prompt, add_special_tokens=False)
        actual_prompt_len = len(prompt_ids)
        
        labels = input_ids.clone()
        safe_prompt_len = min(actual_prompt_len, len(input_ids) - 1)
        labels[:safe_prompt_len] = -100
        labels[input_ids == self.tokenizer.pad_token_id] = -100
        
        return {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'labels': labels,
            'actual_length': len(input_ids)
        }


def dynamic_padding_collate_fn(examples, tokenizer):
    """
    åŠ¨æ€Paddingçš„collateå‡½æ•°
    åªpaddingåˆ°batchå†…æœ€é•¿æ ·æœ¬çš„é•¿åº¦
    """
    # æ‰¾åˆ°batchä¸­æœ€é•¿çš„åºåˆ—é•¿åº¦
    max_length_in_batch = max(ex['input_ids'].shape[0] for ex in examples)
    
    # 5%æ¦‚ç‡æ‰“å°batchä¿¡æ¯
    if random.random() < 0.05:
        lengths = [ex['input_ids'].shape[0] for ex in examples]
        print(f"[Batch] Lengths: min={min(lengths)}, max={max_length_in_batch}, avg={sum(lengths)/len(lengths):.0f}")
    
    batch = {}
    padded_input_ids = []
    padded_attention_mask = []
    padded_labels = []
    
    for ex in examples:
        seq_len = ex['input_ids'].shape[0]
        pad_len = max_length_in_batch - seq_len
        
        # Padding
        padded_input_ids.append(
            torch.cat([
                ex['input_ids'],
                torch.full((pad_len,), tokenizer.pad_token_id, dtype=torch.long)
            ])
        )
        
        padded_attention_mask.append(
            torch.cat([
                ex['attention_mask'],
                torch.zeros(pad_len, dtype=torch.long)
            ])
        )
        
        padded_labels.append(
            torch.cat([
                ex['labels'],
                torch.full((pad_len,), -100, dtype=torch.long)
            ])
        )
    
    batch['input_ids'] = torch.stack(padded_input_ids)
    batch['attention_mask'] = torch.stack(padded_attention_mask)
    batch['labels'] = torch.stack(padded_labels)
    
    return batch


# ================================
# åˆ†å¸ƒå¼è®­ç»ƒè¾…åŠ©å‡½æ•°
# ================================

def setup_distributed():
    """åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ"""
    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:
        rank = int(os.environ["RANK"])
        world_size = int(os.environ['WORLD_SIZE'])
        local_rank = int(os.environ['LOCAL_RANK'])
    elif 'SLURM_PROCID' in os.environ:
        rank = int(os.environ['SLURM_PROCID'])
        world_size = int(os.environ['SLURM_NTASKS'])
        local_rank = rank % torch.cuda.device_count()
    else:
        print('æœªæ£€æµ‹åˆ°åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒï¼Œä½¿ç”¨å•å¡è®­ç»ƒ')
        rank = 0
        world_size = 1
        local_rank = 0
    
    torch.cuda.set_device(local_rank)
    
    if world_size > 1:
        dist.init_process_group(
            backend='nccl',
            init_method='env://',
            world_size=world_size,
            rank=rank
        )
    
    return rank, world_size, local_rank


def cleanup_distributed():
    """æ¸…ç†åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ"""
    if dist.is_initialized():
        dist.destroy_process_group()


# ================================
# ä¸»å‡½æ•°
# ================================

def main():
    parser = argparse.ArgumentParser(description='è±†ç“£å½±è¯„æ¨¡å‹ - åˆ†å¸ƒå¼è®­ç»ƒï¼ˆè‡ªåŒ…å«ç‰ˆæœ¬ï¼‰')
    
    # é…ç½®æ–‡ä»¶
    parser.add_argument('--config', type=str, required=True, help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--ablation_config', type=str, required=True,
                       choices=['profile_and_history', 'profile_only', 'history_only', 'baseline'],
                       help='æ¶ˆèå®éªŒé…ç½®')
    
    # æ•°æ®ç›¸å…³
    parser.add_argument('--data_file', type=str, default=None, help='å½±è¯„æ•°æ®JSONæ–‡ä»¶è·¯å¾„ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶ï¼‰')
    parser.add_argument('--val_ratio', type=float, default=None, help='éªŒè¯é›†æ¯”ä¾‹ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶ï¼‰')
    
    # è¾“å‡ºç›®å½•
    parser.add_argument('--output_dir', type=str, default=None, help='æ¨¡å‹è¾“å‡ºç›®å½•')
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--max_epochs', type=int, default=50, help='æœ€å¤§è®­ç»ƒè½®æ¬¡')
    parser.add_argument('--early_stopping_patience', type=int, default=3, help='æ—©åœè€å¿ƒå€¼')
    parser.add_argument('--early_stopping_threshold', type=float, default=0.001, help='æ—©åœé˜ˆå€¼')
    
    # DeepSpeed
    parser.add_argument('--deepspeed', type=str, default=None, help='DeepSpeedé…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--disable_flash_attn', action='store_true', help='ç¦ç”¨FlashAttention 2')
    
    # é‡‡æ ·å‚æ•°
    parser.add_argument('--one_sample_per_user', action='store_true',
                       help='æ¯ä¸ªç”¨æˆ·åªç”Ÿæˆ1ä¸ªæ ·æœ¬ï¼ˆç”¨å‰n-1æ¡å†å²é¢„æµ‹ç¬¬næ¡ï¼‰')
    
    # W&B
    parser.add_argument('--wandb_project', type=str, default='MovieReview', help='Weights & Biasesé¡¹ç›®åç§°')
    parser.add_argument('--wandb_run_name', type=str, default=None, help='Weights & Biasesè¿è¡Œåç§°')
    
    # å…¶ä»–
    parser.add_argument('--local_rank', type=int, default=-1, help='æœ¬åœ°è¿›ç¨‹rank')
    parser.add_argument('--prompt_style', type=str, default='simple', choices=['simple', 'detailed'],
                       help='Prompté£æ ¼ï¼ˆæœ¬ç‰ˆæœ¬åªæ”¯æŒsimpleï¼‰')
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®æ–‡ä»¶
    with open(args.config, 'r', encoding='utf-8') as f:
        config = json.load(f)
    
    # è·å–æ¶ˆèé…ç½®
    ablation_config = config['ablation_configs'][args.ablation_config]
    use_profile = ablation_config.get('use_profile', True)
    use_history = ablation_config.get('use_history', True)
    config_name = ablation_config['name']
    
    # åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ
    rank, world_size, local_rank = setup_distributed()
    is_main_process = (rank == 0)
    
    # é…ç½®W&B
    if args.wandb_project and is_main_process:
        try:
            import wandb
            os.environ['WANDB_PROJECT'] = args.wandb_project
            if args.wandb_run_name:
                os.environ['WANDB_NAME'] = args.wandb_run_name
            print(f"âœ“ å·²å¯ç”¨ W&B ç›‘æ§: {args.wandb_project}")
        except ImportError:
            print("è­¦å‘Š: wandb æœªå®‰è£…")
            args.wandb_project = None
    
    if is_main_process:
        print("=" * 80)
        print("è±†ç“£å½±è¯„æ¨¡å‹ - åˆ†å¸ƒå¼è®­ç»ƒï¼ˆè‡ªåŒ…å«ç‰ˆæœ¬ï¼‰")
        print("=" * 80)
        print(f"World Size: {world_size}, Rank: {rank}, Local Rank: {local_rank}")
        print(f"æ¶ˆèå®éªŒ: {config_name}")
        print(f"ä½¿ç”¨é…ç½®: Profile={use_profile}, History={use_history}")
        if args.one_sample_per_user:
            print(f"é‡‡æ ·æ¨¡å¼: æ¯ç”¨æˆ·ä¸€ä¸ªæ ·æœ¬")
        if args.deepspeed:
            print(f"DeepSpeed: {args.deepspeed}")
        print("=" * 80)
    
    # æ£€æŸ¥FlashAttention
    use_flash_attn = False
    if not args.disable_flash_attn:
        try:
            import flash_attn
            use_flash_attn = True
            if is_main_process:
                print(f"FlashAttention å·²å®‰è£…")
        except ImportError:
            if is_main_process:
                print("FlashAttention æœªå®‰è£…ï¼Œä½¿ç”¨æ ‡å‡†attention")
    
    # éªŒè¯GPU
    if not torch.cuda.is_available():
        print(f"[Rank {rank}] é”™è¯¯: CUDA ä¸å¯ç”¨")
        cleanup_distributed()
        return
    
    if is_main_process:
        print(f"CUDA å¯ç”¨ï¼ŒGPUæ•°é‡: {torch.cuda.device_count()}")
        print(f"å½“å‰GPU: {local_rank} - {torch.cuda.get_device_name(local_rank)}")
    
    # åŠ è½½æ•°æ®
    if is_main_process:
        print("\n" + "=" * 80)
        print("åŠ è½½å½±è¯„æ•°æ®...")
    
    data_file = args.data_file if args.data_file else config['data']['train_path']
    raw_data = load_movie_review_data(data_file)
    all_samples = extract_movie_review_samples(
        raw_data, 
        one_sample_per_user=args.one_sample_per_user,
        debug=is_main_process
    )
    
    if is_main_process:
        print(f"æ•°æ®æ–‡ä»¶: {data_file}")
        print(f"æå–äº† {len(all_samples)} ä¸ªæ ·æœ¬")
    
    # è·å–æ•°æ®åˆ’åˆ†æ¯”ä¾‹
    data_split = config.get('data_split', {})
    train_ratio = data_split.get('train_ratio', 0.7)
    val_ratio_config = data_split.get('val_ratio', 0.15)
    test_ratio = data_split.get('test_ratio', 0.15)
    
    if args.val_ratio is not None:
        val_ratio_config = args.val_ratio
        test_ratio = 1.0 - train_ratio - val_ratio_config
    
    # æŒ‰æ—¶é—´åˆ’åˆ†æ•°æ®é›†
    train_samples, val_samples, test_samples = split_movie_reviews_by_time(
        all_samples,
        train_ratio=train_ratio,
        val_ratio=val_ratio_config,
        test_ratio=test_ratio,
        debug=is_main_process
    )
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    if args.output_dir:
        output_dir = args.output_dir
    else:
        checkpoint_dir = config['model']['checkpoint_dir']
        output_dir = os.path.join(checkpoint_dir, f"MovieReview_{config_name}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    if is_main_process:
        os.makedirs(output_dir, exist_ok=True)
        print(f"è¾“å‡ºç›®å½•: {output_dir}")
        
        # ä¿å­˜æµ‹è¯•é›†
        test_file = os.path.join(output_dir, 'test_samples.json')
        with open(test_file, 'w', encoding='utf-8') as f:
            json.dump(test_samples, f, ensure_ascii=False, indent=2)
        print(f"æµ‹è¯•é›†å·²ä¿å­˜åˆ°: {test_file}")
    
    if world_size > 1:
        dist.barrier()
    
    # åŠ è½½tokenizerå’Œæ¨¡å‹
    model_path = config['model']['path']
    if is_main_process:
        print(f"\nåŠ è½½æ¨¡å‹: {model_path}")
    
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # ğŸ“Š æ•°æ®é•¿åº¦åˆ†æï¼ˆåœ¨åŠ è½½æ¨¡å‹ä¹‹å‰ï¼‰
    train_config = config.get('training', {})
    if is_main_process:
        print("\n" + "=" * 80)
        print("ğŸ“Š åˆ†æè®­ç»ƒæ•°æ®é•¿åº¦åˆ†å¸ƒ...")
        print("=" * 80)
        
        try:
            # é‡‡æ ·åˆ†æï¼ˆä¸è¶…è¿‡500ä¸ªæ ·æœ¬ï¼‰
            sample_size = min(500, len(all_samples))
            analysis_samples = random.sample(all_samples, sample_size) if len(all_samples) > sample_size else all_samples
            
            lengths = []
            failed_count = 0
            for sample in analysis_samples:
                try:
                    # æ„å»ºå®Œæ•´çš„prompt
                    messages, target_answer = build_movie_review_prompt(
                        sample=sample,
                        use_profile=use_profile,
                        use_history=use_history
                    )
                    
                    # è½¬æ¢ä¸ºæ–‡æœ¬
                    full_text = tokenizer.apply_chat_template(
                        messages,
                        tokenize=False,
                        add_generation_prompt=True
                    ) + target_answer
                    
                    # ç¼–ç è·å–é•¿åº¦
                    token_ids = tokenizer.encode(full_text, add_special_tokens=True)
                    lengths.append(len(token_ids))
                except Exception as e:
                    failed_count += 1
                    if failed_count <= 3:  # åªæ‰“å°å‰3ä¸ªé”™è¯¯
                        print(f"  æ ·æœ¬åˆ†æå¤±è´¥: {type(e).__name__}: {str(e)[:100]}")
                    continue
            
            if lengths:
                import numpy as np
                lengths_array = np.array(lengths)
                
                max_length = int(np.max(lengths_array))
                min_length = int(np.min(lengths_array))
                mean_length = float(np.mean(lengths_array))
                median_length = float(np.median(lengths_array))
                percentile_90 = float(np.percentile(lengths_array, 90))
                percentile_95 = float(np.percentile(lengths_array, 95))
                percentile_99 = float(np.percentile(lengths_array, 99))
                
                print(f"åˆ†æäº† {len(lengths)}/{len(all_samples)} ä¸ªæ ·æœ¬:")
                print(f"  æœ€å°é•¿åº¦: {min_length}")
                print(f"  æœ€å¤§é•¿åº¦: {max_length}")
                print(f"  å¹³å‡é•¿åº¦: {mean_length:.0f}")
                print(f"  ä¸­ä½æ•°é•¿åº¦: {median_length:.0f}")
                print(f"  90åˆ†ä½æ•°é•¿åº¦: {percentile_90:.0f}")
                print(f"  95åˆ†ä½æ•°é•¿åº¦: {percentile_95:.0f}")
                print(f"  99åˆ†ä½æ•°é•¿åº¦: {percentile_99:.0f}")
                
                # ä¸é…ç½®çš„max_lengthå¯¹æ¯”
                configured_max_length = train_config.get('max_length', 4096)
                print(f"\né…ç½®çš„ max_length: {configured_max_length}")
                
                exceeds_count = np.sum(lengths_array > configured_max_length)
                print(f"è¶…è¿‡ max_length çš„æ ·æœ¬æ•°: {exceeds_count} ({exceeds_count/len(lengths)*100:.1f}%)")
                
                # ç»™å‡ºå»ºè®®
                print(f"\nå»ºè®®:")
                if percentile_95 > configured_max_length:
                    print(f"  è­¦å‘Š: 95%çš„æ•°æ®è¶…è¿‡é…ç½®çš„max_lengthï¼Œå¯èƒ½å¯¼è‡´å¤§é‡æˆªæ–­")
                    print(f"  å»ºè®®è°ƒæ•´ max_length è‡³å°‘åˆ° {int(percentile_95)}")
                elif percentile_95 < configured_max_length * 0.7:
                    print(f"  æç¤º: 95%çš„æ•°æ®é•¿åº¦è¿œå°äºmax_lengthï¼Œå¯ä»¥è€ƒè™‘é™ä½ä»¥èŠ‚çœæ˜¾å­˜")
                else:
                    print(f"  âœ“ max_length è®¾ç½®åˆç†")
                print("=" * 80 + "\n")
            else:
                print(f"è­¦å‘Š: æ— æ³•åˆ†ææ ·æœ¬é•¿åº¦ (æˆåŠŸ: 0/{sample_size}, å¤±è´¥: {failed_count})")
                print("=" * 80 + "\n")
        
        except Exception as e:
            print(f"æ•°æ®é•¿åº¦åˆ†æå¤±è´¥: {e}")
            print("=" * 80 + "\n")
    
    # åŠ è½½æ¨¡å‹
    model_kwargs = {
        'torch_dtype': torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,
        'trust_remote_code': True,
    }
    
    if use_flash_attn:
        model_kwargs['attn_implementation'] = 'flash_attention_2'
    
    try:
        model = AutoModelForCausalLM.from_pretrained(model_path, **model_kwargs)
        if is_main_process:
            print(f"âœ“ æ¨¡å‹å·²åŠ è½½")
    except Exception as e:
        if is_main_process:
            print(f"åŠ è½½å¤±è´¥: {e}")
            print("å›é€€åˆ°æ ‡å‡†attention...")
        model_kwargs.pop('attn_implementation', None)
        model = AutoModelForCausalLM.from_pretrained(model_path, **model_kwargs)
        use_flash_attn = False
    
    # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
    if hasattr(model, 'gradient_checkpointing_enable'):
        model.gradient_checkpointing_enable()
        if is_main_process:
            print("âœ“ æ¢¯åº¦æ£€æŸ¥ç‚¹å·²å¯ç”¨")
    
    model = model.to(local_rank)
    
    # åˆ›å»ºæ•°æ®é›†
    if is_main_process:
        print("\nåˆ›å»ºè®­ç»ƒæ•°æ®é›†...")
    
    train_dataset = MovieReviewDataset(
        samples=train_samples,
        tokenizer=tokenizer,
        max_length=train_config.get('max_length', 4096),
        use_profile=use_profile,
        use_history=use_history,
        verbose=is_main_process
    )
    
    val_dataset = None
    if val_samples:
        val_dataset = MovieReviewDataset(
            samples=val_samples,
            tokenizer=tokenizer,
            max_length=train_config.get('max_length', 4096),
            use_profile=use_profile,
            use_history=use_history,
            verbose=False
        )
    
    # Tokené•¿åº¦ç»Ÿè®¡
    if is_main_process:
        print("\n" + "=" * 80)
        print("ğŸ“Š Token é•¿åº¦ç»Ÿè®¡ï¼ˆè®­ç»ƒé›†ï¼‰")
        print("=" * 80)
        
        token_lengths = []
        for i in range(min(len(train_dataset), 1000)):  # åªç»Ÿè®¡å‰1000ä¸ª
            sample = train_dataset[i]
            token_lengths.append(len(sample['input_ids']))
        
        if token_lengths:
            import numpy as np
            max_length_config = train_config.get('max_length', 4096)
            
            print(f"æ ·æœ¬æ€»æ•°: {len(train_dataset)}")
            print(f"é…ç½®çš„ max_length: {max_length_config}")
            print(f"\nToken é•¿åº¦åˆ†å¸ƒï¼ˆå‰1000ä¸ªæ ·æœ¬ï¼‰:")
            print(f"  æœ€å°: {min(token_lengths)}, æœ€å¤§: {max(token_lengths)}, å¹³å‡: {np.mean(token_lengths):.1f}")
            print(f"  ä¸­ä½æ•°: {np.median(token_lengths):.0f}")
            print(f"  75%: {np.percentile(token_lengths, 75):.0f}")
            print(f"  95%: {np.percentile(token_lengths, 95):.0f}")
            
            exceed_count = sum(1 for l in token_lengths if l > max_length_config)
            if exceed_count > 0:
                print(f"\nâš ï¸ {exceed_count} ä¸ªæ ·æœ¬è¶…è¿‡ max_length")
            else:
                print(f"\nâœ… æ‰€æœ‰æ ·æœ¬éƒ½åœ¨ max_length èŒƒå›´å†…")
        
        print("=" * 80)
    
    # æ•°æ®æ•´ç†å™¨
    def collate_fn(examples):
        return dynamic_padding_collate_fn(examples, tokenizer)
    
    # è®¡ç®—è®­ç»ƒæ­¥æ•°
    batch_size = train_config.get('batch_size', 2)
    gradient_accumulation_steps = train_config.get('gradient_accumulation_steps', 8)
    steps_per_epoch = len(train_dataset) // (world_size * batch_size * gradient_accumulation_steps)
    eval_steps = max(1, steps_per_epoch // 2) if val_dataset else None
    save_steps = train_config.get('save_steps', 500)
    
    # è°ƒæ•´save_steps
    if val_dataset and eval_steps and save_steps % eval_steps != 0:
        save_steps = ((save_steps + eval_steps - 1) // eval_steps) * eval_steps
        if is_main_process:
            print(f"è°ƒæ•´ save_steps ä¸º {save_steps}")
    
    # è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=args.max_epochs,
        per_device_train_batch_size=batch_size,
        per_device_eval_batch_size=train_config.get('eval_batch_size', 2),
        gradient_accumulation_steps=gradient_accumulation_steps,
        learning_rate=train_config.get('learning_rate', 1e-5),
        weight_decay=train_config.get('weight_decay', 0.01),
        warmup_steps=train_config.get('warmup_steps', 100),
        logging_steps=train_config.get('logging_steps', 10),
        save_steps=save_steps,
        eval_steps=eval_steps,
        eval_strategy="steps" if val_dataset else "no",
        save_total_limit=train_config.get('save_total_limit', 3),
        load_best_model_at_end=True if val_dataset else False,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        fp16=False,
        bf16=True,
        dataloader_pin_memory=False,
        gradient_checkpointing=True,
        optim="adamw_torch",
        max_grad_norm=0.5,
        report_to="wandb" if args.wandb_project else "none",
        local_rank=local_rank,
        ddp_find_unused_parameters=False,
        ddp_backend="nccl",
        dataloader_num_workers=2,
        save_on_each_node=False,
        logging_first_step=True,
        deepspeed=args.deepspeed,
    )
    
    # æ—©åœå›è°ƒ
    callbacks = []
    if val_dataset:
        early_stopping = EarlyStoppingCallback(
            early_stopping_patience=args.early_stopping_patience,
            early_stopping_threshold=args.early_stopping_threshold
        )
        callbacks.append(early_stopping)
    
    # åˆ›å»ºTrainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        data_collator=collate_fn,
        processing_class=tokenizer,
        callbacks=callbacks,
    )
    
    # å¼€å§‹è®­ç»ƒ
    if is_main_process:
        print("\n" + "=" * 80)
        print("å¼€å§‹è®­ç»ƒ")
        print("=" * 80)
        print(f"è®­ç»ƒæ ·æœ¬: {len(train_dataset)}")
        print(f"éªŒè¯æ ·æœ¬: {len(val_dataset) if val_dataset else 0}")
        print(f"æµ‹è¯•æ ·æœ¬: {len(test_samples)}")
        print(f"æœ‰æ•ˆbatch size: {batch_size * gradient_accumulation_steps * world_size}")
        print(f"é¢„è®¡æ¯epochæ­¥æ•°: {steps_per_epoch}")
        print("=" * 80)
    
    trainer.train()
    
    # ä¿å­˜æ¨¡å‹
    if is_main_process:
        print(f"\nä¿å­˜æ¨¡å‹åˆ° {output_dir}")
        trainer.save_model()
        tokenizer.save_pretrained(output_dir)
        
        # ä¿å­˜é…ç½®
        config_info = {
            'data_file': data_file,
            'ablation_config': args.ablation_config,
            'config_name': config_name,
            'use_profile': use_profile,
            'use_history': use_history,
            'one_sample_per_user': args.one_sample_per_user,
            'train_samples': len(train_samples),
            'val_samples': len(val_samples),
            'test_samples': len(test_samples),
        }
        with open(os.path.join(output_dir, 'training_config.json'), 'w', encoding='utf-8') as f:
            json.dump(config_info, f, indent=2, ensure_ascii=False)
        
        print("âœ“ è®­ç»ƒå®Œæˆï¼")
        print(f"âœ“ æ¨¡å‹å·²ä¿å­˜åˆ°: {output_dir}")
    
    # ç­‰å¾…æ‰€æœ‰è¿›ç¨‹å®Œæˆ
    if world_size > 1:
        dist.barrier()
    
    cleanup_distributed()


if __name__ == '__main__':
    main()
