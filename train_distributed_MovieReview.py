"""
åˆ†å¸ƒå¼è®­ç»ƒè„šæœ¬ - è±†ç“£å½±è¯„æ¨¡å‹ï¼ˆFlashAttention 2 + åŠ¨æ€Batch Paddingï¼‰

ç”¨äºè®­ç»ƒç”¨æˆ·å½±è¯„é£æ ¼æ¨¡æ‹Ÿæ¨¡å‹

ä½¿ç”¨æ–¹æ³•ï¼š
# å•å¡è®­ç»ƒ
python train_distributed_MovieReview.py \
    --data_file movie_review_data.json \
    --output_dir outputs/movie_review_model

# å¤šå¡è®­ç»ƒ
torchrun --nproc_per_node=4 train_distributed_MovieReview.py \
    --data_file movie_review_data.json \
    --output_dir outputs/movie_review_model_4gpu
"""
import json
import argparse
import os
import sys
from pathlib import Path
import random
import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler

from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    EarlyStoppingCallback,
    TrainingArguments,
    Trainer
)
from typing import List, Dict, Any, Optional, Tuple
import torch.nn as nn
from torch.utils.data import Dataset

"""
è±†ç“£å½±è¯„æ•°æ®åŠ è½½å™¨
ä¸“é—¨ç”¨äºå¤„ç†ç”µå½±è¯„è®ºæ•°æ®ï¼ŒæŒ‰æ—¶é—´é¡ºåºåˆ’åˆ†è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†
"""
from datetime import datetime


def load_movie_review_data(file_path: str) -> List[Dict[str, Any]]:
    """
    åŠ è½½è±†ç“£å½±è¯„æ•°æ®
    
    Args:
        file_path: JSONæ–‡ä»¶è·¯å¾„
        
    Returns:
        è§£æåçš„æ•°æ®åˆ—è¡¨
    """
    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # æ”¯æŒå•ä¸ªç”¨æˆ·æˆ–å¤šç”¨æˆ·æ•°æ®
    if isinstance(data, dict):
        data = [data]
    
    return data


def extract_movie_review_samples(
    raw_data: List[Dict[str, Any]], 
    one_sample_per_user: bool = False,
    debug: bool = False
) -> List[Dict[str, Any]]:
    """
    å°†åŸå§‹å½±è¯„æ•°æ®è½¬æ¢ä¸ºè®­ç»ƒæ ·æœ¬æ ¼å¼
    
    ä¸¤ç§æ¨¡å¼ï¼š
    1. one_sample_per_user=Falseï¼ˆé»˜è®¤ï¼‰ï¼šæ¯æ¡å½±è¯„è½¬æ¢ä¸ºä¸€ä¸ªæ ·æœ¬
       - ç”¨æˆ·æœ‰100æ¡å½±è¯„ â†’ ç”Ÿæˆ100ä¸ªæ ·æœ¬
       - æ ·æœ¬1: [] â†’ r1, æ ·æœ¬2: [r1] â†’ r2, ..., æ ·æœ¬100: [r1..r99] â†’ r100
    
    2. one_sample_per_user=Trueï¼šæ¯ä¸ªç”¨æˆ·åªç”Ÿæˆ1ä¸ªæ ·æœ¬
       - ç”¨æˆ·æœ‰100æ¡å½±è¯„ â†’ ç”Ÿæˆ1ä¸ªæ ·æœ¬
       - æ ·æœ¬: [r1..r99] â†’ r100ï¼ˆç”¨å‰n-1æ¡é¢„æµ‹ç¬¬næ¡ï¼‰
       - **å¤§å¹…å‡å°‘è®­ç»ƒæ•°æ®é‡ï¼Œç¼©çŸ­è®­ç»ƒæ—¶é—´**
    
    Args:
        raw_data: åŸå§‹æ•°æ®
        one_sample_per_user: æ˜¯å¦æ¯ä¸ªç”¨æˆ·åªç”Ÿæˆä¸€ä¸ªæ ·æœ¬ï¼ˆé»˜è®¤Falseï¼‰
        debug: æ˜¯å¦è¾“å‡ºè°ƒè¯•ä¿¡æ¯
        
    Returns:
        è®­ç»ƒæ ·æœ¬åˆ—è¡¨ï¼ˆæŒ‰æ—¶é—´é¡ºåºï¼‰
    """
    all_samples = []
    
    for user_data in raw_data:
        user_profile = user_data.get('user', {}).get('profile', {})
        task_desc = user_data.get('task', {}).get('description', '')
        
        # è·å–å½±è¯„æ•°æ®ï¼ˆå·²æŒ‰æ—¶é—´æ’åºï¼‰
        task_collections = user_data.get('task', {}).get('task_behavior_collections', [])
        
        for collection in task_collections:
            if collection.get('type') != 'movie_review':
                continue
            
            reviews = collection.get('data', [])
            
            if debug:
                print(f"å¤„ç†ç”¨æˆ·: {user_profile.get('name', 'Unknown')}")
                print(f"ä»»åŠ¡æè¿°: {task_desc}")
                print(f"å½±è¯„æ€»æ•°: {len(reviews)}")
            
            if one_sample_per_user:
                # ğŸ”¥ æ–°æ¨¡å¼ï¼šæ¯ä¸ªç”¨æˆ·åªç”Ÿæˆ1ä¸ªæ ·æœ¬
                # ä½¿ç”¨å‰ n-1 æ¡ä½œä¸ºå†å²ï¼Œé¢„æµ‹ç¬¬ n æ¡
                if len(reviews) < 2:
                    if debug:
                        print(f"  âš ï¸ è·³è¿‡è¯¥ç”¨æˆ·ï¼ˆå½±è¯„æ•° < 2ï¼‰")
                    continue
                
                # æ‰€æœ‰å½±è¯„é™¤æœ€åä¸€æ¡ä½œä¸ºå†å²
                history_reviews = reviews[:-1]
                last_review = reviews[-1]
                
                sample = {
                    'user_profile': user_profile,
                    'user_hash': user_profile.get('name', 'unknown'),
                    'task_description': task_desc,
                    
                    # å†å²å½±è¯„ï¼ˆå‰ n-1 æ¡ï¼‰
                    'history': [
                        {
                            'movie': h.get('continuation_prefix', '').rstrip(': '),
                            'review': h.get('continuation', ''),
                            'timestamp': h.get('timestamp', '')
                        }
                        for h in history_reviews
                    ],
                    
                    # å½“å‰ç”µå½±ä¿¡æ¯ï¼ˆç¬¬ n æ¡ï¼‰
                    'movie_name': last_review.get('continuation_prefix', '').rstrip(': '),
                    'timestamp': last_review.get('timestamp', ''),
                    
                    # ç›®æ ‡ï¼šè¦é¢„æµ‹çš„å½±è¯„ï¼ˆç¬¬ n æ¡ï¼‰
                    'next_question': last_review.get('continuation', ''),
                    
                    # contextä¿æŒç©ºåˆ—è¡¨ï¼ˆå…¼å®¹ç°æœ‰æ¡†æ¶ï¼‰
                    'context': last_review.get('context', []),
                    
                    # å…ƒæ•°æ®
                    'total_reviews': len(reviews),
                    'history_count': len(history_reviews),
                    'raw_review': last_review
                }
                
                all_samples.append(sample)
                
                if debug:
                    print(f"  ç”Ÿæˆ1ä¸ªæ ·æœ¬: {len(history_reviews)}æ¡å†å² â†’ é¢„æµ‹ç¬¬{len(reviews)}æ¡")
            
            else:
                # åŸæ¨¡å¼ï¼šä¸ºæ¯æ¡å½±è¯„åˆ›å»ºä¸€ä¸ªè®­ç»ƒæ ·æœ¬
                for i, review in enumerate(reviews):
                    # ä¹‹å‰çš„æ‰€æœ‰å½±è¯„ä½œä¸ºå†å²ä¸Šä¸‹æ–‡
                    history_reviews = reviews[:i] if i > 0 else []
                    
                    sample = {
                        'user_profile': user_profile,
                        'user_hash': user_profile.get('name', 'unknown'),
                        'task_description': task_desc,
                        
                        # å†å²å½±è¯„ï¼ˆä½œä¸ºä¸Šä¸‹æ–‡ï¼‰
                        'history': [
                            {
                                'movie': h.get('continuation_prefix', '').rstrip(': '),
                                'review': h.get('continuation', ''),
                                'timestamp': h.get('timestamp', '')
                            }
                            for h in history_reviews
                        ],
                        
                        # å½“å‰ç”µå½±ä¿¡æ¯
                        'movie_name': review.get('continuation_prefix', '').rstrip(': '),
                        'timestamp': review.get('timestamp', ''),
                        
                        # ç›®æ ‡ï¼šè¦é¢„æµ‹çš„å½±è¯„
                        'next_question': review.get('continuation', ''),
                        
                        # contextä¿æŒç©ºåˆ—è¡¨ï¼ˆå…¼å®¹ç°æœ‰æ¡†æ¶ï¼‰
                        'context': review.get('context', []),
                        
                        # åŸå§‹æ•°æ®ï¼ˆç”¨äºè°ƒè¯•ï¼‰
                        'raw_review': review
                    }
                    
                    all_samples.append(sample)
            
            if debug:
                print(f"ç”Ÿæˆæ ·æœ¬æ•°: {len(all_samples)}")
    
    return all_samples


def split_movie_reviews_by_time(
    samples: List[Dict[str, Any]],
    train_ratio: float = 0.7,
    val_ratio: float = 0.15,
    test_ratio: float = 0.15,
    debug: bool = False
) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]], List[Dict[str, Any]]]:
    """
    æŒ‰æ—¶é—´é¡ºåºåˆ’åˆ†è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†
    
    é‡è¦ï¼šä¿æŒæ—¶é—´é¡ºåºï¼Œç”¨æ—©æœŸæ•°æ®è®­ç»ƒï¼ŒåæœŸæ•°æ®æµ‹è¯•
    
    Args:
        samples: æ ·æœ¬åˆ—è¡¨ï¼ˆå·²æŒ‰æ—¶é—´æ’åºï¼‰
        train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
        val_ratio: éªŒè¯é›†æ¯”ä¾‹
        test_ratio: æµ‹è¯•é›†æ¯”ä¾‹
        debug: æ˜¯å¦è¾“å‡ºè°ƒè¯•ä¿¡æ¯
        
    Returns:
        (train_samples, val_samples, test_samples)
    """
    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6, \
        f"æ¯”ä¾‹ä¹‹å’Œå¿…é¡»ä¸º1.0ï¼Œå½“å‰ä¸º {train_ratio + val_ratio + test_ratio}"
    
    total = len(samples)
    train_end = int(total * train_ratio)
    val_end = train_end + int(total * val_ratio)
    
    train_samples = samples[:train_end]
    val_samples = samples[train_end:val_end]
    test_samples = samples[val_end:]
    
    if debug:
        print("=" * 80)
        print("æŒ‰æ—¶é—´é¡ºåºåˆ’åˆ†æ•°æ®é›†:")
        print(f"  æ€»æ ·æœ¬æ•°: {total}")
        print(f"  è®­ç»ƒé›†: {len(train_samples)} ({len(train_samples)/total*100:.1f}%)")
        if train_samples:
            print(f"    æ—¶é—´èŒƒå›´: {train_samples[0].get('timestamp', 'N/A')} -> {train_samples[-1].get('timestamp', 'N/A')}")
        
        print(f"  éªŒè¯é›†: {len(val_samples)} ({len(val_samples)/total*100:.1f}%)")
        if val_samples:
            print(f"    æ—¶é—´èŒƒå›´: {val_samples[0].get('timestamp', 'N/A')} -> {val_samples[-1].get('timestamp', 'N/A')}")
        
        print(f"  æµ‹è¯•é›†: {len(test_samples)} ({len(test_samples)/total*100:.1f}%)")
        if test_samples:
            print(f"    æ—¶é—´èŒƒå›´: {test_samples[0].get('timestamp', 'N/A')} -> {test_samples[-1].get('timestamp', 'N/A')}")
        print("=" * 80)
    
    return train_samples, val_samples, test_samples


def add_cumulative_history_to_samples(
    samples: List[Dict[str, Any]]
) -> List[Dict[str, Any]]:
    """
    ä¸ºæ¯ä¸ªæ ·æœ¬æ·»åŠ ç´¯ç§¯çš„å†å²ä¿¡æ¯
    
    è¿™ä¸ªå‡½æ•°ç¡®ä¿æ¯ä¸ªæ ·æœ¬çš„historyå­—æ®µåŒ…å«äº†ä¹‹å‰æ‰€æœ‰çš„å½±è¯„
    ï¼ˆæ•°æ®åŠ è½½æ—¶å·²ç»å¤„ç†ï¼Œè¿™é‡Œåªæ˜¯ä¿æŒæ¥å£å…¼å®¹ï¼‰
    
    Args:
        samples: æ ·æœ¬åˆ—è¡¨
        
    Returns:
        å¤„ç†åçš„æ ·æœ¬åˆ—è¡¨
    """
    # å½±è¯„æ•°æ®åœ¨extractæ—¶å·²ç»æ·»åŠ äº†historyï¼Œè¿™é‡Œç›´æ¥è¿”å›
    return samples


def format_movie_review_prompt(
    sample: Dict[str, Any],
    use_profile: bool = True,
    use_history: bool = True,
    style: str = 'simple'
) -> str:
    """
    æ ¼å¼åŒ–å½±è¯„æ ·æœ¬ä¸ºè®­ç»ƒprompt
    
    Args:
        sample: æ ·æœ¬æ•°æ®
        use_profile: æ˜¯å¦ä½¿ç”¨ç”¨æˆ·profile
        use_history: æ˜¯å¦ä½¿ç”¨å†å²å½±è¯„
        style: prompté£æ ¼ ('simple' æˆ– 'detailed')
        
    Returns:
        æ ¼å¼åŒ–åçš„promptå­—ç¬¦ä¸²
    """
    parts = []
    
    # 1. ç”¨æˆ·Profile
    if use_profile and sample.get('user_profile'):
        profile = sample['user_profile']
        if style == 'simple':
            parts.append(f"[USER_PROFILE] ç”¨æˆ·: {profile.get('name', 'Unknown')}")
        else:
            parts.append("=== ç”¨æˆ·ä¿¡æ¯ ===")
            parts.append(f"ç”¨æˆ·å: {profile.get('name', 'Unknown')}")
            if sample.get('task_description'):
                parts.append(f"ä»»åŠ¡: {sample['task_description']}")
        parts.append("")
    
    # 2. å†å²å½±è¯„
    if use_history and sample.get('history'):
        history = sample['history']
        if style == 'simple':
            parts.append(f"[HISTORY] å†å²å½±è¯„ ({len(history)}æ¡):")
            for h in history[-10:]:  # åªæ˜¾ç¤ºæœ€è¿‘10æ¡
                parts.append(f"  {h['movie']}: {h['review']}")
        else:
            parts.append("=== å†å²å½±è¯„ ===")
            for i, h in enumerate(history[-10:], 1):
                parts.append(f"{i}. {h['movie']} ({h['timestamp']})")
                parts.append(f"   è¯„è®º: {h['review']}")
        parts.append("")
    
    # 3. å½“å‰ç”µå½±
    movie_name = sample.get('movie_name', '')
    if style == 'simple':
        parts.append(f"[MOVIE] {movie_name}:")
    else:
        parts.append("=== å½“å‰ç”µå½± ===")
        parts.append(f"ç”µå½±: {movie_name}")
        if sample.get('timestamp'):
            parts.append(f"æ—¶é—´: {sample['timestamp']}")
        parts.append("\nè¯·å†™å‡ºè¿™éƒ¨ç”µå½±çš„å½±è¯„ï¼š")
    
    return "\n".join(parts)


if __name__ == '__main__':
    # æµ‹è¯•ä»£ç 
    import sys
    
    if len(sys.argv) > 1:
        test_file = sys.argv[1]
    else:
        print("ç”¨æ³•: python data_loader_movie_review.py <json_file>")
        sys.exit(1)
    
    print("åŠ è½½æ•°æ®...")
    data = load_movie_review_data(test_file)
    
    print("æå–æ ·æœ¬...")
    samples = extract_movie_review_samples(data, debug=True)
    
    print("\nåˆ’åˆ†æ•°æ®é›†...")
    train, val, test = split_movie_reviews_by_time(samples, debug=True)
    
    print("\nç¤ºä¾‹æ ·æœ¬:")
    if train:
        print("\nè®­ç»ƒé›†ç¬¬1ä¸ªæ ·æœ¬:")
        print(format_movie_review_prompt(train[0], style='detailed'))
        print(f"\nç›®æ ‡è¾“å‡º: {train[0]['next_question']}")
    
    if test:
        print("\n" + "="*80)
        print("æµ‹è¯•é›†ç¬¬1ä¸ªæ ·æœ¬:")
        print(format_movie_review_prompt(test[0], style='detailed'))
        print(f"\nç›®æ ‡è¾“å‡º: {test[0]['next_question']}")


"""
æ¶ˆèå®éªŒè®­ç»ƒè„šæœ¬ï¼ˆå¸¦æ—©åœæœºåˆ¶ + åŠ¨æ€Batch Paddingä¼˜åŒ–ï¼‰
å…³é”®ä¼˜åŒ–ï¼šä¸å†å°†batchå†…æ‰€æœ‰æ ·æœ¬paddingåˆ°å›ºå®šmax_lengthï¼Œ
è€Œæ˜¯åŠ¨æ€paddingåˆ°batchå†…æœ€é•¿æ ·æœ¬çš„å®é™…é•¿åº¦ï¼Œå¤§å¹…èŠ‚çœæ˜¾å­˜ã€‚
"""


def split_train_val(samples, val_ratio=0.15, seed=42):
    """
    åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼ˆç”¨æˆ·å†…åˆ’åˆ†ï¼Œä¿è¯æ¯ä¸ªç”¨æˆ·åœ¨è®­ç»ƒå’ŒéªŒè¯é›†éƒ½æœ‰æ ·æœ¬ï¼‰
    
    ç­–ç•¥ï¼šå¯¹æ¯ä¸ªç”¨æˆ·çš„æ ·æœ¬è¿›è¡Œéšæœºæ‰“ä¹±åæŒ‰æ¯”ä¾‹åˆ’åˆ†
    - é€‚ç”¨åœºæ™¯ï¼šæµ‹è¯•é›†ä¸­çš„ç”¨æˆ·ä¹Ÿå‡ºç°åœ¨è®­ç»ƒé›†ä¸­
    - ç›®æ ‡ï¼šå­¦ä¹ åŸºäºç”¨æˆ·å·²æœ‰å¯¹è¯é¢„æµ‹æ–°å¯¹è¯ï¼ˆç”¨æˆ·å†…æ³›åŒ–ï¼‰
    
    Args:
        samples: æ‰€æœ‰è®­ç»ƒæ ·æœ¬
        val_ratio: éªŒè¯é›†æ¯”ä¾‹ï¼ˆé»˜è®¤0.15ï¼Œå³15%ï¼‰
        seed: éšæœºç§å­
    
    Returns:
        (train_samples, val_samples)
    """
    random.seed(seed)
    
    # æŒ‰ç”¨æˆ·åˆ†ç»„
    user_samples = {}
    for sample in samples:
        user_hash = sample['user_hash']
        if user_hash not in user_samples:
            user_samples[user_hash] = []
        user_samples[user_hash].append(sample)
    
    train_samples = []
    val_samples = []
    
    # å¯¹æ¯ä¸ªç”¨æˆ·çš„æ ·æœ¬è¿›è¡Œåˆ’åˆ†
    for user_hash, user_data in user_samples.items():
        # éšæœºæ‰“ä¹±è¯¥ç”¨æˆ·çš„æ ·æœ¬
        random.shuffle(user_data)
        
        # è®¡ç®—åˆ’åˆ†ç‚¹ï¼š(1 - val_ratio) çš„æ ·æœ¬ç”¨äºè®­ç»ƒ
        split_idx = int(len(user_data) * (1 - val_ratio))
        
        # ç¡®ä¿è‡³å°‘æœ‰1ä¸ªæ ·æœ¬åœ¨è®­ç»ƒé›†ï¼ˆå¦‚æœè¯¥ç”¨æˆ·åªæœ‰1ä¸ªæ ·æœ¬ï¼Œå…¨éƒ¨ç»™è®­ç»ƒé›†ï¼‰
        if split_idx == 0 and len(user_data) > 0:
            split_idx = 1
        
        # åˆ’åˆ†
        train_samples.extend(user_data[:split_idx])
        val_samples.extend(user_data[split_idx:])
    
    return train_samples, val_samples


def add_history_to_samples(train_samples, all_samples):
    """ä¸ºæ¯ä¸ªæ ·æœ¬æ·»åŠ å†å²ä¿¡æ¯ï¼ˆåªåŒ…å«ç”¨æˆ·çš„é—®é¢˜ï¼Œä¸åŒ…å«assistantå†…å®¹ï¼‰"""
    samples_with_history = []
    for sample in train_samples:
        user_hash = sample['user_hash']
        history = get_user_only_history(
            all_samples, 
            user_hash,
            current_sample=sample,
            current_context=sample.get('context'),
            max_history=15,
            use_cache=True
        )
        sample['history'] = history
        samples_with_history.append(sample)
    return samples_with_history


class DynamicPaddingDataset(Dataset):
    """
    ä¼˜åŒ–ç‰ˆæ•°æ®é›†ï¼šä¸åšpaddingï¼Œè¿”å›åŸå§‹é•¿åº¦çš„tensor
    paddingå°†åœ¨collate_fnä¸­åŠ¨æ€è¿›è¡Œ
    
    æ³¨æ„ï¼šMovieReviewDataset ä¼šè¦†ç›– format_prompt æ–¹æ³•
    """
    def __init__(self, samples, tokenizer, max_length=32768, use_profile=True, use_history=True, use_context=True, verbose=False, use_detailed_template=True, max_context_turns=15, template_filename=None):
        self.samples = samples
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.use_profile = use_profile
        self.use_history = use_history
        self.use_context = use_context
        self.use_detailed_template = use_detailed_template
        self.max_context_turns = max_context_turns
        self.template_filename = template_filename
        self.verbose = verbose
        
        # æˆªæ–­ç»Ÿè®¡
        self.truncation_stats = {
            'total_samples': 0,
            'truncated_samples': 0,
            'truncated_turns': 0,
            'total_history_items': 0,
            'truncated_history_items': 0,
            'samples_with_history': 0,
            'samples_with_history_truncated': 0
        }
        
        self.first_truncation_logged = False
    
    def build_movie_review_prompt(self, sample: Dict[str, Any]) -> Tuple[List[Dict[str, str]], str]:
        """
        æ„å»ºå½±è¯„è®­ç»ƒpromptï¼ˆç®€æ´æ ¼å¼ï¼‰
        
        è¿”å›:
            messages: èŠå¤©æ¶ˆæ¯åˆ—è¡¨
            target_answer: ç›®æ ‡ç­”æ¡ˆï¼ˆè¦é¢„æµ‹çš„å½±è¯„ï¼‰
        """
        parts = []
        
        # 1. ç”¨æˆ·Profile
        if self.use_profile and sample.get('user_profile'):
            profile = sample['user_profile']
            parts.append(f"ç”¨æˆ·: {profile.get('name', 'Unknown')}")
        
        # 2. å†å²å½±è¯„
        if self.use_history and sample.get('history'):
            history = sample['history']
            if history:
                parts.append(f"\nå†å²å½±è¯„è®°å½• ({len(history)}æ¡):")
                for h in history:
                    parts.append(f"  ç”µå½±ã€Š{h['movie']}ã€‹: {h['review']}")
        
        # 3. å½“å‰ç”µå½±
        movie_name = sample.get('movie_name', '')
        parts.append(f"\næ¨¡ä»¿ç”¨æˆ·é£æ ¼ä¸ºç”µå½±ã€Š{movie_name}ã€‹å†™ä¸€æ¡å½±è¯„ï¼š")
        
        system_content = "\n".join(parts)
        
        messages = [
            {'role': 'system', 'content': system_content}
        ]
        
        target_answer = sample.get('next_question', '')
        
        return messages, target_answer

    def __len__(self):
        return len(self.samples)
    
    def get_truncation_stats(self):
        """è·å–æˆªæ–­ç»Ÿè®¡ä¿¡æ¯"""
        if self.truncation_stats['total_samples'] == 0:
            return {
                'truncation_rate': 0.0,
                'avg_truncated_turns': 0.0,
                'total_samples': 0,
                'truncated_samples': 0,
                # å†å²è®°å½•ç»Ÿè®¡
                'history_truncation_rate': 0.0,
                'total_history_items': 0,
                'truncated_history_items': 0,
                'samples_with_history': 0,
                'samples_with_history_truncated': 0
            }
        
        truncation_rate = self.truncation_stats['truncated_samples'] / self.truncation_stats['total_samples']
        avg_truncated_turns = (self.truncation_stats['truncated_turns'] / self.truncation_stats['truncated_samples'] 
                               if self.truncation_stats['truncated_samples'] > 0 else 0)
        
        # è®¡ç®—å†å²è®°å½•æˆªæ–­ç‡
        history_truncation_rate = 0.0
        if self.truncation_stats['total_history_items'] > 0:
            history_truncation_rate = self.truncation_stats['truncated_history_items'] / self.truncation_stats['total_history_items']
        
        return {
            'truncation_rate': truncation_rate,
            'avg_truncated_turns': avg_truncated_turns,
            'total_samples': self.truncation_stats['total_samples'],
            'truncated_samples': self.truncation_stats['truncated_samples'],
            # å†å²è®°å½•ç»Ÿè®¡
            'history_truncation_rate': history_truncation_rate,
            'total_history_items': self.truncation_stats['total_history_items'],
            'truncated_history_items': self.truncation_stats['truncated_history_items'],
            'samples_with_history': self.truncation_stats['samples_with_history'],
            'samples_with_history_truncated': self.truncation_stats['samples_with_history_truncated']
        }

    def format_prompt(self, sample: Dict[str, Any]) -> str:
        """
        æ ¼å¼åŒ–æ ·æœ¬ä¸ºè®­ç»ƒpromptï¼ˆåº”è¯¥è¢«å­ç±»è¦†ç›–ï¼‰
        
        Returns:
            æ ¼å¼åŒ–åçš„promptå­—ç¬¦ä¸²
        """
        raise NotImplementedError("Subclass must implement format_prompt()")
    
    def __getitem__(self, idx):
        sample = self.samples[idx]
        
        # 1. æ ¼å¼åŒ– promptï¼ˆç”±å­ç±»å®ç°ï¼‰
        prompt_text = self.format_prompt(sample)
        
        # 2. è·å–ç›®æ ‡ç­”æ¡ˆ
        target_answer = sample.get('next_question', '')
        
        # 3. æ„å»ºå®Œæ•´æ–‡æœ¬
        # ä½¿ç”¨ Qwen çš„å¯¹è¯æ ¼å¼
        messages = [
            {"role": "system", "content": prompt_text}
        ]
        
        full_prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)
        generation_suffix = "<|im_start|>assistant\n"
        full_prompt = full_prompt.strip() + generation_suffix
        im_end_token = "<|im_end|>"
        full_text = full_prompt + target_answer + im_end_token
        
        # 4. ç¼–ç  - å…³é”®ï¼šä¸åšpaddingï¼
        encoded = self.tokenizer(
            full_text,
            truncation=True,
            max_length=self.max_length,
            padding=False,
            return_tensors='pt'
        )
        
        input_ids = encoded['input_ids'].squeeze()
        attention_mask = encoded['attention_mask'].squeeze()

        # 5. è®¡ç®—labels
        target_ids = self.tokenizer.encode(target_answer, add_special_tokens=False)
        prompt_ids = self.tokenizer.encode(full_prompt, add_special_tokens=False)
        actual_prompt_len = len(prompt_ids)

        labels = input_ids.clone()
        safe_prompt_len = min(actual_prompt_len, len(input_ids) - 1)
        labels[:safe_prompt_len] = -100
        
        # å±è”½padding token
        labels[input_ids == self.tokenizer.pad_token_id] = -100

        return {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'labels': labels,
            'actual_length': len(input_ids)
        }


def dynamic_padding_collate_fn(examples, tokenizer):
    """
    åŠ¨æ€Paddingçš„collateå‡½æ•°
    å…³é”®ä¼˜åŒ–ï¼šåªpaddingåˆ°batchå†…æœ€é•¿æ ·æœ¬çš„é•¿åº¦ï¼Œè€Œä¸æ˜¯å›ºå®šçš„max_length
    """
    # æ‰¾åˆ°batchä¸­æœ€é•¿çš„åºåˆ—é•¿åº¦
    max_length_in_batch = max(ex['input_ids'].shape[0] for ex in examples)
    
    # æ‰“å°batchä¿¡æ¯ï¼ˆç”¨äºè°ƒè¯•ï¼‰
    lengths = [ex['input_ids'].shape[0] for ex in examples]
    if random.random() < 0.05:  # 5%çš„æ¦‚ç‡æ‰“å°ï¼Œé¿å…åˆ·å±
        print(f"[Batch Info] Lengths: {lengths}, Max: {max_length_in_batch}, Avg: {sum(lengths)/len(lengths):.0f}")
    
    batch = {}
    
    # åŠ¨æ€paddingæ¯ä¸ªå­—æ®µ
    padded_input_ids = []
    padded_attention_mask = []
    padded_labels = []
    
    for ex in examples:
        seq_len = ex['input_ids'].shape[0]
        pad_len = max_length_in_batch - seq_len
        
        # Padding input_ids
        padded_input_ids.append(
            torch.cat([
                ex['input_ids'],
                torch.full((pad_len,), tokenizer.pad_token_id, dtype=torch.long)
            ])
        )
        
        # Padding attention_mask
        padded_attention_mask.append(
            torch.cat([
                ex['attention_mask'],
                torch.zeros(pad_len, dtype=torch.long)
            ])
        )
        
        # Padding labels
        padded_labels.append(
            torch.cat([
                ex['labels'],
                torch.full((pad_len,), -100, dtype=torch.long)
            ])
        )
    
    batch['input_ids'] = torch.stack(padded_input_ids)
    batch['attention_mask'] = torch.stack(padded_attention_mask)
    batch['labels'] = torch.stack(padded_labels)
    
    # æ·»åŠ å…¶ä»–å…ƒä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
    if 'actual_length' in examples[0]:
        batch['actual_length'] = [ex['actual_length'] for ex in examples]
    
    return batch


class AblationTrainerWithDynamicPadding(AblationTrainer):
    """å¸¦æ—©åœ + åŠ¨æ€Paddingçš„è®­ç»ƒå™¨"""
    
    def train(
        self,
        train_samples: List[Dict[str, Any]],
        val_samples: Optional[List[Dict[str, Any]]] = None,
        max_epochs: int = 10,
        early_stopping_patience: int = 3,
        early_stopping_threshold: float = 0.00001
    ):
        """è®­ç»ƒæ¨¡å‹ï¼ˆå¸¦æ—©åœ + åŠ¨æ€Paddingï¼‰"""
        train_config = self.config.get('training', {})
        
        # åˆ›å»ºæ•°æ®é›†ï¼ˆä½¿ç”¨åŠ¨æ€Paddingç‰ˆæœ¬ï¼‰
        print("åˆ›å»ºè®­ç»ƒæ•°æ®é›†ï¼ˆåŠ¨æ€Paddingæ¨¡å¼ï¼‰...")
        train_dataset = DynamicPaddingDataset(
            samples=train_samples,
            tokenizer=self.tokenizer,
            max_length=train_config.get('max_length', 4096),
            use_profile=self.use_profile,
            use_history=self.use_history,
            use_context=self.use_context,
            max_context_turns=train_config.get('max_context_turns', 15)  # æ–°å¢ï¼šä» config è¯»å–
        )
        
        val_dataset = None
        if val_samples:
            print("åˆ›å»ºéªŒè¯æ•°æ®é›†ï¼ˆåŠ¨æ€Paddingæ¨¡å¼ï¼‰...")
            val_dataset = DynamicPaddingDataset(
                samples=val_samples,
                tokenizer=self.tokenizer,
                max_length=train_config.get('max_length', 4096),
                use_profile=self.use_profile,
                use_history=self.use_history,
                use_context=self.use_context,
                max_context_turns=train_config.get('max_context_turns', 15)  # æ–°å¢
            )
        
        # æ•°æ®æ•´ç†å™¨ï¼ˆåŠ¨æ€Paddingï¼‰
        def collate_fn(examples):
            return dynamic_padding_collate_fn(examples, self.tokenizer)
        
        # è®¡ç®—æ¯ä¸ªepochçš„æ­¥æ•°å’Œè¯„ä¼°æ­¥æ•°
        steps_per_epoch = len(train_dataset) // (train_config.get('batch_size', 1) * train_config.get('gradient_accumulation_steps', 16))
        eval_steps_value = max(1, steps_per_epoch // 2) if val_dataset else None
        
        # è°ƒæ•´ save_steps
        save_steps_value = train_config.get('save_steps', 500)
        if val_dataset and eval_steps_value and save_steps_value % eval_steps_value != 0:
            save_steps_value = ((save_steps_value + eval_steps_value - 1) // eval_steps_value) * eval_steps_value
            print(f"è°ƒæ•´ save_steps ä¸º {save_steps_value}ï¼ˆeval_steps={eval_steps_value} çš„æ•´æ•°å€ï¼‰")
        
        # å­¦ä¹ ç‡æ£€æŸ¥
        learning_rate = train_config.get('learning_rate', 1e-5)
        if learning_rate > 1e-5:
            print(f"è­¦å‘Š: å­¦ä¹ ç‡ {learning_rate} å¯èƒ½è¿‡å¤§")
        print(f"ä½¿ç”¨å­¦ä¹ ç‡: {learning_rate}")
        
        # è®­ç»ƒå‚æ•°
        training_args = TrainingArguments(
            output_dir=self.output_dir,
            num_train_epochs=max_epochs,
            per_device_train_batch_size=train_config.get('batch_size', 2),
            per_device_eval_batch_size=train_config.get('eval_batch_size', 2),
            gradient_accumulation_steps=train_config.get('gradient_accumulation_steps', 8),
            learning_rate=learning_rate,
            weight_decay=train_config.get('weight_decay', 0.01),
            warmup_steps=train_config.get('warmup_steps', 100),
            logging_steps=train_config.get('logging_steps', 10),
            save_steps=save_steps_value,
            eval_steps=eval_steps_value,
            eval_strategy="steps" if val_dataset else "no",
            save_total_limit=train_config.get('save_total_limit', 3),
            load_best_model_at_end=True if val_dataset else False,
            metric_for_best_model="eval_loss",
            greater_is_better=False,
            fp16=False,
            bf16=True,
            dataloader_pin_memory=False,
            gradient_checkpointing=True,
            optim="adamw_torch",
            max_grad_norm=0.5,
            report_to="wandb" if os.environ.get('WANDB_PROJECT') else "none",
            ddp_find_unused_parameters=False,
        )
        
        # è‡ªå®šä¹‰ Trainer
        class CustomTrainer(Trainer):
            def __init__(self, *args, verbose_logging=False, log_dir=None, **kwargs):
                super().__init__(*args, **kwargs)
                self.verbose_logging = verbose_logging
                self.log_dir = log_dir
                self.io_log_file = None
                
                # åˆ›å»ºè¾“å…¥è¾“å‡ºæ—¥å¿—æ–‡ä»¶
                if self.log_dir:
                    try:
                        os.makedirs(self.log_dir, exist_ok=True)
                        self.io_log_file = open(os.path.join(self.log_dir, 'io_logs.jsonl'), 'w', encoding='utf-8')
                        print(f"è¾“å…¥è¾“å‡ºæ—¥å¿—å°†ä¿å­˜åˆ°: {os.path.join(self.log_dir, 'io_logs.jsonl')}")
                    except Exception as e:
                        print(f"è­¦å‘Š: æ— æ³•åˆ›å»ºè¾“å…¥è¾“å‡ºæ—¥å¿—æ–‡ä»¶: {e}")
                        self.io_log_file = None
                else:
                    self.io_log_file = None
            
            def __del__(self):
                """å…³é—­æ—¥å¿—æ–‡ä»¶"""
                if hasattr(self, 'io_log_file') and self.io_log_file:
                    try:
                        self.io_log_file.close()
                    except:
                        pass
            
            def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
                """è®¡ç®—æŸå¤±ï¼ˆå¸¦æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥ï¼‰"""
                # ç§»é™¤actual_lengthå­—æ®µï¼ˆå¦‚æœå­˜åœ¨ï¼‰ï¼Œé¿å…ä¼ ç»™æ¨¡å‹
                actual_lengths = inputs.pop('actual_length', None)
                
                outputs = model(**inputs)
                logits = outputs.get("logits")
                labels = inputs.get("labels")
                input_ids = inputs.get("input_ids")
                
                # æ£€æŸ¥å¹¶æ¸…ç†logitsä¸­çš„nan/inf
                if logits is not None:
                    has_nan = False
                    has_inf = False
                    
                    # åªæ£€æŸ¥éƒ¨åˆ†æ•°æ®ï¼Œæé«˜æ•ˆç‡
                    if logits.numel() > 0:
                        check_size = min(1000, logits.numel() // 2)
                        if logits.numel() > check_size * 2:
                            head_values = logits.view(-1)[:check_size]
                            tail_values = logits.view(-1)[-check_size:]
                            if torch.isnan(head_values).any() or torch.isnan(tail_values).any():
                                has_nan = True
                            if torch.isinf(head_values).any() or torch.isinf(tail_values).any():
                                has_inf = True
                        else:
                            if torch.isnan(logits).any():
                                has_nan = True
                            if torch.isinf(logits).any():
                                has_inf = True
                    
                    # å¦‚æœå‘ç°é—®é¢˜ï¼Œè¿›è¡Œæ¸…ç†
                    if has_nan or has_inf:
                        nan_count = torch.isnan(logits).sum().item()
                        inf_count = torch.isinf(logits).sum().item()
                        
                        if nan_count > 0 or inf_count > 0:
                            print(f"è­¦å‘Š: Step {self.state.global_step} logitsä¸­æœ‰ {nan_count} ä¸ªnan, {inf_count} ä¸ªinf")
                            logits = torch.where(
                                torch.isnan(logits) | torch.isinf(logits),
                                torch.tensor(0.0, device=logits.device, dtype=logits.dtype),
                                logits
                            )
                            logits = torch.clamp(logits, min=-50.0, max=50.0)
                
                # è®¡ç®—æŸå¤±
                if hasattr(outputs, 'loss') and outputs.loss is not None:
                    loss = outputs.loss
                elif labels is not None:
                    valid_labels_count = (labels != -100).sum().item()
                    
                    if valid_labels_count == 0:
                        print(f"é”™è¯¯: Step {self.state.global_step} æ²¡æœ‰æœ‰æ•ˆçš„labels")
                        loss = torch.tensor(2.0, device=logits.device, requires_grad=True)
                    else:
                        loss_fct = nn.CrossEntropyLoss(ignore_index=-100, reduction='mean')
                        shift_logits = logits[..., :-1, :].contiguous()
                        shift_labels = labels[..., 1:].contiguous()
                        
                        loss = loss_fct(
                            shift_logits.view(-1, shift_logits.size(-1)),
                            shift_labels.view(-1)
                        )
                else:
                    loss = torch.tensor(2.0, device=logits.device, requires_grad=True)
                
                # æ£€æŸ¥æŸå¤±å€¼
                if loss is not None and torch.is_tensor(loss):
                    if loss.dim() > 0:
                        loss = loss.mean()
                    
                    loss_value = loss.item()
                    if torch.isnan(loss) or torch.isinf(loss):
                        print(f"é”™è¯¯: Step {self.state.global_step} lossä¸ºnan/inf")
                        loss = torch.tensor(2.0, device=logits.device, requires_grad=True)
                        loss_value = 2.0
                    elif loss_value > 1e6:
                        print(f"è­¦å‘Š: Step {self.state.global_step} lossè¿‡å¤§ ({loss_value:.2f})")
                        loss = torch.clamp(loss, max=100.0)
                        loss_value = 100.0
                
                # å®šæœŸæ¸…ç†CUDAç¼“å­˜
                if self.state.global_step % 10 == 0:
                    torch.cuda.empty_cache()
                
                if return_outputs:
                    return loss, outputs
                return loss
        
        # åˆ›å»ºæ—©åœå›è°ƒ
        early_stopping = EarlyStoppingCallback(
            early_stopping_patience=early_stopping_patience,
            early_stopping_threshold=early_stopping_threshold
        )
        
        # è®¾ç½®æ—¥å¿—ç›®å½•
        log_dir = os.path.join(self.output_dir, "logs")
        os.makedirs(log_dir, exist_ok=True)
        
        # åˆ›å»º Trainer
        trainer = CustomTrainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=val_dataset,
            data_collator=collate_fn,  # ä½¿ç”¨åŠ¨æ€paddingçš„collate_fn
            processing_class=self.tokenizer,
            callbacks=[early_stopping] if val_dataset else [],
            verbose_logging=True,
            log_dir=log_dir,
        )
        
        # å¼€å§‹è®­ç»ƒ
        print("="*80)
        print("ğŸš€ å¼€å§‹è®­ç»ƒï¼ˆåŠ¨æ€Batch Paddingä¼˜åŒ–ç‰ˆï¼‰")
        print("="*80)
        print(f"è®­ç»ƒæ ·æœ¬æ•°: {len(train_dataset)}")
        if val_dataset:
            print(f"éªŒè¯æ ·æœ¬æ•°: {len(val_dataset)}")
        print(f"ä½¿ç”¨é…ç½®: profile={self.use_profile}, history={self.use_history}, context={self.use_context}")
        print(f"æœ€å¤§åºåˆ—é•¿åº¦: {train_config.get('max_length', 4096)} (åŠ¨æ€padding)")
        print(f"æœ€å¤§è½®æ¬¡: {max_epochs}")
        print(f"æ—©åœè€å¿ƒå€¼: {early_stopping_patience}")
        print("="*80)
        
        trainer.train()
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        print(f"ä¿å­˜æœ€ç»ˆæ¨¡å‹åˆ° {self.output_dir}")
        try:
            trainer.save_model()
            self.tokenizer.save_pretrained(self.output_dir)
            print("âœ“ æ¨¡å‹ä¿å­˜æˆåŠŸ")
        except Exception as e:
            print(f"è­¦å‘Š: ä¿å­˜æ¨¡å‹æ—¶å‡ºé”™: {e}")
        
        # è¾“å‡ºæˆªæ–­ç»Ÿè®¡
        if hasattr(train_dataset, 'get_truncation_stats'):
            stats = train_dataset.get_truncation_stats()
            print("\n" + "="*80)
            print("ğŸ“Š è®­ç»ƒæ•°æ®æˆªæ–­ç»Ÿè®¡:")
            print(f"  æ€»æ ·æœ¬æ•°: {stats['total_samples']}")
            print(f"  è¢«æˆªæ–­æ ·æœ¬æ•°: {stats['truncated_samples']}")
            print(f"  Context æˆªæ–­ç‡: {stats['truncation_rate']:.2%}")
            print(f"  å¹³å‡æˆªæ–­è½®æ¬¡: {stats['avg_truncated_turns']:.2f}")
            
            # å¦‚æœä½¿ç”¨äº†å†å²è®°å½•ï¼Œè¾“å‡ºå†å²è®°å½•ç»Ÿè®¡
            if stats['samples_with_history'] > 0:
                print("\n  ğŸ“š å†å²è®°å½•ç»Ÿè®¡:")
                print(f"    åŒ…å«å†å²è®°å½•çš„æ ·æœ¬æ•°: {stats['samples_with_history']}")
                print(f"    å†å²è®°å½•æ€»æ¡ç›®æ•°: {stats['total_history_items']}")
                print(f"    è¢«æˆªæ–­çš„å†å²æ¡ç›®æ•°: {stats['truncated_history_items']}")
                print(f"    å†å²è®°å½•æˆªæ–­ç‡: {stats['history_truncation_rate']:.2%}")
                print(f"    åŒ…å«è¢«æˆªæ–­å†å²çš„æ ·æœ¬æ•°: {stats['samples_with_history_truncated']}")
                if stats['samples_with_history'] > 0:
                    history_sample_rate = stats['samples_with_history_truncated'] / stats['samples_with_history']
                    print(f"    æ ·æœ¬çº§å†å²æˆªæ–­ç‡: {history_sample_rate:.2%}")
            print("="*80)
        
        print("è®­ç»ƒå®Œæˆï¼")


def main():
    parser = argparse.ArgumentParser(description='æ¶ˆèå®éªŒè®­ç»ƒï¼ˆåŠ¨æ€Paddingä¼˜åŒ–ç‰ˆï¼‰')
    parser.add_argument('--config', type=str,
                       default='/data/lingyu.li/parallel-post-train/ablation/config.json',
                       help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--ablation_config', type=str, required=True,
                       choices=['profile_and_history_and_context', 'profile_and_history', 'profile_and_context', 
                               'history_and_context', 'profile_only', 'history_only', 'context_only'],
                       help='æ¶ˆèå®éªŒé…ç½®')
    parser.add_argument('--val_ratio', type=float, default=0.1,
                       help='éªŒè¯é›†æ¯”ä¾‹')
    parser.add_argument('--gpu', type=int, default=1,
                       help='ä½¿ç”¨çš„GPUç¼–å·ï¼ˆé»˜è®¤ï¼š1ï¼‰')
    parser.add_argument('--max_epochs', type=int, default=50,
                       help='æœ€å¤§è®­ç»ƒè½®æ¬¡ï¼ˆé»˜è®¤ï¼š50ï¼‰')
    parser.add_argument('--early_stopping_patience', type=int, default=3,
                       help='æ—©åœè€å¿ƒå€¼ï¼ˆé»˜è®¤ï¼š3ï¼‰')
    parser.add_argument('--early_stopping_threshold', type=float, default=0.001,
                       help='æ—©åœé˜ˆå€¼ï¼ˆé»˜è®¤ï¼š0.001ï¼‰')
    parser.add_argument('--output_dir', type=str, default=None,
                       help='æ¨¡å‹è¾“å‡ºç›®å½•')
    parser.add_argument('--wandb_project', type=str, default=None,
                       help='Weights & Biasesé¡¹ç›®åç§°')
    parser.add_argument('--wandb_run_name', type=str, default=None,
                       help='Weights & Biasesè¿è¡Œåç§°')
    
    args = parser.parse_args()
    
    # é…ç½® Weights & Biases
    if args.wandb_project:
        try:
            import wandb
            os.environ['WANDB_PROJECT'] = args.wandb_project
            if args.wandb_run_name:
                os.environ['WANDB_NAME'] = args.wandb_run_name
            print(f"âœ“ å·²å¯ç”¨ Weights & Biases ç›‘æ§")
            print(f"  é¡¹ç›®: {args.wandb_project}")
            if args.wandb_run_name:
                print(f"  è¿è¡Œåç§°: {args.wandb_run_name}")
        except ImportError:
            print("è­¦å‘Š: wandb æœªå®‰è£…")
            args.wandb_project = None
    
    # è®¾ç½®GPU
    physical_gpu_id = args.gpu
    os.environ['CUDA_VISIBLE_DEVICES'] = str(physical_gpu_id)
    print(f"=" * 60)
    print(f"GPU è®¾ç½®: ç‰©ç†GPU {physical_gpu_id}")
    print(f"=" * 60)
    
    # éªŒè¯GPU
    if torch.cuda.is_available():
        print(f"CUDA å¯ç”¨ï¼ŒGPU æ•°é‡: {torch.cuda.device_count()}")
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"GPU åç§°: {gpu_name}")
        print(f"GPU æ€»å†…å­˜: {gpu_memory:.2f} GB")
    else:
        print("è­¦å‘Š: CUDA ä¸å¯ç”¨")
    
    # åŠ è½½é…ç½®
    with open(args.config, 'r', encoding='utf-8') as f:
        config = json.load(f)
    
    # è·å–æ¶ˆèé…ç½®
    ablation_config = config['ablation_configs'][args.ablation_config]
    use_profile = ablation_config.get('use_profile', True)
    use_history = ablation_config.get('use_history', True)
    use_context = ablation_config.get('use_context', True)
    config_name = ablation_config['name']
    
    print("=" * 60)
    print(f"æ¶ˆèå®éªŒï¼ˆåŠ¨æ€Paddingä¼˜åŒ–ç‰ˆï¼‰: {config_name}")
    print(f"ä½¿ç”¨é…ç½®: profile={use_profile}, history={use_history}, context={use_context}")
    print("=" * 60)
    
    # åŠ è½½è®­ç»ƒæ•°æ®
    print("åŠ è½½è®­ç»ƒæ•°æ®...")
    train_path = config['data']['train_path']
    train_data = load_train_data(train_path)
    
    if not train_data:
        print(f"é”™è¯¯: æ— æ³•åŠ è½½è®­ç»ƒæ•°æ®")
        return
    
    # æå–è®­ç»ƒæ ·æœ¬
    all_samples = extract_training_samples(train_data, debug=True)
    print(f"æå–äº† {len(all_samples)} ä¸ªè®­ç»ƒæ ·æœ¬")
    
    # æ·»åŠ å†å²ä¿¡æ¯
    if use_history:
        print("æ·»åŠ å†å²ä¿¡æ¯...")
        all_samples = add_history_to_samples(all_samples, all_samples)
    
    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    train_samples, val_samples = split_train_val(all_samples, args.val_ratio)
    print(f"è®­ç»ƒé›†: {len(train_samples)} ä¸ªæ ·æœ¬")
    print(f"éªŒè¯é›†: {len(val_samples)} ä¸ªæ ·æœ¬")
    
    # è·å–æ¨¡å‹é…ç½®
    model_config = config['model']
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    if args.output_dir:
        output_dir = args.output_dir
        os.makedirs(output_dir, exist_ok=True)
        print(f"ä½¿ç”¨æŒ‡å®šçš„è¾“å‡ºç›®å½•: {output_dir}")
    else:
        checkpoint_dir = model_config['checkpoint_dir']
        dataset_name = os.path.basename(os.path.dirname(train_path))
        output_dir = os.path.join(checkpoint_dir, f"{dataset_name}_ablation_{config_name}_dynamic_padding")
        
        try:
            os.makedirs(output_dir, exist_ok=True)
            print(f"è¾“å‡ºç›®å½•: {output_dir}")
        except (OSError, IOError) as e:
            print(f"è­¦å‘Š: æ— æ³•åˆ›å»ºç›®å½•: {e}")
            local_checkpoint_dir = os.path.join(os.path.expanduser("~"), "checkpoints")
            output_dir = os.path.join(local_checkpoint_dir, f"{dataset_name}_ablation_{config_name}_dynamic_padding")
            os.makedirs(output_dir, exist_ok=True)
            print(f"ä½¿ç”¨æœ¬åœ°ç›®å½•: {output_dir}")
    
    # åˆ›å»ºè®­ç»ƒå™¨
    model_path = model_config['path']
    trainer = AblationTrainerWithDynamicPadding(
        model_path=model_path,
        output_dir=output_dir,
        config=config,
        use_profile=use_profile,
        use_history=use_history,
        use_context=use_context
    )
    
    # å¼€å§‹è®­ç»ƒ
    trainer.train(
        train_samples, 
        val_samples,
        max_epochs=args.max_epochs,
        early_stopping_patience=args.early_stopping_patience,
        early_stopping_threshold=args.early_stopping_threshold
    )
    
    print(f"\nâœ… è®­ç»ƒå®Œæˆï¼æ¨¡å‹ä¿å­˜åœ¨: {output_dir}")


if __name__ == '__main__':
    main()


def check_flash_attention_support():
    """æ£€æŸ¥ç³»ç»Ÿæ˜¯å¦æ”¯æŒ FlashAttention 2"""
    try:
        import flash_attn
        flash_version = getattr(flash_attn, '__version__', 'unknown')
        print(f"FlashAttention å·²å®‰è£…ï¼Œç‰ˆæœ¬: {flash_version}")
        return True
    except ImportError:
        print("è­¦å‘Š: FlashAttention æœªå®‰è£…")
        return False


def setup_distributed():
    """åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ"""
    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:
        rank = int(os.environ["RANK"])
        world_size = int(os.environ['WORLD_SIZE'])
        local_rank = int(os.environ['LOCAL_RANK'])
    elif 'SLURM_PROCID' in os.environ:
        rank = int(os.environ['SLURM_PROCID'])
        world_size = int(os.environ['SLURM_NTASKS'])
        local_rank = rank % torch.cuda.device_count()
    else:
        print('æœªæ£€æµ‹åˆ°åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒï¼Œä½¿ç”¨å•å¡è®­ç»ƒ')
        rank = 0
        world_size = 1
        local_rank = 0
    
    torch.cuda.set_device(local_rank)
    
    if world_size > 1:
        dist.init_process_group(
            backend='nccl',
            init_method='env://',
            world_size=world_size,
            rank=rank
        )
    
    return rank, world_size, local_rank


def cleanup_distributed():
    """æ¸…ç†åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ"""
    if dist.is_initialized():
        dist.destroy_process_group()


class MovieReviewDataset(DynamicPaddingDataset):
    """
    å½±è¯„æ•°æ®é›†ï¼ˆç»§æ‰¿è‡ªDynamicPaddingDatasetï¼‰
    """
    
    def format_prompt(self, sample: Dict[str, Any]) -> str:
        """
        æ ¼å¼åŒ–å½±è¯„æ ·æœ¬ä¸ºè®­ç»ƒprompt
        
        è¦†ç›–çˆ¶ç±»æ–¹æ³•ï¼Œä½¿ç”¨å½±è¯„ä¸“ç”¨æ ¼å¼
        """
        parts = []
        
        # 1. ç”¨æˆ·Profile
        if self.use_profile and sample.get('user_profile'):
            profile = sample['user_profile']
            parts.append(f"ç”¨æˆ·: {profile.get('name', 'Unknown')}")
            if sample.get('task_description'):
                parts.append(f"ä»»åŠ¡: {sample['task_description']}")
            parts.append("")
        
        # 2. å†å²å½±è¯„ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.use_history and sample.get('history'):
            history = sample['history']
            parts.append(f"å†å²å½±è¯„è®°å½• ({len(history)}æ¡):")
            
            # åªä½¿ç”¨æœ€è¿‘çš„Næ¡å†å²
            # max_history = 15
            for h in history:
                parts.append(f"  ç”µå½±ã€Š{h['movie']}ã€‹: {h['review']}")
            
            # if len(history) > max_history:
            #     parts.append(f"  ...ï¼ˆè¿˜æœ‰{len(history) - max_history}æ¡æ›´æ—©çš„è¯„è®ºï¼‰")
            parts.append("")
        
        # 3. å½“å‰ç”µå½±
        movie_name = sample.get('movie_name', '')
        parts.append(f"æ¨¡ä»¿ç”¨æˆ·é£æ ¼ä¸ºç”µå½±ã€Š{movie_name}ã€‹å†™ä¸€æ¡å½±è¯„ï¼š")
        
        return "\n".join(parts)


def main():
    parser = argparse.ArgumentParser(description='è±†ç“£å½±è¯„æ¨¡å‹ - åˆ†å¸ƒå¼è®­ç»ƒ')
    
    # é…ç½®æ–‡ä»¶
    parser.add_argument('--config', type=str,
                       default='config_MovieReview.json',
                       help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--ablation_config', type=str, required=True,
                       choices=['profile_and_history', 'profile_only', 'history_only', 'baseline'],
                       help='æ¶ˆèå®éªŒé…ç½®')
    
    # æ•°æ®ç›¸å…³ï¼ˆå¯é€‰ï¼Œè¦†ç›–é…ç½®æ–‡ä»¶ï¼‰
    parser.add_argument('--data_file', type=str, default=None,
                       help='å½±è¯„æ•°æ®JSONæ–‡ä»¶è·¯å¾„ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶ï¼‰')
    parser.add_argument('--val_ratio', type=float, default=None,
                       help='éªŒè¯é›†æ¯”ä¾‹ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶ï¼‰')
    
    # è¾“å‡ºç›®å½•
    parser.add_argument('--output_dir', type=str, default=None,
                       help='æ¨¡å‹è¾“å‡ºç›®å½•')
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--max_epochs', type=int, default=50,
                       help='æœ€å¤§è®­ç»ƒè½®æ¬¡ï¼ˆé»˜è®¤50ï¼‰')
    parser.add_argument('--early_stopping_patience', type=int, default=3,
                       help='æ—©åœè€å¿ƒå€¼ï¼ˆé»˜è®¤3ï¼‰')
    parser.add_argument('--early_stopping_threshold', type=float, default=0.001,
                       help='æ—©åœé˜ˆå€¼ï¼ˆé»˜è®¤0.001ï¼‰')
    
    # DeepSpeedå’Œå…¶ä»–
    parser.add_argument('--deepspeed', type=str, default=None,
                       help='DeepSpeedé…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--disable_flash_attn', action='store_true',
                       help='ç¦ç”¨FlashAttention 2')
    
    # æ–°å¢ï¼šæ¯ç”¨æˆ·é‡‡æ ·å‚æ•°
    parser.add_argument('--max_samples_per_user', type=int, default=None,
                       help='æ¯ä¸ªç”¨æˆ·æœ€å¤šä¿ç•™å¤šå°‘ä¸ªæ ·æœ¬ï¼ˆç”¨äºå‡å°‘è®­ç»ƒæ•°æ®é‡ï¼‰')
    parser.add_argument('--sample_seed', type=int, default=42,
                       help='é‡‡æ ·éšæœºç§å­ï¼ˆé»˜è®¤ï¼š42ï¼Œä¿è¯å¯å¤ç°ï¼‰')
    
    # æ–°å¢ï¼šæ¯ç”¨æˆ·ä¸€ä¸ªæ ·æœ¬æ¨¡å¼
    parser.add_argument('--one_sample_per_user', action='store_true',
                       help='æ¯ä¸ªç”¨æˆ·åªç”Ÿæˆ1ä¸ªæ ·æœ¬ï¼ˆç”¨å‰n-1æ¡å†å²é¢„æµ‹ç¬¬næ¡ï¼Œå¤§å¹…å‡å°‘è®­ç»ƒæ—¶é—´ï¼‰')
    
    parser.add_argument('--wandb_project', type=str, default='MovieReview',
                       help='Weights & Biasesé¡¹ç›®åç§°')
    parser.add_argument('--wandb_run_name', type=str, default=None,
                       help='Weights & Biasesè¿è¡Œåç§°')
    parser.add_argument('--local_rank', type=int, default=-1,
                       help='æœ¬åœ°è¿›ç¨‹rank')
    parser.add_argument('--prompt_style', type=str, default='simple',
                       choices=['simple', 'detailed'],
                       help='Prompté£æ ¼ï¼šsimple=ç®€æ´ï¼Œdetailed=è¯¦ç»†')
    
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®æ–‡ä»¶
    if os.path.isabs(args.config):
        config_path = args.config
    else:
        local_config = Path(__file__).parent / args.config
        if local_config.exists():
            config_path = str(local_config)
        else:
            config_path = args.config
    
    with open(config_path, 'r', encoding='utf-8') as f:
        config = json.load(f)
    
    # è·å–æ¶ˆèé…ç½®
    ablation_config = config['ablation_configs'][args.ablation_config]
    use_profile = ablation_config.get('use_profile', True)
    use_history = ablation_config.get('use_history', True)
    config_name = ablation_config['name']
    
    # åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ
    rank, world_size, local_rank = setup_distributed()
    is_main_process = (rank == 0)
    
    # é…ç½®W&B
    if args.wandb_project and is_main_process:
        try:
            import wandb
            os.environ['WANDB_PROJECT'] = args.wandb_project
            if args.wandb_run_name:
                os.environ['WANDB_NAME'] = args.wandb_run_name
            print(f"âœ“ å·²å¯ç”¨ W&B ç›‘æ§: {args.wandb_project}")
        except ImportError:
            print("è­¦å‘Š: wandb æœªå®‰è£…")
            args.wandb_project = None
    
    if is_main_process:
        print("=" * 80)
        print("è±†ç“£å½±è¯„æ¨¡å‹ - åˆ†å¸ƒå¼è®­ç»ƒ")
        print("=" * 80)
        print(f"World Size: {world_size}")
        print(f"Rank: {rank}")
        print(f"Local Rank: {local_rank}")
        print(f"æ¶ˆèå®éªŒ: {config_name}")
        print(f"ä½¿ç”¨é…ç½®:")
        print(f"  Profile: {use_profile}")
        print(f"  History: {use_history}")
        print(f"  Prompt Style: {args.prompt_style}")
        if args.deepspeed:
            print(f"  DeepSpeed: {args.deepspeed}")
        print("=" * 80)
    
    # æ£€æŸ¥FlashAttentionï¼ˆæ‰€æœ‰è¿›ç¨‹ç‹¬ç«‹æ£€æŸ¥ï¼Œé¿å…CUDA broadcasté—®é¢˜ï¼‰
    use_flash_attn = False
    if not args.disable_flash_attn:
        # æ‰€æœ‰è¿›ç¨‹éƒ½æ£€æŸ¥FlashAttentionæ”¯æŒ
        try:
            import flash_attn
            use_flash_attn = True
            if is_main_process:
                flash_version = getattr(flash_attn, '__version__', 'unknown')
                print(f"FlashAttention å·²å®‰è£…ï¼Œç‰ˆæœ¬: {flash_version}")
        except ImportError:
            if is_main_process:
                print("FlashAttention æœªå®‰è£…ï¼Œä½¿ç”¨æ ‡å‡†attention")
    
    # éªŒè¯GPUæ˜¯å¦å¯ç”¨
    if not torch.cuda.is_available():
        print(f"[Rank {rank}] é”™è¯¯: CUDA ä¸å¯ç”¨")
        cleanup_distributed()
        return
    
    if is_main_process:
        print(f"CUDA å¯ç”¨ï¼ŒGPUæ•°é‡: {torch.cuda.device_count()}")
        print(f"å½“å‰GPU: {local_rank} - {torch.cuda.get_device_name(local_rank)}")
        compute_cap = torch.cuda.get_device_capability(local_rank)
        print(f"è®¡ç®—èƒ½åŠ›: {compute_cap[0]}.{compute_cap[1]}")
        print(f"FlashAttention 2: {'å¯ç”¨' if use_flash_attn else 'ç¦ç”¨'}")
    
    # åŠ è½½æ•°æ®
    if is_main_process:
        print("\n" + "=" * 80)
        print("åŠ è½½å½±è¯„æ•°æ®...")
    
    # æ•°æ®è·¯å¾„ï¼ˆä¼˜å…ˆä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°ï¼Œå¦åˆ™ä½¿ç”¨é…ç½®æ–‡ä»¶ï¼‰
    data_file = args.data_file if args.data_file else config['data']['train_path']
    if not os.path.isabs(data_file):
        data_file = str(Path(__file__).parent / data_file)
    
    raw_data = load_movie_review_data(data_file)
    all_samples = extract_movie_review_samples(
        raw_data, 
        one_sample_per_user=args.one_sample_per_user,  # ğŸ”¥ æ–°å¢ï¼šå¯ç”¨æ¯ç”¨æˆ·ä¸€ä¸ªæ ·æœ¬æ¨¡å¼
        debug=is_main_process
    )
    
    if is_main_process:
        print(f"æ•°æ®æ–‡ä»¶: {data_file}")
        print(f"æå–äº† {len(all_samples)} ä¸ªæ ·æœ¬")
        if args.one_sample_per_user:
            print(f"  âœ… æ¯ç”¨æˆ·ä¸€ä¸ªæ ·æœ¬æ¨¡å¼ï¼šç”¨å‰n-1æ¡å†å²é¢„æµ‹ç¬¬næ¡")
    
    # æ–°å¢ï¼šæ¯ç”¨æˆ·é‡‡æ ·ï¼ˆå¦‚æœæŒ‡å®šäº† max_samples_per_user ä¸”æœªå¯ç”¨ one_sample_per_userï¼‰
    if args.max_samples_per_user is not None and not args.one_sample_per_user:
        if is_main_process:
            print(f"\nå¯¹æ¯ä¸ªç”¨æˆ·è¿›è¡Œé‡‡æ ·ï¼ˆæ¯ç”¨æˆ·æœ€å¤š {args.max_samples_per_user} ä¸ªæ ·æœ¬ï¼‰...")
        all_samples = sample_per_user(
            all_samples,
            max_samples_per_user=args.max_samples_per_user,
            random_seed=args.sample_seed
        )
    
    # è·å–æ•°æ®åˆ’åˆ†æ¯”ä¾‹
    data_split = config.get('data_split', {})
    train_ratio = data_split.get('train_ratio', 0.7)
    val_ratio_config = data_split.get('val_ratio', 0.15)
    test_ratio = data_split.get('test_ratio', 0.15)
    
    # å¦‚æœæŒ‡å®šäº†val_ratioï¼Œéœ€è¦é‡æ–°è®¡ç®—test_ratio
    if args.val_ratio is not None:
        val_ratio_config = args.val_ratio
        test_ratio = 1.0 - train_ratio - val_ratio_config
    
    # æŒ‰æ—¶é—´é¡ºåºåˆ’åˆ†æ•°æ®é›†
    train_samples, val_samples, test_samples = split_movie_reviews_by_time(
        all_samples,
        train_ratio=train_ratio,
        val_ratio=val_ratio_config,
        test_ratio=test_ratio,
        debug=is_main_process
    )
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    if args.output_dir:
        output_dir = args.output_dir
    else:
        checkpoint_dir = config['model']['checkpoint_dir']
        flash_suffix = "flashattn2" if use_flash_attn else "standard"
        output_dir = os.path.join(checkpoint_dir, f"MovieReview_{config_name}_{flash_suffix}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    if is_main_process:
        os.makedirs(output_dir, exist_ok=True)
        print(f"è¾“å‡ºç›®å½•: {output_dir}")
        
        # ä¿å­˜æµ‹è¯•é›†ï¼ˆç”¨äºåç»­è¯„ä¼°ï¼‰
        test_file = os.path.join(output_dir, 'test_samples.json')
        with open(test_file, 'w', encoding='utf-8') as f:
            json.dump(test_samples, f, ensure_ascii=False, indent=2)
        print(f"æµ‹è¯•é›†å·²ä¿å­˜åˆ°: {test_file}")
    
    if world_size > 1:
        dist.barrier()
    
    # åŠ è½½tokenizerå’Œæ¨¡å‹
    model_path = config['model']['path']
    if is_main_process:
        print(f"\nåŠ è½½æ¨¡å‹: {model_path}")
    
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # åŠ è½½æ¨¡å‹
    model_kwargs = {
        'torch_dtype': torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,
        'trust_remote_code': True,
    }
    
    if use_flash_attn:
        model_kwargs['attn_implementation'] = 'flash_attention_2'
    
    try:
        model = AutoModelForCausalLM.from_pretrained(model_path, **model_kwargs)
        if is_main_process:
            print(f"âœ“ æ¨¡å‹å·²åŠ è½½ ({'FlashAttention 2' if use_flash_attn else 'æ ‡å‡†Attention'})")
    except Exception as e:
        if is_main_process:
            print(f"åŠ è½½å¤±è´¥: {e}")
            print("å›é€€åˆ°æ ‡å‡†attention...")
        model_kwargs.pop('attn_implementation', None)
        model = AutoModelForCausalLM.from_pretrained(model_path, **model_kwargs)
        use_flash_attn = False
    
    # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
    if hasattr(model, 'gradient_checkpointing_enable'):
        model.gradient_checkpointing_enable()
        if is_main_process:
            print("âœ“ æ¢¯åº¦æ£€æŸ¥ç‚¹å·²å¯ç”¨")
    
    model = model.to(local_rank)
    
    # åˆ›å»ºæ•°æ®é›†
    train_config = config.get('training', {})
    if is_main_process:
        print("\nåˆ›å»ºè®­ç»ƒæ•°æ®é›†...")
    
    train_dataset = MovieReviewDataset(
        samples=train_samples,
        tokenizer=tokenizer,
        max_length=train_config.get('max_length', 4096),
        use_profile=use_profile,
        use_history=use_history,
        use_context=False,  # å½±è¯„æ•°æ®ä¸ä½¿ç”¨context
        verbose=is_main_process,
        use_detailed_template=False  # ğŸ”¥ æ˜ç¡®æŒ‡å®šä½¿ç”¨ç®€å•æ ¼å¼ï¼ˆè™½ç„¶ä¼šè¢« format_prompt è¦†ç›–ï¼‰
    )
    
    val_dataset = None
    if val_samples:
        val_dataset = MovieReviewDataset(
            samples=val_samples,
            tokenizer=tokenizer,
            max_length=train_config.get('max_length', 4096),
            use_profile=use_profile,
            use_history=use_history,
            use_context=False,
            verbose=False,
            use_detailed_template=False  # ğŸ”¥ æ˜ç¡®æŒ‡å®šä½¿ç”¨ç®€å•æ ¼å¼
        )
    
    # ğŸ”¥ æ–°å¢ï¼šç»Ÿè®¡æ‰€æœ‰æ ·æœ¬çš„ token é•¿åº¦
    if is_main_process:
        print("\n" + "=" * 80)
        print("ğŸ“Š Token é•¿åº¦ç»Ÿè®¡ï¼ˆè®­ç»ƒé›†ï¼‰")
        print("=" * 80)
        
        # æ”¶é›†æ‰€æœ‰æ ·æœ¬çš„ token é•¿åº¦
        token_lengths = []
        for i in range(len(train_dataset)):
            sample = train_dataset[i]
            input_len = len(sample['input_ids'])
            token_lengths.append(input_len)
        
        if token_lengths:
            import numpy as np
            max_length_config = train_config.get('max_length', 4096)
            
            print(f"æ ·æœ¬æ€»æ•°: {len(token_lengths)}")
            print(f"é…ç½®çš„ max_length: {max_length_config}")
            print(f"\nToken é•¿åº¦åˆ†å¸ƒ:")
            print(f"  æœ€å°é•¿åº¦: {min(token_lengths)} tokens")
            print(f"  æœ€å¤§é•¿åº¦: {max(token_lengths)} tokens")
            print(f"  å¹³å‡é•¿åº¦: {np.mean(token_lengths):.1f} tokens")
            print(f"  ä¸­ä½æ•°: {np.median(token_lengths):.0f} tokens")
            print(f"\nåˆ†ä½æ•°:")
            print(f"  25%: {np.percentile(token_lengths, 25):.0f} tokens")
            print(f"  50%: {np.percentile(token_lengths, 50):.0f} tokens")
            print(f"  75%: {np.percentile(token_lengths, 75):.0f} tokens")
            print(f"  90%: {np.percentile(token_lengths, 90):.0f} tokens")
            print(f"  95%: {np.percentile(token_lengths, 95):.0f} tokens")
            print(f"  99%: {np.percentile(token_lengths, 99):.0f} tokens")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ ·æœ¬è¶…è¿‡ max_length
            exceed_count = sum(1 for l in token_lengths if l > max_length_config)
            if exceed_count > 0:
                print(f"\nâš ï¸ è­¦å‘Š: {exceed_count} ä¸ªæ ·æœ¬ ({exceed_count/len(token_lengths)*100:.1f}%) è¶…è¿‡ max_length={max_length_config}")
                print(f"   è¿™äº›æ ·æœ¬ä¼šè¢«æˆªæ–­ï¼Œå¯èƒ½å½±å“è®­ç»ƒæ•ˆæœ")
                print(f"   å»ºè®®:")
                max_needed = max(token_lengths)
                if max_needed <= 8192:
                    print(f"     - å¢åŠ  max_length åˆ° 8192")
                elif max_needed <= 16384:
                    print(f"     - å¢åŠ  max_length åˆ° 16384")
                    print(f"     - æˆ–å¯ç”¨ CPU checkpointing")
                else:
                    print(f"     - å¢åŠ  max_length åˆ° {max_needed}")
                    print(f"     - éœ€è¦ä½¿ç”¨åºåˆ—å¹¶è¡Œï¼ˆUlyssesï¼‰")
            else:
                print(f"\nâœ… æ‰€æœ‰æ ·æœ¬éƒ½åœ¨ max_length={max_length_config} èŒƒå›´å†…")
        
        print("=" * 80)
    
    # æ•°æ®æ•´ç†å™¨
    def collate_fn(examples):
        return dynamic_padding_collate_fn(examples, tokenizer)
    
    # æ‰“å°æ ·æœ¬ç¤ºä¾‹
    if is_main_process:
        print("\n" + "=" * 80)
        print("ğŸ“ è®­ç»ƒæ ·æœ¬ç¤ºä¾‹ï¼ˆå‰3ä¸ªï¼‰")
        print("=" * 80)
        
        sample_log_file = os.path.join(output_dir, "training_samples_preview.txt")
        with open(sample_log_file, 'w', encoding='utf-8') as log_file:
            for i in range(min(3, len(train_samples))):
                sample = train_samples[i]
                
                print(f"\n--- æ ·æœ¬ {i+1} ---")
                log_file.write(f"\n{'='*80}\næ ·æœ¬ {i+1}\n{'='*80}\n\n")
                
                # ç”µå½±ä¿¡æ¯
                movie_name = sample.get('movie_name', 'N/A')
                timestamp = sample.get('timestamp', 'N/A')
                print(f"ç”µå½±: {movie_name}")
                print(f"æ—¶é—´: {timestamp}")
                log_file.write(f"ç”µå½±: {movie_name}\n")
                log_file.write(f"æ—¶é—´: {timestamp}\n\n")
                
                # å†å²å½±è¯„æ•°é‡
                history_count = len(sample.get('history', []))
                print(f"å†å²å½±è¯„: {history_count}æ¡")
                log_file.write(f"å†å²å½±è¯„: {history_count}æ¡\n")
                
                if history_count > 0:
                    log_file.write("æœ€è¿‘3æ¡:\n")
                    for h in sample['history'][-3:]:
                        log_file.write(f"  - {h['movie']}: {h['review']}\n")
                log_file.write("\n")
                
                # ç›®æ ‡å½±è¯„
                target = sample.get('next_question', '')
                print(f"ç›®æ ‡å½±è¯„: {target[:80]}...")
                log_file.write(f"ç›®æ ‡å½±è¯„:\n{target}\n\n")
                
                # ç¼–ç ä¿¡æ¯
                encoded = train_dataset[i]
                input_len = len(encoded['input_ids'])
                valid_labels = (encoded['labels'] != -100).sum().item()
                print(f"ç¼–ç é•¿åº¦: {input_len} tokens, æœ‰æ•ˆæ ‡ç­¾: {valid_labels}")
                log_file.write(f"ç¼–ç é•¿åº¦: {input_len} tokens\n")
                log_file.write(f"æœ‰æ•ˆæ ‡ç­¾: {valid_labels} tokens\n")
        
        print(f"\nâœ“ æ ·æœ¬è¯¦æƒ…å·²ä¿å­˜åˆ°: {sample_log_file}")
        print("=" * 80)
    
    # è®¡ç®—è®­ç»ƒæ­¥æ•°
    batch_size = train_config.get('batch_size', 2)
    gradient_accumulation_steps = train_config.get('gradient_accumulation_steps', 8)
    steps_per_epoch = len(train_dataset) // (world_size * batch_size * gradient_accumulation_steps)
    eval_steps = max(1, steps_per_epoch // 2) if val_dataset else None
    save_steps = train_config.get('save_steps', 500)
    
    # è°ƒæ•´save_stepsä¸ºeval_stepsçš„æ•´æ•°å€
    if val_dataset and eval_steps and save_steps % eval_steps != 0:
        save_steps = ((save_steps + eval_steps - 1) // eval_steps) * eval_steps
        if is_main_process:
            print(f"è°ƒæ•´ save_steps ä¸º {save_steps}ï¼ˆeval_steps={eval_steps} çš„æ•´æ•°å€ï¼‰")
    
    # è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=args.max_epochs,
        per_device_train_batch_size=batch_size,
        per_device_eval_batch_size=train_config.get('eval_batch_size', 2),
        gradient_accumulation_steps=gradient_accumulation_steps,
        learning_rate=train_config.get('learning_rate', 1e-5),
        weight_decay=train_config.get('weight_decay', 0.01),
        warmup_steps=train_config.get('warmup_steps', 100),
        logging_steps=train_config.get('logging_steps', 10),
        save_steps=save_steps,
        eval_steps=eval_steps,
        eval_strategy="steps" if val_dataset else "no",
        save_total_limit=train_config.get('save_total_limit', 3),
        load_best_model_at_end=True if val_dataset else False,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        fp16=False,
        bf16=True,
        dataloader_pin_memory=False,
        gradient_checkpointing=True,
        optim="adamw_torch",
        max_grad_norm=0.5,
        report_to="wandb" if args.wandb_project else "none",
        local_rank=local_rank,
        ddp_find_unused_parameters=False,
        ddp_backend="nccl",
        dataloader_num_workers=2,
        save_on_each_node=False,
        logging_first_step=True,
        deepspeed=args.deepspeed,
    )
    
    # æ—©åœå›è°ƒ
    callbacks = []
    if val_dataset:
        early_stopping = EarlyStoppingCallback(
            early_stopping_patience=args.early_stopping_patience,
            early_stopping_threshold=args.early_stopping_threshold
        )
        callbacks.append(early_stopping)
    
    # åˆ›å»ºTrainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        data_collator=collate_fn,
        processing_class=tokenizer,
        callbacks=callbacks,
    )
    
    # å¼€å§‹è®­ç»ƒ
    if is_main_process:
        print("\n" + "=" * 80)
        print("å¼€å§‹è®­ç»ƒ")
        print("=" * 80)
        print(f"è®­ç»ƒæ ·æœ¬: {len(train_dataset)}")
        print(f"éªŒè¯æ ·æœ¬: {len(val_dataset) if val_dataset else 0}")
        print(f"æµ‹è¯•æ ·æœ¬: {len(test_samples)}")
        print(f"æ¯ä¸ªGPU batch size: {batch_size}")
        print(f"æ¢¯åº¦ç´¯ç§¯: {gradient_accumulation_steps}")
        print(f"æœ‰æ•ˆbatch size: {batch_size * gradient_accumulation_steps * world_size}")
        print(f"é¢„è®¡æ¯epochæ­¥æ•°: {steps_per_epoch}")
        print(f"Max Length: {train_config.get('max_length', 4096)}")
        print(f"Learning Rate: {train_config.get('learning_rate', 1e-5)}")
        if args.deepspeed:
            print(f"DeepSpeed: {args.deepspeed}")
        print("=" * 80)
    
    trainer.train()
    
    # ä¿å­˜æ¨¡å‹
    if is_main_process:
        print(f"\nä¿å­˜æ¨¡å‹åˆ° {output_dir}")
        trainer.save_model()
        tokenizer.save_pretrained(output_dir)
        
        # ä¿å­˜é…ç½®
        config_info = {
            'data_file': data_file,
            'ablation_config': args.ablation_config,
            'config_name': config_name,
            'use_profile': use_profile,
            'use_history': use_history,
            'flash_attention_2': use_flash_attn,
            'max_length': train_config.get('max_length', 4096),
            'train_samples': len(train_samples),
            'val_samples': len(val_samples),
            'test_samples': len(test_samples),
            'prompt_style': args.prompt_style,
        }
        with open(os.path.join(output_dir, 'training_config.json'), 'w', encoding='utf-8') as f:
            json.dump(config_info, f, indent=2, ensure_ascii=False)
        
        print("âœ“ è®­ç»ƒå®Œæˆï¼")
        print(f"âœ“ æ¨¡å‹å·²ä¿å­˜åˆ°: {output_dir}")
    
    # ç­‰å¾…æ‰€æœ‰è¿›ç¨‹å®Œæˆ
    if world_size > 1:
        dist.barrier()
    
    cleanup_distributed()


if __name__ == '__main__':
    main()
