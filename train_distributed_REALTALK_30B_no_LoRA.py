
import json
import argparse
import os
import sys
from pathlib import Path
import random
import torch
import torch.distributed as dist
# ç»Ÿä¸€ä½¿ç”¨ data_loader_more_dataï¼ˆåŒ…å« [ANSWER] æ ‡ç­¾ï¼Œä¸æ¨ç†è„šæœ¬ä¸€è‡´ï¼‰
from data_loader_more_data import load_train_data, extract_training_samples, get_user_only_history
from train_with_dynamic_padding import DynamicPaddingDataset, dynamic_padding_collate_fn, split_train_val, add_history_to_samples, CustomTrainerWithAnswerWeight
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    EarlyStoppingCallback,
    TrainingArguments,
    Trainer
)
from typing import List, Dict, Any, Optional
import torch.nn as nn


def sample_per_user(samples: List[Dict], max_samples_per_user: Optional[int], seed: int = 42) -> List[Dict]:
    """
    å¯¹æ¯ä¸ªç”¨æˆ·çš„æ ·æœ¬è¿›è¡Œé‡‡æ ·
    
    Args:
        samples: è®­ç»ƒæ ·æœ¬åˆ—è¡¨
        max_samples_per_user: æ¯ä¸ªç”¨æˆ·æœ€å¤šé‡‡æ ·çš„æ ·æœ¬æ•°ï¼ŒNoneè¡¨ç¤ºä¸é‡‡æ ·
        seed: éšæœºç§å­
    
    Returns:
        é‡‡æ ·åçš„æ ·æœ¬åˆ—è¡¨
    """
    if max_samples_per_user is None:
        return samples
    
    # æŒ‰ç”¨æˆ·åˆ†ç»„
    user_samples = {}
    for sample in samples:
        user_hash = sample.get('user_hash', 'unknown')
        if user_hash not in user_samples:
            user_samples[user_hash] = []
        user_samples[user_hash].append(sample)
    
    # å¯¹æ¯ä¸ªç”¨æˆ·é‡‡æ ·
    random.seed(seed)
    sampled_samples = []
    for user_hash, user_sample_list in user_samples.items():
        if len(user_sample_list) <= max_samples_per_user:
            sampled_samples.extend(user_sample_list)
        else:
            sampled = random.sample(user_sample_list, max_samples_per_user)
            sampled_samples.extend(sampled)
    
    return sampled_samples


def check_flash_attention_support():
    """æ£€æŸ¥ç³»ç»Ÿæ˜¯å¦æ”¯æŒ FlashAttention 2"""
    try:
        import flash_attn
        flash_version = getattr(flash_attn, '__version__', 'unknown')
        print(f"FlashAttention å·²å®‰è£…ï¼Œç‰ˆæœ¬: {flash_version}")
        return True
    except ImportError:
        print("è­¦å‘Š: FlashAttention æœªå®‰è£…")
        return False


def setup_distributed():
    """åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ"""
    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:
        rank = int(os.environ["RANK"])
        world_size = int(os.environ['WORLD_SIZE'])
        local_rank = int(os.environ['LOCAL_RANK'])
    elif 'SLURM_PROCID' in os.environ:
        rank = int(os.environ['SLURM_PROCID'])
        world_size = int(os.environ['SLURM_NTASKS'])
        local_rank = rank % torch.cuda.device_count()
    else:
        print('è­¦å‘Š: æœªæ£€æµ‹åˆ°åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒå˜é‡ï¼Œä½¿ç”¨å•å¡è®­ç»ƒ')
        rank = 0
        world_size = 1
        local_rank = 0
    
    torch.cuda.set_device(local_rank)
    
    if world_size > 1:
        dist.init_process_group(
            backend='nccl',
            init_method='env://',
            world_size=world_size,
            rank=rank
        )
    
    return rank, world_size, local_rank


def cleanup_distributed():
    """æ¸…ç†åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ"""
    if dist.is_initialized():
        dist.destroy_process_group()


def main():
    parser = argparse.ArgumentParser(description='åˆ†å¸ƒå¼æ¶ˆèå®éªŒè®­ç»ƒï¼ˆFlashAttention 2 + åŠ¨æ€Paddingï¼‰- REALTALK')
    parser.add_argument('--config', type=str,
                       default='config_REALTALK.json',
                       help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--ablation_config', type=str, required=True,
                       choices=['profile_and_history_and_context', 'profile_and_history', 'profile_and_context', 
                               'history_and_context', 'profile_only', 'history_only', 'context_only'],
                       help='æ¶ˆèå®éªŒé…ç½®')
    parser.add_argument('--val_ratio', type=float, default=0.1,
                       help='éªŒè¯é›†æ¯”ä¾‹')
    parser.add_argument('--max_epochs', type=int, default=3,
                       help='æœ€å¤§è®­ç»ƒè½®æ¬¡ï¼ˆé»˜è®¤ï¼š3ï¼‰')
    parser.add_argument('--early_stopping_patience', type=int, default=2,
                       help='æ—©åœè€å¿ƒå€¼ï¼ˆé»˜è®¤ï¼š2ï¼‰')
    parser.add_argument('--early_stopping_threshold', type=float, default=0.001,
                       help='æ—©åœé˜ˆå€¼ï¼ˆé»˜è®¤ï¼š0.001ï¼‰')
    parser.add_argument('--output_dir', type=str, default=None,
                       help='æ¨¡å‹è¾“å‡ºç›®å½•')
    parser.add_argument('--local_rank', type=int, default=-1,
                       help='æœ¬åœ°è¿›ç¨‹rankï¼ˆç”± torch.distributed.launch è‡ªåŠ¨è®¾ç½®ï¼‰')
    parser.add_argument('--wandb_project', type=str, default='Qwen3-REALTALK',
                       help='Weights & Biasesé¡¹ç›®åç§°ï¼ˆé»˜è®¤ï¼šQwen3-REALTALKï¼‰')
    parser.add_argument('--wandb_run_name', type=str, default=None,
                       help='Weights & Biasesè¿è¡Œåç§°ï¼ˆé»˜è®¤ï¼šè‡ªåŠ¨ç”Ÿæˆï¼‰')
    parser.add_argument('--disable_flash_attn', action='store_true',
                       help='ç¦ç”¨FlashAttention 2ï¼Œä½¿ç”¨æ ‡å‡†attention')
    parser.add_argument('--deepspeed', type=str, default=None,
                       help='DeepSpeedé…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰')
    
    # æ–°å¢ï¼šPrompt æ¨¡æ¿æ§åˆ¶å‚æ•°
    parser.add_argument('--prompt_style', type=str, default='simple',
                       choices=['simple', 'detailed', 'lovink'],
                       help='Prompt é£æ ¼ï¼šsimple=ç®€æ´æ ‡ç­¾æ ¼å¼ï¼ˆé»˜è®¤ï¼‰ï¼Œdetailed=è¯¦ç»†æ¨¡æ¿ï¼Œlovink=Lovinké£æ ¼')
    parser.add_argument('--template_filename', type=str, default=None,
                       help='æŒ‡å®šæ¨¡æ¿æ–‡ä»¶åï¼ˆä»…å½“ prompt_style=detailed æ—¶ç”Ÿæ•ˆï¼‰')
    
    # æ–°å¢ï¼šç”¨æˆ·é‡‡æ ·å‚æ•°
    parser.add_argument('--max_samples_per_user', type=int, default=None,
                       help='æ¯ä¸ªç”¨æˆ·æœ€å¤šé‡‡æ ·çš„æ ·æœ¬æ•°ï¼ˆNoneè¡¨ç¤ºä½¿ç”¨æ‰€æœ‰æ ·æœ¬ï¼‰')
    parser.add_argument('--sample_seed', type=int, default=42,
                       help='é‡‡æ ·éšæœºç§å­ï¼ˆé»˜è®¤ï¼š42ï¼‰')
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ
    rank, world_size, local_rank = setup_distributed()
    
    # åªåœ¨ä¸»è¿›ç¨‹æ‰“å°ä¿¡æ¯
    is_main_process = (rank == 0)
    
    # é…ç½® Weights & Biases (åªåœ¨ä¸»è¿›ç¨‹)
    if args.wandb_project:
        try:
            import wandb
            os.environ['WANDB_PROJECT'] = args.wandb_project
            if args.wandb_run_name:
                os.environ['WANDB_NAME'] = args.wandb_run_name
            if is_main_process:
                print(f"âœ“ å·²å¯ç”¨ Weights & Biases ç›‘æ§")
        except ImportError:
            if is_main_process:
                print("è­¦å‘Š: wandb æœªå®‰è£…")
            args.wandb_project = None
    
    if is_main_process:
        print(f"=" * 80)
        print(f"åˆ†å¸ƒå¼è®­ç»ƒè®¾ç½®ï¼ˆFlashAttention 2 + åŠ¨æ€Paddingï¼‰:")
        print(f"  World Size (æ€»è¿›ç¨‹æ•°): {world_size}")
        print(f"  Rank (è¿›ç¨‹ID): {rank}")
        print(f"  Local Rank (æœ¬åœ°GPU ID): {local_rank}")
        print(f"  ä½¿ç”¨ {world_size} å¼ GPUè¿›è¡Œå¹¶è¡Œè®­ç»ƒ")
        print(f"  ä¼˜åŒ–ç­–ç•¥: FlashAttention 2 + åŠ¨æ€Batch Padding")
        if args.deepspeed:
            print(f"  DeepSpeedé…ç½®: {args.deepspeed}")
        print(f"=" * 80)
    
    # æ£€æŸ¥ FlashAttention æ”¯æŒ
    use_flash_attn = False
    if not args.disable_flash_attn and is_main_process:
        use_flash_attn = check_flash_attention_support()
    
    # å¹¿æ’­ use_flash_attn åˆ°æ‰€æœ‰è¿›ç¨‹
    if world_size > 1:
        use_flash_attn_tensor = torch.tensor([use_flash_attn], dtype=torch.bool, device=f'cuda:{local_rank}')
        dist.broadcast(use_flash_attn_tensor, src=0)
        use_flash_attn = use_flash_attn_tensor.item()
    
    # éªŒè¯GPUæ˜¯å¦å¯ç”¨
    if torch.cuda.is_available():
        if is_main_process:
            print(f"CUDA å¯ç”¨ï¼Œæ€»GPUæ•°é‡: {torch.cuda.device_count()}")
            print(f"å½“å‰è¿›ç¨‹ä½¿ç”¨ GPU: {local_rank}")
            gpu_name = torch.cuda.get_device_name(local_rank)
            gpu_memory = torch.cuda.get_device_properties(local_rank).total_memory / 1024**3
            print(f"GPU åç§°: {gpu_name}")
            print(f"GPU æ€»å†…å­˜: {gpu_memory:.2f} GB")
            
            # æ£€æŸ¥GPUè®¡ç®—èƒ½åŠ›
            compute_capability = torch.cuda.get_device_capability(local_rank)
            print(f"GPU è®¡ç®—èƒ½åŠ›: {compute_capability[0]}.{compute_capability[1]}")
            if compute_capability[0] >= 8:  # A100/H100
                print("âœ“ GPUæ”¯æŒFlashAttention 2ä¼˜åŒ–")
            else:
                print("GPUè®¡ç®—èƒ½åŠ›è¾ƒä½ï¼ŒFlashAttention 2æ€§èƒ½å¯èƒ½å—é™")
    else:
        print("é”™è¯¯: CUDA ä¸å¯ç”¨")
        cleanup_distributed()
        return
    
    # åŠ è½½é…ç½®ï¼ˆä¼˜å…ˆä½¿ç”¨å½“å‰ç›®å½•ï¼Œæ”¯æŒç»å¯¹è·¯å¾„ï¼‰
    if os.path.isabs(args.config):
        config_path = args.config
    else:
        # ä¼˜å…ˆæŸ¥æ‰¾å½“å‰ç›®å½•
        local_config = Path(__file__).parent / args.config
        if local_config.exists():
            config_path = str(local_config)
        else:
            # å›é€€åˆ°çˆ¶ç›®å½•ï¼ˆå‘åå…¼å®¹ï¼‰
            config_path = os.path.join(Path(__file__).parent.parent, args.config)
    
    with open(config_path, 'r', encoding='utf-8') as f:
        config = json.load(f)
    
    # è·å–æ¶ˆèé…ç½®
    ablation_config = config['ablation_configs'][args.ablation_config]
    use_profile = ablation_config.get('use_profile', True)
    use_history = ablation_config.get('use_history', True)
    use_context = ablation_config.get('use_context', True)
    config_name = ablation_config['name']
    
    if is_main_process:
        print("=" * 80)
        print(f"æ¶ˆèå®éªŒï¼ˆFlashAttn2 + åŠ¨æ€Paddingï¼‰: {config_name}")
        print(f"ä½¿ç”¨é…ç½®: profile={use_profile}, history={use_history}, context={use_context}")
        print(f"FlashAttention 2: {'å¯ç”¨' if use_flash_attn else 'ç¦ç”¨'}")
        print("=" * 80)
    
    # åŠ è½½è®­ç»ƒæ•°æ®
    if is_main_process:
        print("åŠ è½½è®­ç»ƒæ•°æ®...")
    train_path = config['data']['train_path']
    train_data = load_train_data(train_path)
    
    if not train_data:
        print(f"é”™è¯¯: æ— æ³•åŠ è½½è®­ç»ƒæ•°æ®")
        cleanup_distributed()
        return
    
    # æå–è®­ç»ƒæ ·æœ¬
    all_samples = extract_training_samples(train_data, debug=is_main_process)
    if is_main_process:
        print(f"æå–äº† {len(all_samples)} ä¸ªè®­ç»ƒæ ·æœ¬")
    
    # ç”¨æˆ·é‡‡æ ·ï¼ˆå¦‚æœæŒ‡å®šï¼‰
    if args.max_samples_per_user is not None:
        if is_main_process:
            print(f"å¯¹æ¯ä¸ªç”¨æˆ·é‡‡æ ·æœ€å¤š {args.max_samples_per_user} ä¸ªæ ·æœ¬ï¼ˆç§å­={args.sample_seed}ï¼‰...")
        all_samples = sample_per_user(all_samples, args.max_samples_per_user, args.sample_seed)
        if is_main_process:
            print(f"é‡‡æ ·åå‰©ä½™ {len(all_samples)} ä¸ªæ ·æœ¬")
    
    # æ·»åŠ å†å²ä¿¡æ¯
    if use_history:
        if is_main_process:
            print("æ·»åŠ å†å²ä¿¡æ¯...")
        all_samples = add_history_to_samples(all_samples, all_samples)
    
    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    train_samples, val_samples = split_train_val(all_samples, args.val_ratio)
    if is_main_process:
        print(f"è®­ç»ƒé›†: {len(train_samples)} ä¸ªæ ·æœ¬")
        print(f"éªŒè¯é›†: {len(val_samples)} ä¸ªæ ·æœ¬")
        print(f"æ¯ä¸ªGPUå®é™…å¤„ç†çº¦ {len(train_samples) // world_size} ä¸ªè®­ç»ƒæ ·æœ¬")
    
    # è·å–æ¨¡å‹é…ç½®
    model_config = config['model']
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    if args.output_dir:
        output_dir = args.output_dir
    else:
        checkpoint_dir = model_config['checkpoint_dir']
        dataset_name = os.path.basename(os.path.dirname(train_path))
        flash_suffix = "flashattn2" if use_flash_attn else "standard"
        output_dir = os.path.join(checkpoint_dir, f"{dataset_name}_ablation_{config_name}_{flash_suffix}_dynamic_distributed")
    
    # åªåœ¨ä¸»è¿›ç¨‹åˆ›å»ºç›®å½•
    if is_main_process:
        try:
            os.makedirs(output_dir, exist_ok=True)
            print(f"è¾“å‡ºç›®å½•: {output_dir}")
        except (OSError, IOError) as e:
            print(f"è­¦å‘Š: æ— æ³•åˆ›å»ºè¾“å‡ºç›®å½•: {e}")
    
    # ç­‰å¾…ä¸»è¿›ç¨‹åˆ›å»ºå®Œç›®å½•
    if world_size > 1:
        dist.barrier()
    
    # åŠ è½½æ¨¡å‹å’Œtokenizer
    model_path = model_config['path']
    if is_main_process:
        print(f"åŠ è½½æ¨¡å‹: {model_path}")
        if use_flash_attn:
            print("  ä½¿ç”¨ FlashAttention 2 å®ç°...")
    
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    train_config = config.get('training', {})
    # åŠ è½½æ¨¡å‹åˆ°æŒ‡å®šGPUï¼ˆä½¿ç”¨FlashAttention 2ï¼‰
    model_kwargs = {
        'torch_dtype': torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,
        'trust_remote_code': True,
    }
    
    # å¦‚æœæ”¯æŒä¸”æœªç¦ç”¨ï¼Œåˆ™ä½¿ç”¨FlashAttention 2
    if use_flash_attn:
        model_kwargs['attn_implementation'] = 'flash_attention_2'
    
    try:
        model = AutoModelForCausalLM.from_pretrained(model_path, **model_kwargs)
        if is_main_process:
            if use_flash_attn:
                print("âœ“ æ¨¡å‹å·²åŠ è½½ï¼ˆFlashAttention 2ï¼‰")
            else:
                print("âœ“ æ¨¡å‹å·²åŠ è½½ï¼ˆæ ‡å‡†Attentionï¼‰")
    except Exception as e:
        if is_main_process:
            print(f"åŠ è½½FlashAttention 2å¤±è´¥: {e}")
            print("   å›é€€åˆ°æ ‡å‡†attention...")
        # å›é€€åˆ°æ ‡å‡†attention
        model_kwargs.pop('attn_implementation', None)
        model = AutoModelForCausalLM.from_pretrained(model_path, **model_kwargs)
        use_flash_attn = False
    
    # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
    if hasattr(model, 'gradient_checkpointing_enable'):
        model.gradient_checkpointing_enable()
        if is_main_process:
            print("âœ“ æ¢¯åº¦æ£€æŸ¥ç‚¹å·²å¯ç”¨")
    
    # å°†æ¨¡å‹ç§»åˆ°å¯¹åº”çš„GPU
    model = model.to(local_rank)
    
    # åˆ›å»ºæ•°æ®é›†ï¼ˆä½¿ç”¨åŠ¨æ€Paddingç‰ˆæœ¬ï¼‰
    if is_main_process:
        print("åˆ›å»ºè®­ç»ƒæ•°æ®é›†ï¼ˆåŠ¨æ€Paddingæ¨¡å¼ï¼‰...")
    
    # âœ… æ ¹æ®å‘½ä»¤è¡Œå‚æ•°å†³å®šä½¿ç”¨å“ªç§ prompt é£æ ¼
    use_detailed_template = (args.prompt_style != 'simple')
    template_filename = args.template_filename if args.prompt_style == 'detailed' else None
    
    if is_main_process:
        print(f"Prompt é£æ ¼: {args.prompt_style}")
        if args.prompt_style == 'simple':
            print("   ä½¿ç”¨ç®€æ´æ ‡ç­¾æ ¼å¼ï¼ˆ[USER_PROFILE] [DIM_XXX=score] ...ï¼‰")
        elif args.prompt_style == 'detailed':
            if template_filename:
                print(f"   ä½¿ç”¨è¯¦ç»†æ¨¡æ¿: {template_filename} (æ ‡å‡† {{VAR_NAME}} æ ¼å¼)")
            else:
                print("   ä½¿ç”¨è¯¦ç»†æ¨¡æ¿ï¼ˆé»˜è®¤é¡ºåºæŸ¥æ‰¾ï¼‰")
        elif args.prompt_style == 'lovink':
            print("   ä½¿ç”¨ Lovink é£æ ¼æ¨¡æ¿")
    
    train_dataset = DynamicPaddingDataset(
        samples=train_samples,
        tokenizer=tokenizer,
        max_length=train_config.get('max_length', 4096),
        use_profile=use_profile,
        use_history=use_history,
        use_context=use_context,
        verbose=is_main_process,  # åªåœ¨ä¸»è¿›ç¨‹è¾“å‡ºè¯¦ç»†æ—¥å¿—
        use_detailed_template=use_detailed_template,
        template_filename=template_filename
    )
    
    val_dataset = None
    if val_samples:
        if is_main_process:
            print("åˆ›å»ºéªŒè¯æ•°æ®é›†ï¼ˆåŠ¨æ€Paddingæ¨¡å¼ï¼‰...")
        val_dataset = DynamicPaddingDataset(
            samples=val_samples,
            tokenizer=tokenizer,
            max_length=train_config.get('max_length', 4096),
            use_profile=use_profile,
            use_history=use_history,
            use_context=use_context,
            use_detailed_template=use_detailed_template,
            template_filename=template_filename
        )
    
    # æ•°æ®æ•´ç†å™¨ï¼ˆä½¿ç”¨åŠ¨æ€Paddingç‰ˆæœ¬ï¼‰
    def collate_fn(examples):
        return dynamic_padding_collate_fn(examples, tokenizer)
    
    # æ‰“å°ç¬¬ä¸€ä¸ªæ ·æœ¬çš„è¾“å…¥è¾“å‡ºç¤ºä¾‹ï¼ˆç”¨äºè°ƒè¯•ï¼‰
    if is_main_process and len(train_samples) > 0:
        print("\n" + "=" * 80)
        print("ğŸ“‹ ç¬¬ä¸€ä¸ªè®­ç»ƒæ ·æœ¬ç¤ºä¾‹")
        print("=" * 80)
        first_sample = train_samples[0]
        first_encoded = train_dataset[0]
        
        print(f"User Hash: {first_sample.get('user_hash', 'N/A')}")
        print(f"Context è½®æ•°: {len(first_sample.get('context', []))}")
        print(f"Target é•¿åº¦: {len(first_sample.get('next_question', ''))} å­—ç¬¦")
        print(f"\nç¼–ç ä¿¡æ¯:")
        print(f"  Input length: {len(first_encoded['input_ids'])} tokens")
        print(f"  Valid labels: {(first_encoded['labels'] != -100).sum().item()} tokens")
        print(f"  è®­ç»ƒæ¯”ä¾‹: {(first_encoded['labels'] != -100).sum().item() / len(first_encoded['labels']):.2%}")
        
        # è§£ç æ˜¾ç¤ºè¾“å…¥å’Œè¾“å‡º
        print(f"\nè¾“å…¥æ–‡æœ¬ï¼ˆå‰500 tokensï¼‰:")
        input_text = tokenizer.decode(first_encoded['input_ids'][:500], skip_special_tokens=False)
        print(f"  {input_text[:300]}...")
        
        valid_label_indices = (first_encoded['labels'] != -100).nonzero(as_tuple=True)[0]
        if len(valid_label_indices) > 0:
            valid_labels = first_encoded['labels'][valid_label_indices]
            print(f"\nç›®æ ‡è¾“å‡ºï¼ˆæ¨¡å‹è¦ç”Ÿæˆçš„éƒ¨åˆ†ï¼Œå‰200 tokensï¼‰:")
            target_text = tokenizer.decode(valid_labels[:200], skip_special_tokens=False)
            print(f"  {target_text[:300]}...")
        
        print("=" * 80 + "\n")
    
    # è®¡ç®—è®­ç»ƒæ­¥æ•°
    steps_per_epoch = len(train_dataset) // (world_size * train_config.get('batch_size', 2) * train_config.get('gradient_accumulation_steps', 8))
    eval_steps_value = max(1, steps_per_epoch // 2) if val_dataset else None
    save_steps_value = train_config.get('save_steps', 500)
    
    if val_dataset and eval_steps_value and save_steps_value % eval_steps_value != 0:
        save_steps_value = ((save_steps_value + eval_steps_value - 1) // eval_steps_value) * eval_steps_value
        if is_main_process:
            print(f"è°ƒæ•´ save_steps ä¸º {save_steps_value}ï¼ˆeval_steps={eval_steps_value} çš„æ•´æ•°å€ï¼‰")
    
    # è®­ç»ƒå‚æ•°ï¼ˆåˆ†å¸ƒå¼ + FlashAttention 2 + åŠ¨æ€Paddingï¼‰
    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=args.max_epochs,
        per_device_train_batch_size=train_config.get('batch_size', 2),
        per_device_eval_batch_size=train_config.get('eval_batch_size', 2),
        gradient_accumulation_steps=train_config.get('gradient_accumulation_steps', 8),
        learning_rate=train_config.get('learning_rate', 1e-5),
        weight_decay=train_config.get('weight_decay', 0.01),
        warmup_steps=train_config.get('warmup_steps', 100),
        logging_steps=train_config.get('logging_steps', 10),
        save_steps=save_steps_value,
        eval_steps=eval_steps_value,
        eval_strategy="steps" if val_dataset else "no",
        save_total_limit=train_config.get('save_total_limit', 3),
        load_best_model_at_end=True if val_dataset else False,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        fp16=False,
        bf16=True,  # FlashAttention 2 ä¸ BF16 é…åˆæ•ˆæœæ›´å¥½
        dataloader_pin_memory=False,
        gradient_checkpointing=True,
        optim="adamw_torch",
        max_grad_norm=0.5,
        report_to="wandb" if args.wandb_project else "none",
        # åˆ†å¸ƒå¼è®­ç»ƒå…³é”®å‚æ•°
        local_rank=local_rank,
        ddp_find_unused_parameters=False,
        ddp_backend="nccl",
        dataloader_num_workers=2,
        save_on_each_node=False,
        logging_first_step=True,
        # DeepSpeedé…ç½®ï¼ˆå¯é€‰ï¼‰
        deepspeed=args.deepspeed,
    )
    
    # åˆ›å»ºæ—©åœå›è°ƒ
    callbacks = []
    if val_dataset:
        early_stopping = EarlyStoppingCallback(
            early_stopping_patience=args.early_stopping_patience,
            early_stopping_threshold=args.early_stopping_threshold
        )
        callbacks.append(early_stopping)
    
    # åˆ›å»º Trainerï¼ˆä½¿ç”¨ç»Ÿä¸€çš„ CustomTrainerWithAnswerWeightï¼‰
    trainer = CustomTrainerWithAnswerWeight(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        data_collator=collate_fn,  # ä½¿ç”¨åŠ¨æ€paddingçš„collate_fn
        processing_class=tokenizer,
        callbacks=callbacks,
        tokenizer=tokenizer,
        is_main_process=is_main_process,
        rank=rank,
        debug_steps=3,
    )
    
    # å¼€å§‹è®­ç»ƒ
    if is_main_process:
        print("=" * 80)
        print("å¼€å§‹åˆ†å¸ƒå¼è®­ç»ƒï¼ˆFlashAttention 2 + åŠ¨æ€Paddingï¼‰")
        print("=" * 80)
        print(f"æ€»æ ·æœ¬æ•°: {len(train_dataset)}")
        print(f"æ¯ä¸ªGPUå¤„ç†çº¦: {len(train_dataset) // world_size} ä¸ªæ ·æœ¬")
        effective_batch = train_config.get('batch_size', 2) * train_config.get('gradient_accumulation_steps', 8) * world_size
        print(f"æœ‰æ•ˆ batch size: {effective_batch}")
        print(f"é¢„è®¡æ¯ä¸ªepochæ­¥æ•°: {steps_per_epoch}")
        print(f"Max Length: {train_config.get('max_length', 4096)} (åŠ¨æ€padding)")
        print(f"Attention: {'FlashAttention 2' if use_flash_attn else 'æ ‡å‡†Attention'}")
        if args.wandb_project:
            print(f"W&B ç›‘æ§: é¡¹ç›®={args.wandb_project}, è¿è¡Œ={args.wandb_run_name or 'auto'}")
        
        # è¾“å‡ºåˆå§‹æˆªæ–­ç»Ÿè®¡ï¼ˆè®­ç»ƒå‰ï¼‰
        if hasattr(train_dataset, 'get_truncation_stats'):
            stats = train_dataset.get_truncation_stats()
            print(f"\næ•°æ®é¢„å¤„ç†æˆªæ–­ç»Ÿè®¡:")
            print(f"  å·²å¤„ç†æ ·æœ¬: {stats['total_samples']}")
            print(f"  è¢«æˆªæ–­æ ·æœ¬: {stats['truncated_samples']}")
            if stats['total_samples'] > 0:
                print(f"  æˆªæ–­ç‡: {stats['truncation_rate']:.2%}")
                if stats['truncated_samples'] > 0:
                    print(f"  å¹³å‡æˆªæ–­è½®æ¬¡: {stats['avg_truncated_turns']:.2f}")
        
        print("=" * 80)
    
    trainer.train()
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹ï¼ˆåªåœ¨ä¸»è¿›ç¨‹ä¿å­˜ï¼‰
    if is_main_process:
        print(f"ä¿å­˜æœ€ç»ˆæ¨¡å‹åˆ° {output_dir}")
        try:
            trainer.save_model()
            tokenizer.save_pretrained(output_dir)
            print("âœ“ æ¨¡å‹ä¿å­˜æˆåŠŸ")
            
            # ä¿å­˜è®­ç»ƒé…ç½®ä¿¡æ¯
            config_info = {
                'flash_attention_2': use_flash_attn,
                'dynamic_padding': True,
                'gradient_checkpointing': True,
                'ablation_config': args.ablation_config,
                'config_name': config_name
            }
            with open(os.path.join(output_dir, 'training_config.json'), 'w', encoding='utf-8') as f:
                json.dump(config_info, f, indent=2, ensure_ascii=False)
            print("âœ“ è®­ç»ƒé…ç½®å·²ä¿å­˜")
            
        except Exception as e:
            print(f"è­¦å‘Š: ä¿å­˜æ¨¡å‹æ—¶å‡ºé”™: {e}")
        
        # è¾“å‡ºæˆªæ–­ç»Ÿè®¡
        if hasattr(train_dataset, 'get_truncation_stats'):
            stats = train_dataset.get_truncation_stats()
            print("\n" + "="*80)
            print("è®­ç»ƒæ•°æ®æˆªæ–­ç»Ÿè®¡:")
            print(f"  æ€»æ ·æœ¬æ•°: {stats['total_samples']}")
            print(f"  è¢«æˆªæ–­æ ·æœ¬æ•°: {stats['truncated_samples']}")
            print(f"  æˆªæ–­ç‡: {stats['truncation_rate']:.2%}")
            if stats['truncated_samples'] > 0:
                print(f"  å¹³å‡æˆªæ–­è½®æ¬¡: {stats['avg_truncated_turns']:.2f}")
            print("="*80)
    
    # ç­‰å¾…æ‰€æœ‰è¿›ç¨‹å®Œæˆ
    if world_size > 1:
        dist.barrier()
    
    if is_main_process:
        print(f"\n è®­ç»ƒå®Œæˆï¼æ¨¡å‹ä¿å­˜åœ¨: {output_dir}")
        if use_flash_attn:
            print(" ä½¿ç”¨äº† FlashAttention 2 åŠ é€Ÿè®­ç»ƒ")
    
    # æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ
    cleanup_distributed()


if __name__ == '__main__':
    main()
