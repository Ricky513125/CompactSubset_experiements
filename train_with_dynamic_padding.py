"""
æ¶ˆèå®éªŒè®­ç»ƒè„šæœ¬ï¼ˆå¸¦æ—©åœæœºåˆ¶ + åŠ¨æ€Batch Paddingä¼˜åŒ–ï¼‰
å…³é”®ä¼˜åŒ–ï¼šä¸å†å°†batchå†…æ‰€æœ‰æ ·æœ¬paddingåˆ°å›ºå®šmax_lengthï¼Œ
è€Œæ˜¯åŠ¨æ€paddingåˆ°batchå†…æœ€é•¿æ ·æœ¬çš„å®é™…é•¿åº¦ï¼Œå¤§å¹…èŠ‚çœæ˜¾å­˜ã€‚
"""
import json
import argparse
import os
import sys
from pathlib import Path
import random
import torch

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))
# from data_loader import load_train_data, extract_training_samples, get_user_history_samples, get_user_only_history # æ—§ç‰ˆæœ¬ å¤æ‚çš„è®­ç»ƒprompt  
from data_loader_more_data import load_train_data, extract_training_samples, get_user_only_history # æ–°ç‰ˆæœ¬ ç®€çŸ­çš„è®­ç»ƒprompt
from trainer_pc import AblationTrainer
from transformers import EarlyStoppingCallback, TrainingArguments, Trainer
from typing import List, Dict, Any, Optional
import torch.nn as nn
from torch.utils.data import Dataset


def split_train_val(samples, val_ratio=0.1, seed=42):
    """
    åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼ˆç”¨æˆ·å†…åˆ’åˆ†ï¼Œä¿è¯æ¯ä¸ªç”¨æˆ·åœ¨è®­ç»ƒå’ŒéªŒè¯é›†éƒ½æœ‰æ ·æœ¬ï¼‰
    
    ç­–ç•¥ï¼šå¯¹æ¯ä¸ªç”¨æˆ·çš„æ ·æœ¬è¿›è¡Œéšæœºæ‰“ä¹±åæŒ‰æ¯”ä¾‹åˆ’åˆ†
    - é€‚ç”¨åœºæ™¯ï¼šæµ‹è¯•é›†ä¸­çš„ç”¨æˆ·ä¹Ÿå‡ºç°åœ¨è®­ç»ƒé›†ä¸­
    - ç›®æ ‡ï¼šå­¦ä¹ åŸºäºç”¨æˆ·å·²æœ‰å¯¹è¯é¢„æµ‹æ–°å¯¹è¯ï¼ˆç”¨æˆ·å†…æ³›åŒ–ï¼‰
    
    Args:
        samples: æ‰€æœ‰è®­ç»ƒæ ·æœ¬
        val_ratio: éªŒè¯é›†æ¯”ä¾‹ï¼ˆé»˜è®¤0.15ï¼Œå³15%ï¼‰
        seed: éšæœºç§å­
    
    Returns:
        (train_samples, val_samples)
    """
    random.seed(seed)
    
    # æŒ‰ç”¨æˆ·åˆ†ç»„
    user_samples = {}
    for sample in samples:
        user_hash = sample['user_hash']
        if user_hash not in user_samples:
            user_samples[user_hash] = []
        user_samples[user_hash].append(sample)
    
    train_samples = []
    val_samples = []
    
    # å¯¹æ¯ä¸ªç”¨æˆ·çš„æ ·æœ¬è¿›è¡Œåˆ’åˆ†
    for user_hash, user_data in user_samples.items():
        # éšæœºæ‰“ä¹±è¯¥ç”¨æˆ·çš„æ ·æœ¬
        random.shuffle(user_data)
        
        # è®¡ç®—åˆ’åˆ†ç‚¹ï¼š(1 - val_ratio) çš„æ ·æœ¬ç”¨äºè®­ç»ƒ
        split_idx = int(len(user_data) * (1 - val_ratio))
        
        # ç¡®ä¿è‡³å°‘æœ‰1ä¸ªæ ·æœ¬åœ¨è®­ç»ƒé›†ï¼ˆå¦‚æœè¯¥ç”¨æˆ·åªæœ‰1ä¸ªæ ·æœ¬ï¼Œå…¨éƒ¨ç»™è®­ç»ƒé›†ï¼‰
        if split_idx == 0 and len(user_data) > 0:
            split_idx = 1
        
        # åˆ’åˆ†
        train_samples.extend(user_data[:split_idx])
        val_samples.extend(user_data[split_idx:])
    
    return train_samples, val_samples


def add_history_to_samples(train_samples, all_samples):
    """ä¸ºæ¯ä¸ªæ ·æœ¬æ·»åŠ å†å²ä¿¡æ¯ï¼ˆåªåŒ…å«ç”¨æˆ·çš„é—®é¢˜ï¼Œä¸åŒ…å«assistantå†…å®¹ï¼‰"""
    samples_with_history = []
    for sample in train_samples:
        user_hash = sample['user_hash']
        history = get_user_only_history(
            all_samples, 
            user_hash,
            current_sample=sample,
            current_context=sample.get('context'),
            max_history=15,
            use_cache=True
        )
        sample['history'] = history
        samples_with_history.append(sample)
    return samples_with_history


class DynamicPaddingDataset(Dataset):
    """
    ä¼˜åŒ–ç‰ˆæ•°æ®é›†ï¼šä¸åšpaddingï¼Œè¿”å›åŸå§‹é•¿åº¦çš„tensor
    paddingå°†åœ¨collate_fnä¸­åŠ¨æ€è¿›è¡Œ
    """
    def __init__(self, samples, tokenizer, max_length=32768, use_profile=True, use_history=True, use_context=True, verbose=False, use_detailed_template=True, max_context_turns=15, template_filename=None, require_token_type_ids=None):
        # ä½¿ç”¨ç»å¯¹è·¯å¾„å¯¼å…¥ï¼Œç¡®ä¿ä½¿ç”¨å½“å‰ç›®å½•çš„æ¨¡å—
        import sys
        from pathlib import Path
        current_dir = str(Path(__file__).parent.absolute())
        if current_dir not in sys.path:
            sys.path.insert(0, current_dir)
        
        # âœ… æ ¹æ® use_detailed_template é€‰æ‹© prompt æ„å»ºå‡½æ•°
        if use_detailed_template:
            # ä½¿ç”¨è¯¦ç»†æ¨¡æ¿ï¼ˆæ ‡å‡† markdown æ ¼å¼ï¼Œä½¿ç”¨ {VAR_NAME} å ä½ç¬¦ï¼‰
            from prompt_builder import build_training_prompt
            print("ä½¿ç”¨è¯¦ç»† Prompt æ¨¡æ¿ (prompt_builder)")
            self.build_training_prompt = build_training_prompt
        else:
            # ä½¿ç”¨ç®€çŸ­æ¨¡æ¿
            # ä¼˜å…ˆå°è¯•ä» data_loader.py å¯¼å…¥ï¼ˆæ–°ç‰ˆæœ¬ï¼Œåªé¢„æµ‹ continuationï¼‰
            # å¦‚æœå¤±è´¥ï¼Œåˆ™ä» data_loader_more_data.py å¯¼å…¥ï¼ˆæ—§ç‰ˆæœ¬ï¼Œæ•°æ®æ‰©å……ï¼‰
            try:
                from data_loader import build_simple_training_prompt as build_training_prompt
                print("âœ… ä½¿ç”¨ç®€çŸ­ Prompt æ¨¡æ¿ (data_loader.build_simple_training_prompt - åªé¢„æµ‹continuation)")
                self.build_training_prompt = build_training_prompt
            except ImportError:
                from data_loader_more_data import build_simple_training_prompt as build_training_prompt
                print("âœ… ä½¿ç”¨ç®€çŸ­ Prompt æ¨¡æ¿ (data_loader_more_data.build_simple_training_prompt)")
                self.build_training_prompt = build_training_prompt
        
        self.samples = samples
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.use_profile = use_profile
        self.use_history = use_history
        self.use_context = use_context
        self.use_detailed_template = use_detailed_template  # æ˜¯å¦ä½¿ç”¨è¯¦ç»†æ¨¡æ¿
        self.max_context_turns = max_context_turns  # æ–°å¢ï¼šæœ€å¤§ä¿ç•™çš„ context è½®æ¬¡æ•°
        self.template_filename = template_filename  # æ–°å¢ï¼šæ¨¡æ¿æ–‡ä»¶å
        self.verbose = verbose  # æ˜¯å¦è¾“å‡ºè¯¦ç»†æ—¥å¿—
        
        # âœ… è‡ªåŠ¨æ£€æµ‹æ˜¯å¦éœ€è¦ token_type_idsï¼ˆç”¨äº Gemma3 æ¨¡å‹ï¼‰
        if require_token_type_ids is None:
            # æ ¹æ® tokenizer è‡ªåŠ¨åˆ¤æ–­
            model_type = getattr(tokenizer, 'name_or_path', '').lower()
            self.require_token_type_ids = 'gemma' in model_type
            if self.require_token_type_ids and verbose:
                print(f"âœ… æ£€æµ‹åˆ° Gemma æ¨¡å‹ï¼Œå°†æ·»åŠ  token_type_ids")
        else:
            self.require_token_type_ids = require_token_type_ids
        
        # æˆªæ–­ç»Ÿè®¡
        self.truncation_stats = {
            'total_samples': 0,
            'truncated_samples': 0,
            'truncated_turns': 0,
            # å†å²è®°å½•ç»Ÿè®¡
            'total_history_items': 0,
            'truncated_history_items': 0,
            'samples_with_history': 0,
            'samples_with_history_truncated': 0
        }
        
        # ç”¨äºè®°å½•ç¬¬ä¸€æ¬¡æˆªæ–­çš„æ ·æœ¬ä¿¡æ¯ï¼ˆè°ƒè¯•ç”¨ï¼‰
        self.first_truncation_logged = False

    def __len__(self):
        return len(self.samples)
    
    def get_truncation_stats(self):
        """è·å–æˆªæ–­ç»Ÿè®¡ä¿¡æ¯"""
        if self.truncation_stats['total_samples'] == 0:
            return {
                'truncation_rate': 0.0,
                'avg_truncated_turns': 0.0,
                'total_samples': 0,
                'truncated_samples': 0,
                # å†å²è®°å½•ç»Ÿè®¡
                'history_truncation_rate': 0.0,
                'total_history_items': 0,
                'truncated_history_items': 0,
                'samples_with_history': 0,
                'samples_with_history_truncated': 0
            }
        
        truncation_rate = self.truncation_stats['truncated_samples'] / self.truncation_stats['total_samples']
        avg_truncated_turns = (self.truncation_stats['truncated_turns'] / self.truncation_stats['truncated_samples'] 
                               if self.truncation_stats['truncated_samples'] > 0 else 0)
        
        # è®¡ç®—å†å²è®°å½•æˆªæ–­ç‡
        history_truncation_rate = 0.0
        if self.truncation_stats['total_history_items'] > 0:
            history_truncation_rate = self.truncation_stats['truncated_history_items'] / self.truncation_stats['total_history_items']
        
        return {
            'truncation_rate': truncation_rate,
            'avg_truncated_turns': avg_truncated_turns,
            'total_samples': self.truncation_stats['total_samples'],
            'truncated_samples': self.truncation_stats['truncated_samples'],
            # å†å²è®°å½•ç»Ÿè®¡
            'history_truncation_rate': history_truncation_rate,
            'total_history_items': self.truncation_stats['total_history_items'],
            'truncated_history_items': self.truncation_stats['truncated_history_items'],
            'samples_with_history': self.truncation_stats['samples_with_history'],
            'samples_with_history_truncated': self.truncation_stats['samples_with_history_truncated']
        }

    def __getitem__(self, idx):
        sample = self.samples[idx]
        
        # ç»Ÿè®¡å†å²è®°å½•ä¿¡æ¯
        original_history = sample.get('history', []) if self.use_history else []
        has_history = len(original_history) > 0
        original_history_count = len(original_history)
        
        if has_history:
            self.truncation_stats['samples_with_history'] += 1
            self.truncation_stats['total_history_items'] += original_history_count
        
        # 1. æ„å»ºæ¶ˆæ¯
        # âœ… æ ¹æ®æ¨¡æ¿ç±»å‹ï¼Œä¼ é€’ä¸åŒçš„å‚æ•°
        if self.use_detailed_template:
            # è¯¦ç»†æ¨¡æ¿éœ€è¦é¢å¤–çš„å‚æ•°
            messages, target_answer = self.build_training_prompt(
                context=sample['context'],
                next_question=sample['next_question'],
                user_profile=sample.get('user_profile') if self.use_profile else None,
                task_description=sample.get('task_description'),
                history=original_history,
                use_profile=self.use_profile,
                use_history=self.use_history,
                use_context=self.use_context,
                use_detailed_template=self.use_detailed_template,
                max_context_turns=self.max_context_turns,
                tokenizer=self.tokenizer,
                template_filename=self.template_filename  # âœ… ä¼ é€’æ¨¡æ¿æ–‡ä»¶å
            )
        else:
            # ç®€çŸ­æ¨¡æ¿ - âœ… æ·»åŠ  tokenizer å’Œ max_length ç”¨äºåŠ¨æ€é•¿åº¦è°ƒæ•´
            messages, target_answer = self.build_training_prompt(
                context=sample['context'],
                next_question=sample['next_question'],
                user_profile=sample.get('user_profile') if self.use_profile else None,
                task_description=sample.get('task_description'),
                history=original_history,
                use_profile=self.use_profile,
                use_history=self.use_history,
                use_context=self.use_context,
                tokenizer=self.tokenizer,         # âœ… ä¼ é€’ tokenizer
                max_length=self.max_length,       # âœ… ä¼ é€’ max_length
                min_target_tokens=64,             # âœ… é¢„ç•™ 64 tokens ç»™ target
                user_hash=sample.get('user_hash')  # âœ… ä¼ é€’ user_hashï¼ˆå§‹ç»ˆåŒ…å«ï¼‰
            )


        # æ£€æŸ¥å†å²è®°å½•æ˜¯å¦è¢«æˆªæ–­ï¼ˆåœ¨ prompt_builder ä¸­é™åˆ¶ä¸ºå‰5ä¸ªï¼‰
        if has_history and original_history_count > 5:
            truncated_history_count = original_history_count - 5
            self.truncation_stats['truncated_history_items'] += truncated_history_count
            self.truncation_stats['samples_with_history_truncated'] += 1


        # 2. ç”Ÿæˆå®Œæ•´æ–‡æœ¬
        full_prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)
        # ä¿®æ­£ï¼šåº”è¯¥ç”Ÿæˆassistantè§’è‰²çš„å›å¤ï¼ˆç›®æ ‡ç”¨æˆ·ï¼‰
        generation_suffix = "<|im_start|>assistant\n"
        full_prompt = full_prompt.strip() + generation_suffix
        im_end_token = "<|im_end|>"
        full_text = full_prompt + target_answer + im_end_token
        
        # âœ… ç¬¬äºŒå±‚ä¿æŠ¤ï¼šå¦‚æœä»ç„¶è¶…é•¿ï¼Œé€æ­¥ä»å‰å¾€ååˆ é™¤å¯¹è¯è½®æ¬¡
        target_with_end = target_answer + im_end_token
        target_tokens = len(self.tokenizer.encode(target_with_end, add_special_tokens=False))
        min_buffer = 64
        
        full_length = len(self.tokenizer.encode(full_text, add_special_tokens=False))
        is_truncated = False
        removed_turns = 0
        
        if full_length > self.max_length:
            is_truncated = True
            
            # å…è®¸çš„æœ€å¤§ prompt é•¿åº¦
            max_prompt_tokens = self.max_length - target_tokens - min_buffer
            
            # å¦‚æœæœ‰ RECENT_DIALOGUE éƒ¨åˆ†ï¼Œé€æ­¥ä»å‰å¾€ååˆ é™¤æ—§å¯¹è¯
            if len(messages) > 0 and messages[0].get('role') == 'system':
                system_content = messages[0]['content']
                
                if '[RECENT_DIALOGUE]' in system_content:
                    # è§£æ dialogue éƒ¨åˆ†
                    parts = system_content.split('[RECENT_DIALOGUE]')
                    if len(parts) > 1:
                        prefix = parts[0].strip()  # Profile + Task
                        dialogue_section = parts[1].strip()
                        
                        # æå–å¯¹è¯è¡Œï¼ˆè·³è¿‡ "Predict the user's next message:"ï¼‰
                        dialogue_lines = []
                        for line in dialogue_section.split('\n'):
                            line = line.strip()
                            if line and not line.startswith('Predict') and not line.startswith('ï¼ˆå‰é¢çœç•¥'):
                                if line.startswith('User:') or line.startswith('Assistant:'):
                                    dialogue_lines.append(line)
                        
                        # ä»å‰å¾€åé€æ­¥åˆ é™¤å¯¹è¯è½®æ¬¡ï¼Œç›´åˆ°é•¿åº¦åˆé€‚
                        while dialogue_lines and full_length > self.max_length:
                            # åˆ é™¤æœ€æ—§çš„ä¸€è½®ï¼ˆç¬¬ä¸€ä¸ªï¼‰
                            dialogue_lines.pop(0)
                            removed_turns += 1
                            
                            # é‡å»º system message
                            if removed_turns > 0 and dialogue_lines:
                                new_dialogue = f"\n[RECENT_DIALOGUE]\nï¼ˆå‰é¢çœç•¥äº† {removed_turns} è½®å¯¹è¯ï¼‰\n" + "\n".join(dialogue_lines)
                            elif dialogue_lines:
                                new_dialogue = "\n[RECENT_DIALOGUE]\n" + "\n".join(dialogue_lines)
                            else:
                                new_dialogue = ""
                            
                            new_system = prefix + new_dialogue + "\n\nPredict the user's next message:"
                            messages[0]['content'] = new_system
                            
                            # é‡æ–°ç”Ÿæˆå¹¶æµ‹è¯•é•¿åº¦
                            full_prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)
                            full_prompt = full_prompt.strip() + generation_suffix
                            full_text = full_prompt + target_answer + im_end_token
                            full_length = len(self.tokenizer.encode(full_text, add_special_tokens=False))
        
        # æ›´æ–°æˆªæ–­ç»Ÿè®¡
        self.truncation_stats['total_samples'] += 1
        if is_truncated:
            self.truncation_stats['truncated_samples'] += 1
            self.truncation_stats['truncated_turns'] += removed_turns
            
            # ç¬¬ä¸€æ¬¡é‡åˆ°æˆªæ–­æ—¶è¾“å‡ºæ—¥å¿—
            if not self.first_truncation_logged and self.verbose:
                self.first_truncation_logged = True
                print(f"\nâš ï¸  ç¬¬äºŒå±‚ä¿æŠ¤ï¼šé€æ­¥åˆ é™¤æ—§å¯¹è¯ (æ ·æœ¬#{idx}):")
                print(f"  åˆ é™¤äº† {removed_turns} è½®å¯¹è¯ï¼ˆä»æœ€æ—§çš„å¼€å§‹ï¼‰")
                print(f"  è°ƒæ•´åé•¿åº¦: {full_length} tokens")
                print(f"  æœ€å¤§é•¿åº¦: {self.max_length} tokens")
                print(f"  Target é•¿åº¦: {target_tokens} tokens (å·²å®Œæ•´ä¿ç•™)")
                print(f"  (åç»­æˆªæ–­å°†ä¸å†è¾“å‡ºè¯¦ç»†ä¿¡æ¯)\n")

        # 3. ç¼–ç  - å…³é”®ï¼šä¸åšpaddingï¼
        encoded = self.tokenizer(
            full_text,
            truncation=True,
            max_length=self.max_length,
            padding=False,  # å…³é”®æ”¹åŠ¨ï¼šä¸padding
            return_tensors='pt'
        )
        
        input_ids = encoded['input_ids'].squeeze()
        attention_mask = encoded['attention_mask'].squeeze()

        # 4. è®¡ç®—labels
        target_ids = self.tokenizer.encode(target_answer, add_special_tokens=False)
        prompt_ids = self.tokenizer.encode(full_prompt, add_special_tokens=False)
        actual_prompt_len = len(prompt_ids)

        labels = input_ids.clone()
        safe_prompt_len = min(actual_prompt_len, len(input_ids) - 1)
        labels[:safe_prompt_len] = -100
        
        # å±è”½padding tokenï¼ˆè™½ç„¶ç°åœ¨æ²¡æœ‰paddingï¼Œä½†ä¸ºäº†å…¼å®¹æ€§ä¿ç•™ï¼‰
        labels[input_ids == self.tokenizer.pad_token_id] = -100

        result = {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'labels': labels,
            'actual_length': len(input_ids)  # è®°å½•å®é™…é•¿åº¦ï¼Œç”¨äºè°ƒè¯•
        }
        
        # âœ… åªæœ‰ Gemma3 æ¨¡å‹éœ€è¦æ—¶æ‰æ·»åŠ  token_type_ids
        if self.require_token_type_ids:
            token_type_ids = torch.zeros_like(input_ids)
            result['token_type_ids'] = token_type_ids
        
        return result


def dynamic_padding_collate_fn(examples, tokenizer):
    """
    åŠ¨æ€Paddingçš„collateå‡½æ•°
    å…³é”®ä¼˜åŒ–ï¼šåªpaddingåˆ°batchå†…æœ€é•¿æ ·æœ¬çš„é•¿åº¦ï¼Œè€Œä¸æ˜¯å›ºå®šçš„max_length
    """
    # æ‰¾åˆ°batchä¸­æœ€é•¿çš„åºåˆ—é•¿åº¦
    max_length_in_batch = max(ex['input_ids'].shape[0] for ex in examples)
    
    # æ‰“å°batchä¿¡æ¯ï¼ˆç”¨äºè°ƒè¯•ï¼‰
    lengths = [ex['input_ids'].shape[0] for ex in examples]
    if random.random() < 0.05:  # 5%çš„æ¦‚ç‡æ‰“å°ï¼Œé¿å…åˆ·å±
        print(f"[Batch Info] Lengths: {lengths}, Max: {max_length_in_batch}, Avg: {sum(lengths)/len(lengths):.0f}")
    
    batch = {}
    
    # åŠ¨æ€paddingæ¯ä¸ªå­—æ®µ
    padded_input_ids = []
    padded_attention_mask = []
    padded_labels = []
    padded_token_type_ids = []  # âœ… æ·»åŠ  token_type_ids
    
    for ex in examples:
        seq_len = ex['input_ids'].shape[0]
        pad_len = max_length_in_batch - seq_len
        
        # Padding input_ids
        padded_input_ids.append(
            torch.cat([
                ex['input_ids'],
                torch.full((pad_len,), tokenizer.pad_token_id, dtype=torch.long)
            ])
        )
        
        # Padding attention_mask
        padded_attention_mask.append(
            torch.cat([
                ex['attention_mask'],
                torch.zeros(pad_len, dtype=torch.long)
            ])
        )
        
        # Padding labels
        padded_labels.append(
            torch.cat([
                ex['labels'],
                torch.full((pad_len,), -100, dtype=torch.long)
            ])
        )
        
        # âœ… Padding token_type_ids
        if 'token_type_ids' in ex:
            padded_token_type_ids.append(
                torch.cat([
                    ex['token_type_ids'],
                    torch.zeros(pad_len, dtype=torch.long)
                ])
            )
    
    batch['input_ids'] = torch.stack(padded_input_ids)
    batch['attention_mask'] = torch.stack(padded_attention_mask)
    batch['labels'] = torch.stack(padded_labels)
    
    # âœ… æ·»åŠ  token_type_ids åˆ° batch
    if padded_token_type_ids:
        batch['token_type_ids'] = torch.stack(padded_token_type_ids)
    
    # æ·»åŠ å…¶ä»–å…ƒä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
    if 'actual_length' in examples[0]:
        batch['actual_length'] = [ex['actual_length'] for ex in examples]
    
    return batch


class AblationTrainerWithDynamicPadding(AblationTrainer):
    """å¸¦æ—©åœ + åŠ¨æ€Paddingçš„è®­ç»ƒå™¨"""
    
    def train(
        self,
        train_samples: List[Dict[str, Any]],
        val_samples: Optional[List[Dict[str, Any]]] = None,
        max_epochs: int = 10,
        early_stopping_patience: int = 3,
        early_stopping_threshold: float = 0.00001
    ):
        """è®­ç»ƒæ¨¡å‹ï¼ˆå¸¦æ—©åœ + åŠ¨æ€Paddingï¼‰"""
        train_config = self.config.get('training', {})
        
        # åˆ›å»ºæ•°æ®é›†ï¼ˆä½¿ç”¨åŠ¨æ€Paddingç‰ˆæœ¬ï¼‰
        print("åˆ›å»ºè®­ç»ƒæ•°æ®é›†ï¼ˆåŠ¨æ€Paddingæ¨¡å¼ï¼‰...")
        train_dataset = DynamicPaddingDataset(
            samples=train_samples,
            tokenizer=self.tokenizer,
            max_length=train_config.get('max_length', 4096),
            use_profile=self.use_profile,
            use_history=self.use_history,
            use_context=self.use_context,
            max_context_turns=train_config.get('max_context_turns', 15)  # æ–°å¢ï¼šä» config è¯»å–
        )
        
        val_dataset = None
        if val_samples:
            print("åˆ›å»ºéªŒè¯æ•°æ®é›†ï¼ˆåŠ¨æ€Paddingæ¨¡å¼ï¼‰...")
            val_dataset = DynamicPaddingDataset(
                samples=val_samples,
                tokenizer=self.tokenizer,
                max_length=train_config.get('max_length', 4096),
                use_profile=self.use_profile,
                use_history=self.use_history,
                use_context=self.use_context,
                max_context_turns=train_config.get('max_context_turns', 15)  # æ–°å¢
            )
        
        # æ•°æ®æ•´ç†å™¨ï¼ˆåŠ¨æ€Paddingï¼‰
        def collate_fn(examples):
            return dynamic_padding_collate_fn(examples, self.tokenizer)
        
        # è®¡ç®—æ¯ä¸ªepochçš„æ­¥æ•°å’Œè¯„ä¼°æ­¥æ•°
        steps_per_epoch = len(train_dataset) // (train_config.get('batch_size', 1) * train_config.get('gradient_accumulation_steps', 16))
        eval_steps_value = max(1, steps_per_epoch // 2) if val_dataset else None
        
        # è°ƒæ•´ save_steps
        save_steps_value = train_config.get('save_steps', 500)
        if val_dataset and eval_steps_value and save_steps_value % eval_steps_value != 0:
            save_steps_value = ((save_steps_value + eval_steps_value - 1) // eval_steps_value) * eval_steps_value
            print(f"è°ƒæ•´ save_steps ä¸º {save_steps_value}ï¼ˆeval_steps={eval_steps_value} çš„æ•´æ•°å€ï¼‰")
        
        # å­¦ä¹ ç‡æ£€æŸ¥
        learning_rate = train_config.get('learning_rate', 1e-5)
        if learning_rate > 1e-5:
            print(f"è­¦å‘Š: å­¦ä¹ ç‡ {learning_rate} å¯èƒ½è¿‡å¤§")
        print(f"ä½¿ç”¨å­¦ä¹ ç‡: {learning_rate}")
        
        # è®­ç»ƒå‚æ•°
        training_args = TrainingArguments(
            output_dir=self.output_dir,
            num_train_epochs=max_epochs,
            per_device_train_batch_size=train_config.get('batch_size', 2),
            per_device_eval_batch_size=train_config.get('eval_batch_size', 2),
            gradient_accumulation_steps=train_config.get('gradient_accumulation_steps', 8),
            learning_rate=learning_rate,
            weight_decay=train_config.get('weight_decay', 0.01),
            warmup_steps=train_config.get('warmup_steps', 100),
            logging_steps=train_config.get('logging_steps', 10),
            save_steps=save_steps_value,
            eval_steps=eval_steps_value,
            eval_strategy="steps" if val_dataset else "no",
            save_total_limit=train_config.get('save_total_limit', 3),
            load_best_model_at_end=True if val_dataset else False,
            metric_for_best_model="eval_loss",
            greater_is_better=False,
            fp16=False,
            bf16=True,
            dataloader_pin_memory=False,
            gradient_checkpointing=True,
            optim="adamw_torch",
            max_grad_norm=0.5,
            report_to="wandb" if os.environ.get('WANDB_PROJECT') else "none",
            ddp_find_unused_parameters=False,
        )
        
        # è‡ªå®šä¹‰ Trainer
        class CustomTrainer(Trainer):
            def __init__(self, *args, verbose_logging=False, log_dir=None, **kwargs):
                super().__init__(*args, **kwargs)
                self.verbose_logging = verbose_logging
                self.log_dir = log_dir
                self.io_log_file = None
                
                # åˆ›å»ºè¾“å…¥è¾“å‡ºæ—¥å¿—æ–‡ä»¶
                if self.log_dir:
                    try:
                        os.makedirs(self.log_dir, exist_ok=True)
                        self.io_log_file = open(os.path.join(self.log_dir, 'io_logs.jsonl'), 'w', encoding='utf-8')
                        print(f"è¾“å…¥è¾“å‡ºæ—¥å¿—å°†ä¿å­˜åˆ°: {os.path.join(self.log_dir, 'io_logs.jsonl')}")
                    except Exception as e:
                        print(f"è­¦å‘Š: æ— æ³•åˆ›å»ºè¾“å…¥è¾“å‡ºæ—¥å¿—æ–‡ä»¶: {e}")
                        self.io_log_file = None
                else:
                    self.io_log_file = None
            
            def __del__(self):
                """å…³é—­æ—¥å¿—æ–‡ä»¶"""
                if hasattr(self, 'io_log_file') and self.io_log_file:
                    try:
                        self.io_log_file.close()
                    except:
                        pass
            
            def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
                """è®¡ç®—æŸå¤±ï¼ˆå¸¦æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥ï¼‰"""
                # ç§»é™¤actual_lengthå­—æ®µï¼ˆå¦‚æœå­˜åœ¨ï¼‰ï¼Œé¿å…ä¼ ç»™æ¨¡å‹
                actual_lengths = inputs.pop('actual_length', None)
                
                outputs = model(**inputs)
                logits = outputs.get("logits")
                labels = inputs.get("labels")
                input_ids = inputs.get("input_ids")
                
                # æ£€æŸ¥å¹¶æ¸…ç†logitsä¸­çš„nan/inf
                if logits is not None:
                    has_nan = False
                    has_inf = False
                    
                    # åªæ£€æŸ¥éƒ¨åˆ†æ•°æ®ï¼Œæé«˜æ•ˆç‡
                    if logits.numel() > 0:
                        check_size = min(1000, logits.numel() // 2)
                        if logits.numel() > check_size * 2:
                            head_values = logits.view(-1)[:check_size]
                            tail_values = logits.view(-1)[-check_size:]
                            if torch.isnan(head_values).any() or torch.isnan(tail_values).any():
                                has_nan = True
                            if torch.isinf(head_values).any() or torch.isinf(tail_values).any():
                                has_inf = True
                        else:
                            if torch.isnan(logits).any():
                                has_nan = True
                            if torch.isinf(logits).any():
                                has_inf = True
                    
                    # å¦‚æœå‘ç°é—®é¢˜ï¼Œè¿›è¡Œæ¸…ç†
                    if has_nan or has_inf:
                        nan_count = torch.isnan(logits).sum().item()
                        inf_count = torch.isinf(logits).sum().item()
                        
                        if nan_count > 0 or inf_count > 0:
                            print(f"è­¦å‘Š: Step {self.state.global_step} logitsä¸­æœ‰ {nan_count} ä¸ªnan, {inf_count} ä¸ªinf")
                            logits = torch.where(
                                torch.isnan(logits) | torch.isinf(logits),
                                torch.tensor(0.0, device=logits.device, dtype=logits.dtype),
                                logits
                            )
                            logits = torch.clamp(logits, min=-50.0, max=50.0)
                
                # è®¡ç®—æŸå¤±
                if hasattr(outputs, 'loss') and outputs.loss is not None:
                    loss = outputs.loss
                elif labels is not None:
                    valid_labels_count = (labels != -100).sum().item()
                    
                    if valid_labels_count == 0:
                        print(f"é”™è¯¯: Step {self.state.global_step} æ²¡æœ‰æœ‰æ•ˆçš„labels")
                        loss = torch.tensor(2.0, device=logits.device, requires_grad=True)
                    else:
                        loss_fct = nn.CrossEntropyLoss(ignore_index=-100, reduction='mean')
                        shift_logits = logits[..., :-1, :].contiguous()
                        shift_labels = labels[..., 1:].contiguous()
                        
                        loss = loss_fct(
                            shift_logits.view(-1, shift_logits.size(-1)),
                            shift_labels.view(-1)
                        )
                else:
                    loss = torch.tensor(2.0, device=logits.device, requires_grad=True)
                
                # æ£€æŸ¥æŸå¤±å€¼
                if loss is not None and torch.is_tensor(loss):
                    if loss.dim() > 0:
                        loss = loss.mean()
                    
                    loss_value = loss.item()
                    if torch.isnan(loss) or torch.isinf(loss):
                        print(f"é”™è¯¯: Step {self.state.global_step} lossä¸ºnan/inf")
                        loss = torch.tensor(2.0, device=logits.device, requires_grad=True)
                        loss_value = 2.0
                    elif loss_value > 1e6:
                        print(f"è­¦å‘Š: Step {self.state.global_step} lossè¿‡å¤§ ({loss_value:.2f})")
                        loss = torch.clamp(loss, max=100.0)
                        loss_value = 100.0
                
                # å®šæœŸæ¸…ç†CUDAç¼“å­˜
                if self.state.global_step % 10 == 0:
                    torch.cuda.empty_cache()
                
                if return_outputs:
                    return loss, outputs
                return loss
        
        # åˆ›å»ºæ—©åœå›è°ƒ
        early_stopping = EarlyStoppingCallback(
            early_stopping_patience=early_stopping_patience,
            early_stopping_threshold=early_stopping_threshold
        )
        
        # è®¾ç½®æ—¥å¿—ç›®å½•
        log_dir = os.path.join(self.output_dir, "logs")
        os.makedirs(log_dir, exist_ok=True)
        
        # åˆ›å»º Trainer
        trainer = CustomTrainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=val_dataset,
            data_collator=collate_fn,  # ä½¿ç”¨åŠ¨æ€paddingçš„collate_fn
            processing_class=self.tokenizer,
            callbacks=[early_stopping] if val_dataset else [],
            verbose_logging=True,
            log_dir=log_dir,
        )
        
        # å¼€å§‹è®­ç»ƒ
        print("="*80)
        print("ğŸš€ å¼€å§‹è®­ç»ƒï¼ˆåŠ¨æ€Batch Paddingä¼˜åŒ–ç‰ˆï¼‰")
        print("="*80)
        print(f"è®­ç»ƒæ ·æœ¬æ•°: {len(train_dataset)}")
        if val_dataset:
            print(f"éªŒè¯æ ·æœ¬æ•°: {len(val_dataset)}")
        print(f"ä½¿ç”¨é…ç½®: profile={self.use_profile}, history={self.use_history}, context={self.use_context}")
        print(f"æœ€å¤§åºåˆ—é•¿åº¦: {train_config.get('max_length', 4096)} (åŠ¨æ€padding)")
        print(f"æœ€å¤§è½®æ¬¡: {max_epochs}")
        print(f"æ—©åœè€å¿ƒå€¼: {early_stopping_patience}")
        print("="*80)
        
        trainer.train()
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        print(f"ä¿å­˜æœ€ç»ˆæ¨¡å‹åˆ° {self.output_dir}")
        try:
            trainer.save_model()
            self.tokenizer.save_pretrained(self.output_dir)
            print("âœ“ æ¨¡å‹ä¿å­˜æˆåŠŸ")
        except Exception as e:
            print(f"è­¦å‘Š: ä¿å­˜æ¨¡å‹æ—¶å‡ºé”™: {e}")
        
        # è¾“å‡ºæˆªæ–­ç»Ÿè®¡
        if hasattr(train_dataset, 'get_truncation_stats'):
            stats = train_dataset.get_truncation_stats()
            print("\n" + "="*80)
            print("ğŸ“Š è®­ç»ƒæ•°æ®æˆªæ–­ç»Ÿè®¡:")
            print(f"  æ€»æ ·æœ¬æ•°: {stats['total_samples']}")
            print(f"  è¢«æˆªæ–­æ ·æœ¬æ•°: {stats['truncated_samples']}")
            print(f"  Context æˆªæ–­ç‡: {stats['truncation_rate']:.2%}")
            print(f"  å¹³å‡æˆªæ–­è½®æ¬¡: {stats['avg_truncated_turns']:.2f}")
            
            # å¦‚æœä½¿ç”¨äº†å†å²è®°å½•ï¼Œè¾“å‡ºå†å²è®°å½•ç»Ÿè®¡
            if stats['samples_with_history'] > 0:
                print("\n  ğŸ“š å†å²è®°å½•ç»Ÿè®¡:")
                print(f"    åŒ…å«å†å²è®°å½•çš„æ ·æœ¬æ•°: {stats['samples_with_history']}")
                print(f"    å†å²è®°å½•æ€»æ¡ç›®æ•°: {stats['total_history_items']}")
                print(f"    è¢«æˆªæ–­çš„å†å²æ¡ç›®æ•°: {stats['truncated_history_items']}")
                print(f"    å†å²è®°å½•æˆªæ–­ç‡: {stats['history_truncation_rate']:.2%}")
                print(f"    åŒ…å«è¢«æˆªæ–­å†å²çš„æ ·æœ¬æ•°: {stats['samples_with_history_truncated']}")
                if stats['samples_with_history'] > 0:
                    history_sample_rate = stats['samples_with_history_truncated'] / stats['samples_with_history']
                    print(f"    æ ·æœ¬çº§å†å²æˆªæ–­ç‡: {history_sample_rate:.2%}")
            print("="*80)
        
        print("è®­ç»ƒå®Œæˆï¼")


def main():
    parser = argparse.ArgumentParser(description='æ¶ˆèå®éªŒè®­ç»ƒï¼ˆåŠ¨æ€Paddingä¼˜åŒ–ç‰ˆï¼‰')
    parser.add_argument('--config', type=str,
                       default='/data/lingyu.li/parallel-post-train/ablation/config.json',
                       help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--ablation_config', type=str, required=True,
                       choices=['profile_and_history_and_context', 'profile_and_history', 'profile_and_context', 
                               'history_and_context', 'profile_only', 'history_only', 'context_only'],
                       help='æ¶ˆèå®éªŒé…ç½®')
    parser.add_argument('--val_ratio', type=float, default=0.1,
                       help='éªŒè¯é›†æ¯”ä¾‹')
    parser.add_argument('--gpu', type=int, default=1,
                       help='ä½¿ç”¨çš„GPUç¼–å·ï¼ˆé»˜è®¤ï¼š1ï¼‰')
    parser.add_argument('--max_epochs', type=int, default=50,
                       help='æœ€å¤§è®­ç»ƒè½®æ¬¡ï¼ˆé»˜è®¤ï¼š50ï¼‰')
    parser.add_argument('--early_stopping_patience', type=int, default=3,
                       help='æ—©åœè€å¿ƒå€¼ï¼ˆé»˜è®¤ï¼š3ï¼‰')
    parser.add_argument('--early_stopping_threshold', type=float, default=0.001,
                       help='æ—©åœé˜ˆå€¼ï¼ˆé»˜è®¤ï¼š0.001ï¼‰')
    parser.add_argument('--output_dir', type=str, default=None,
                       help='æ¨¡å‹è¾“å‡ºç›®å½•')
    parser.add_argument('--wandb_project', type=str, default=None,
                       help='Weights & Biasesé¡¹ç›®åç§°')
    parser.add_argument('--wandb_run_name', type=str, default=None,
                       help='Weights & Biasesè¿è¡Œåç§°')
    
    args = parser.parse_args()
    
    # é…ç½® Weights & Biases
    if args.wandb_project:
        try:
            import wandb
            os.environ['WANDB_PROJECT'] = args.wandb_project
            if args.wandb_run_name:
                os.environ['WANDB_NAME'] = args.wandb_run_name
            print(f"âœ“ å·²å¯ç”¨ Weights & Biases ç›‘æ§")
            print(f"  é¡¹ç›®: {args.wandb_project}")
            if args.wandb_run_name:
                print(f"  è¿è¡Œåç§°: {args.wandb_run_name}")
        except ImportError:
            print("è­¦å‘Š: wandb æœªå®‰è£…")
            args.wandb_project = None
    
    # è®¾ç½®GPU
    physical_gpu_id = args.gpu
    os.environ['CUDA_VISIBLE_DEVICES'] = str(physical_gpu_id)
    print(f"=" * 60)
    print(f"GPU è®¾ç½®: ç‰©ç†GPU {physical_gpu_id}")
    print(f"=" * 60)
    
    # éªŒè¯GPU
    if torch.cuda.is_available():
        print(f"CUDA å¯ç”¨ï¼ŒGPU æ•°é‡: {torch.cuda.device_count()}")
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"GPU åç§°: {gpu_name}")
        print(f"GPU æ€»å†…å­˜: {gpu_memory:.2f} GB")
    else:
        print("è­¦å‘Š: CUDA ä¸å¯ç”¨")
    
    # åŠ è½½é…ç½®
    with open(args.config, 'r', encoding='utf-8') as f:
        config = json.load(f)
    
    # è·å–æ¶ˆèé…ç½®
    ablation_config = config['ablation_configs'][args.ablation_config]
    use_profile = ablation_config.get('use_profile', True)
    use_history = ablation_config.get('use_history', True)
    use_context = ablation_config.get('use_context', True)
    config_name = ablation_config['name']
    
    print("=" * 60)
    print(f"æ¶ˆèå®éªŒï¼ˆåŠ¨æ€Paddingä¼˜åŒ–ç‰ˆï¼‰: {config_name}")
    print(f"ä½¿ç”¨é…ç½®: profile={use_profile}, history={use_history}, context={use_context}")
    print("=" * 60)
    
    # åŠ è½½è®­ç»ƒæ•°æ®
    print("åŠ è½½è®­ç»ƒæ•°æ®...")
    train_path = config['data']['train_path']
    train_data = load_train_data(train_path)
    
    if not train_data:
        print(f"é”™è¯¯: æ— æ³•åŠ è½½è®­ç»ƒæ•°æ®")
        return
    
    # æå–è®­ç»ƒæ ·æœ¬
    all_samples = extract_training_samples(train_data, debug=True)
    print(f"æå–äº† {len(all_samples)} ä¸ªè®­ç»ƒæ ·æœ¬")
    
    # æ·»åŠ å†å²ä¿¡æ¯
    if use_history:
        print("æ·»åŠ å†å²ä¿¡æ¯...")
        all_samples = add_history_to_samples(all_samples, all_samples)
    
    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    train_samples, val_samples = split_train_val(all_samples, args.val_ratio)
    print(f"è®­ç»ƒé›†: {len(train_samples)} ä¸ªæ ·æœ¬")
    print(f"éªŒè¯é›†: {len(val_samples)} ä¸ªæ ·æœ¬")
    
    # è·å–æ¨¡å‹é…ç½®
    model_config = config['model']
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    if args.output_dir:
        output_dir = args.output_dir
        os.makedirs(output_dir, exist_ok=True)
        print(f"ä½¿ç”¨æŒ‡å®šçš„è¾“å‡ºç›®å½•: {output_dir}")
    else:
        checkpoint_dir = model_config['checkpoint_dir']
        dataset_name = os.path.basename(os.path.dirname(train_path))
        output_dir = os.path.join(checkpoint_dir, f"{dataset_name}_ablation_{config_name}_dynamic_padding")
        
        try:
            os.makedirs(output_dir, exist_ok=True)
            print(f"è¾“å‡ºç›®å½•: {output_dir}")
        except (OSError, IOError) as e:
            print(f"è­¦å‘Š: æ— æ³•åˆ›å»ºç›®å½•: {e}")
            local_checkpoint_dir = os.path.join(os.path.expanduser("~"), "checkpoints")
            output_dir = os.path.join(local_checkpoint_dir, f"{dataset_name}_ablation_{config_name}_dynamic_padding")
            os.makedirs(output_dir, exist_ok=True)
            print(f"ä½¿ç”¨æœ¬åœ°ç›®å½•: {output_dir}")
    
    # åˆ›å»ºè®­ç»ƒå™¨
    model_path = model_config['path']
    trainer = AblationTrainerWithDynamicPadding(
        model_path=model_path,
        output_dir=output_dir,
        config=config,
        use_profile=use_profile,
        use_history=use_history,
        use_context=use_context
    )
    
    # å¼€å§‹è®­ç»ƒ
    trainer.train(
        train_samples, 
        val_samples,
        max_epochs=args.max_epochs,
        early_stopping_patience=args.early_stopping_patience,
        early_stopping_threshold=args.early_stopping_threshold
    )
    
    print(f"\nâœ… è®­ç»ƒå®Œæˆï¼æ¨¡å‹ä¿å­˜åœ¨: {output_dir}")


if __name__ == '__main__':
    main()
