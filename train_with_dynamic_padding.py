"""
æ¶ˆèå®éªŒè®­ç»ƒè„šæœ¬ï¼ˆå¸¦æ—©åœæœºåˆ¶ + åŠ¨æ€Batch Paddingä¼˜åŒ–ï¼‰
å…³é”®ä¼˜åŒ–ï¼šä¸å†å°†batchå†…æ‰€æœ‰æ ·æœ¬paddingåˆ°å›ºå®šmax_lengthï¼Œ
è€Œæ˜¯åŠ¨æ€paddingåˆ°batchå†…æœ€é•¿æ ·æœ¬çš„å®é™…é•¿åº¦ï¼Œå¤§å¹…èŠ‚çœæ˜¾å­˜ã€‚
"""
import json
import argparse
import os
import sys
from pathlib import Path
import random
import torch

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))
# from data_loader import load_train_data, extract_training_samples, get_user_history_samples, get_user_only_history # æ—§ç‰ˆæœ¬ å¤æ‚çš„è®­ç»ƒprompt  
from data_loader_more_data import load_train_data, extract_training_samples, get_user_only_history # æ–°ç‰ˆæœ¬ ç®€çŸ­çš„è®­ç»ƒprompt
from trainer_pc import AblationTrainer
from transformers import EarlyStoppingCallback, TrainingArguments, Trainer
from typing import List, Dict, Any, Optional
import torch.nn as nn
from torch.utils.data import Dataset


# ============================================================================
# ç»Ÿä¸€çš„ CustomTrainerï¼ˆæ”¯æŒ [ANSWER] æ ‡ç­¾æƒé‡ï¼‰
# ============================================================================

class CustomTrainerWithAnswerWeight(Trainer):
    """
    è‡ªå®šä¹‰ Trainerï¼Œæ”¯æŒ [ANSWER] æ ‡ç­¾æƒé‡å’Œæ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
    
    ç‰¹æ€§ï¼š
    - å¯¹ [ANSWER] å’Œ [/ANSWER] æ ‡ç­¾æœ¬èº«å¢åŠ æƒé‡ 3.0
    - ä¿®æ­£æ¢¯åº¦ç´¯ç§¯å¯¼è‡´çš„ loss æ˜¾ç¤ºé—®é¢˜
    - æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥ï¼ˆnan/inf æ£€æµ‹å’Œæ¸…ç†ï¼‰
    - å¯é€‰çš„è°ƒè¯•ä¿¡æ¯è¾“å‡º
    """
    
    def __init__(self, *args, tokenizer=None, is_main_process=True, rank=0, debug_steps=3, **kwargs):
        """
        Args:
            tokenizer: tokenizer å®ä¾‹ï¼ˆç”¨äº [ANSWER] æ ‡ç­¾æƒé‡è®¡ç®—ï¼‰
            is_main_process: æ˜¯å¦ä¸ºä¸»è¿›ç¨‹ï¼ˆç”¨äºåˆ†å¸ƒå¼è®­ç»ƒï¼‰
            rank: è¿›ç¨‹ rankï¼ˆç”¨äºåˆ†å¸ƒå¼è®­ç»ƒï¼‰
            debug_steps: æ‰“å°è°ƒè¯•ä¿¡æ¯çš„å‰ N ä¸ª step
        """
        super().__init__(*args, **kwargs)
        self.tokenizer = tokenizer
        self.is_main_process = is_main_process
        self.rank = rank
        self.debug_steps = debug_steps
    
    def log(self, logs: Dict[str, float], **kwargs) -> None:
        """ä¿®æ­£æ¢¯åº¦ç´¯ç§¯å¯¼è‡´çš„train_lossæ˜¾ç¤ºé—®é¢˜"""
        if "loss" in logs:
            logs["loss"] = logs["loss"] / self.args.gradient_accumulation_steps
        super().log(logs, **kwargs)
    
    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
        """è®¡ç®—æŸå¤±ï¼ˆå¯¹ [ANSWER] å’Œ [/ANSWER] æ ‡ç­¾æœ¬èº«å¢åŠ æƒé‡ 3ï¼Œå†…å®¹ä¿æŒæƒé‡ 1ï¼‰"""
        # ç§»é™¤actual_lengthå­—æ®µï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        actual_lengths = inputs.pop('actual_length', None)
        
        # å‰Nä¸ªstepæ‰“å°batchä¿¡æ¯ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        if self.is_main_process and self.state.global_step < self.debug_steps:
            batch_size = inputs['input_ids'].shape[0]
            seq_len = inputs['input_ids'].shape[1]
            valid_labels = (inputs['labels'] != -100).sum().item()
            print(f"\n[Step {self.state.global_step}] Batchä¿¡æ¯: size={batch_size}, seq_len={seq_len}, valid_labels={valid_labels}")
        
        outputs = model(**inputs)
        logits = outputs.get("logits")
        labels = inputs.get("labels")
        
        # æ£€æŸ¥å¹¶æ¸…ç†logitsä¸­çš„nan/inf
        if logits is not None and logits.numel() > 0:
            # å¿«é€Ÿé‡‡æ ·æ£€æŸ¥
            check_size = min(1000, logits.numel() // 2)
            if logits.numel() > check_size * 2:
                head_values = logits.view(-1)[:check_size]
                tail_values = logits.view(-1)[-check_size:]
                has_issue = torch.isnan(head_values).any() or torch.isnan(tail_values).any() or \
                            torch.isinf(head_values).any() or torch.isinf(tail_values).any()
            else:
                has_issue = torch.isnan(logits).any() or torch.isinf(logits).any()
            
            if has_issue:
                if self.is_main_process:
                    print(f"è­¦å‘Š: [GPU {self.rank}] Step {self.state.global_step} æ£€æµ‹åˆ°nan/infï¼Œæ­£åœ¨æ¸…ç†...")
                logits = torch.where(
                    torch.isnan(logits) | torch.isinf(logits),
                    torch.tensor(0.0, device=logits.device, dtype=logits.dtype),
                    logits
                )
                logits = torch.clamp(logits, min=-50.0, max=50.0)
        
        # è®¡ç®—æŸå¤±ï¼ˆå¯¹ [ANSWER] å’Œ [/ANSWER] æ ‡ç­¾æœ¬èº«å¢åŠ æƒé‡ 3ï¼Œå†…å®¹ä¿æŒæƒé‡ 1ï¼‰
        if hasattr(outputs, 'loss') and outputs.loss is not None:
            loss = outputs.loss
        elif labels is not None:
            valid_labels_count = (labels != -100).sum().item()
            
            if valid_labels_count == 0:
                if self.is_main_process:
                    print(f"è­¦å‘Š: [GPU {self.rank}] Step {self.state.global_step} æ²¡æœ‰æœ‰æ•ˆçš„labels")
                loss = torch.tensor(2.0, device=logits.device, requires_grad=True)
            else:
                shift_logits = logits[..., :-1, :].contiguous()
                shift_labels = labels[..., 1:].contiguous()
                
                # åˆ›å»ºæŸå¤±æƒé‡ï¼šå¯¹ [ANSWER] å’Œ [/ANSWER] token å¢åŠ æƒé‡
                # è·å– tokenizer ä¸­çš„ [ANSWER] å’Œ [/ANSWER] çš„æ‰€æœ‰ token IDs
                answer_start_token_ids = set()
                answer_end_token_ids = set()
                
                try:
                    # å°è¯•è·å– [ANSWER] å’Œ [/ANSWER] çš„æ‰€æœ‰ token IDs
                    if self.tokenizer and hasattr(self.tokenizer, 'encode'):
                        # ç¼–ç æ ‡ç­¾ï¼ˆå¯èƒ½è¢«ç¼–ç ä¸ºå¤šä¸ª tokenï¼‰
                        answer_start_tokens = self.tokenizer.encode("[ANSWER]", add_special_tokens=False)
                        answer_end_tokens = self.tokenizer.encode("[/ANSWER]", add_special_tokens=False)
                        
                        # ä¿å­˜æ‰€æœ‰ç›¸å…³çš„ token IDsï¼ˆä¸ä»…ä»…æ˜¯ç¬¬ä¸€ä¸ªï¼‰
                        if answer_start_tokens:
                            answer_start_token_ids = set(answer_start_tokens)
                        if answer_end_tokens:
                            answer_end_token_ids = set(answer_end_tokens)
                except:
                    pass
                
                # åˆ›å»ºæƒé‡å¼ é‡ï¼ˆé»˜è®¤æƒé‡ä¸º 1.0ï¼‰
                batch_size, seq_len = shift_labels.shape
                loss_weights = torch.ones_like(shift_labels, dtype=torch.float32)
                
                # å¯¹ [ANSWER] å’Œ [/ANSWER] æ ‡ç­¾æœ¬èº«çš„æ‰€æœ‰ token å¢åŠ æƒé‡ï¼ˆæƒé‡è®¾ä¸º 3.0ï¼‰
                # æ³¨æ„ï¼šæ ‡ç­¾ä¹‹é—´çš„å†…å®¹ä¿æŒæ™®é€šæƒé‡ 1.0
                answer_tag_count = 0
                if answer_start_token_ids:
                    for token_id in answer_start_token_ids:
                        count = (shift_labels == token_id).sum().item()
                        loss_weights[shift_labels == token_id] = 3.0
                        answer_tag_count += count
                if answer_end_token_ids:
                    for token_id in answer_end_token_ids:
                        count = (shift_labels == token_id).sum().item()
                        loss_weights[shift_labels == token_id] = 3.0
                        answer_tag_count += count
                
                # å‰Nä¸ªstepæ‰“å°æƒé‡ä¿¡æ¯ï¼ˆç”¨äºè°ƒè¯•ï¼‰
                if self.is_main_process and self.state.global_step < self.debug_steps:
                    weighted_tokens = (loss_weights == 3.0).sum().item()
                    print(f"  [ANSWER]æ ‡ç­¾tokens: {answer_tag_count}, åŠ æƒtokensæ€»æ•°: {weighted_tokens}")
                
                # ä½¿ç”¨åŠ æƒæŸå¤±
                loss_fct = nn.CrossEntropyLoss(ignore_index=-100, reduction='none')
                per_token_loss = loss_fct(
                    shift_logits.view(-1, shift_logits.size(-1)),
                    shift_labels.view(-1)
                )
                
                # åº”ç”¨æƒé‡å¹¶è®¡ç®—å¹³å‡æŸå¤±
                per_token_loss = per_token_loss.view(batch_size, seq_len)
                valid_mask = (shift_labels != -100)
                weighted_loss = (per_token_loss * loss_weights * valid_mask.float()).sum()
                valid_count = (valid_mask.float() * loss_weights).sum()
                
                if valid_count > 0:
                    loss = weighted_loss / valid_count
                    # å‰Nä¸ªstepæ‰“å°æŸå¤±ä¿¡æ¯ï¼ˆç”¨äºè°ƒè¯•ï¼‰
                    if self.is_main_process and self.state.global_step < self.debug_steps:
                        print(f"  è®¡ç®—æŸå¤±: weighted_loss={weighted_loss.item():.4f}, valid_count={valid_count.item():.0f}, final_loss={loss.item():.4f}")
                else:
                    loss = torch.tensor(2.0, device=logits.device, requires_grad=True)
        else:
            loss = torch.tensor(2.0, device=logits.device, requires_grad=True)
        
        # æ£€æŸ¥æŸå¤±å€¼
        if loss is not None and torch.is_tensor(loss):
            if loss.dim() > 0:
                loss = loss.mean()
            
            if torch.isnan(loss) or torch.isinf(loss):
                if self.is_main_process:
                    print(f"è­¦å‘Š: [GPU {self.rank}] Step {self.state.global_step} lossä¸ºnan/inf")
                loss = torch.tensor(2.0, device=logits.device, requires_grad=True)
            elif loss.item() > 1e6:
                if self.is_main_process:
                    print(f"è­¦å‘Š: [GPU {self.rank}] Step {self.state.global_step} lossè¿‡å¤§")
                loss = torch.clamp(loss, max=100.0)
        
        # å®šæœŸæ¸…ç†CUDAç¼“å­˜
        if self.state.global_step % 10 == 0:
            torch.cuda.empty_cache()
        
        if return_outputs:
            return loss, outputs
        return loss


def split_train_val(samples, val_ratio=0.1, seed=42):
    """
    åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼ˆç”¨æˆ·å†…åˆ’åˆ†ï¼Œä¿è¯æ¯ä¸ªç”¨æˆ·åœ¨è®­ç»ƒå’ŒéªŒè¯é›†éƒ½æœ‰æ ·æœ¬ï¼‰
    
    ç­–ç•¥ï¼šå¯¹æ¯ä¸ªç”¨æˆ·çš„æ ·æœ¬è¿›è¡Œéšæœºæ‰“ä¹±åæŒ‰æ¯”ä¾‹åˆ’åˆ†
    - é€‚ç”¨åœºæ™¯ï¼šæµ‹è¯•é›†ä¸­çš„ç”¨æˆ·ä¹Ÿå‡ºç°åœ¨è®­ç»ƒé›†ä¸­
    - ç›®æ ‡ï¼šå­¦ä¹ åŸºäºç”¨æˆ·å·²æœ‰å¯¹è¯é¢„æµ‹æ–°å¯¹è¯ï¼ˆç”¨æˆ·å†…æ³›åŒ–ï¼‰
    
    Args:
        samples: æ‰€æœ‰è®­ç»ƒæ ·æœ¬
        val_ratio: éªŒè¯é›†æ¯”ä¾‹ï¼ˆé»˜è®¤0.15ï¼Œå³15%ï¼‰
        seed: éšæœºç§å­
    
    Returns:
        (train_samples, val_samples)
    """
    random.seed(seed)
    
    # æŒ‰ç”¨æˆ·åˆ†ç»„
    user_samples = {}
    for sample in samples:
        user_hash = sample['user_hash']
        if user_hash not in user_samples:
            user_samples[user_hash] = []
        user_samples[user_hash].append(sample)
    
    train_samples = []
    val_samples = []
    
    # å¯¹æ¯ä¸ªç”¨æˆ·çš„æ ·æœ¬è¿›è¡Œåˆ’åˆ†
    for user_hash, user_data in user_samples.items():
        # éšæœºæ‰“ä¹±è¯¥ç”¨æˆ·çš„æ ·æœ¬
        random.shuffle(user_data)
        
        # è®¡ç®—åˆ’åˆ†ç‚¹ï¼š(1 - val_ratio) çš„æ ·æœ¬ç”¨äºè®­ç»ƒ
        split_idx = int(len(user_data) * (1 - val_ratio))
        
        # ç¡®ä¿è‡³å°‘æœ‰1ä¸ªæ ·æœ¬åœ¨è®­ç»ƒé›†ï¼ˆå¦‚æœè¯¥ç”¨æˆ·åªæœ‰1ä¸ªæ ·æœ¬ï¼Œå…¨éƒ¨ç»™è®­ç»ƒé›†ï¼‰
        if split_idx == 0 and len(user_data) > 0:
            split_idx = 1
        
        # åˆ’åˆ†
        train_samples.extend(user_data[:split_idx])
        val_samples.extend(user_data[split_idx:])
    
    return train_samples, val_samples


def add_history_to_samples(train_samples, all_samples):
    """ä¸ºæ¯ä¸ªæ ·æœ¬æ·»åŠ å†å²ä¿¡æ¯ï¼ˆåªåŒ…å«ç”¨æˆ·çš„é—®é¢˜ï¼Œä¸åŒ…å«assistantå†…å®¹ï¼‰"""
    # ä¼˜åŒ–ï¼šå…ˆæŒ‰ç”¨æˆ·åˆ†ç»„ï¼Œé¿å…é‡å¤éå†æ•´ä¸ªall_samplesåˆ—è¡¨
    from collections import defaultdict
    
    print(f"  å¼€å§‹ä¼˜åŒ–å†å²ä¿¡æ¯æ·»åŠ ï¼ˆæ€»æ ·æœ¬æ•°: {len(all_samples)}ï¼‰...")
    
    # æŒ‰ç”¨æˆ·åˆ†ç»„æ‰€æœ‰æ ·æœ¬ï¼ˆO(n) å¤æ‚åº¦ï¼‰
    user_samples_map = defaultdict(list)
    for sample in all_samples:
        user_hash = sample.get('user_hash', 'unknown')
        user_samples_map[user_hash].append(sample)
    
    print(f"  å·²æŒ‰ç”¨æˆ·åˆ†ç»„ï¼Œå…± {len(user_samples_map)} ä¸ªç”¨æˆ·")
    
    # ä¸ºæ¯ä¸ªæ ·æœ¬æ·»åŠ å†å²ä¿¡æ¯
    samples_with_history = []
    total_samples = len(train_samples)
    
    for idx, sample in enumerate(train_samples):
        user_hash = sample['user_hash']
        
        # åªéå†è¯¥ç”¨æˆ·çš„æ ·æœ¬ï¼Œè€Œä¸æ˜¯æ‰€æœ‰æ ·æœ¬
        # è¿™å¤§å¤§å‡å°‘äº†éå†æ¬¡æ•°ï¼šä» O(nÂ²) é™ä½åˆ° O(n * avg_samples_per_user)
        user_samples = user_samples_map.get(user_hash, [])
        
        # ä½¿ç”¨ä¼˜åŒ–ç‰ˆæœ¬ï¼šç›´æ¥åœ¨è¯¥ç”¨æˆ·çš„æ ·æœ¬ä¸­æŸ¥æ‰¾å†å²
        history = get_user_only_history(
            user_samples,  # åªä¼ å…¥è¯¥ç”¨æˆ·çš„æ ·æœ¬ï¼Œè€Œä¸æ˜¯æ‰€æœ‰æ ·æœ¬
            user_hash,
            current_sample=sample,
            current_context=sample.get('context'),
            max_history=15,
            use_cache=True
        )
        sample['history'] = history
        samples_with_history.append(sample)
        
        # æ¯å¤„ç†10000ä¸ªæ ·æœ¬æ‰“å°ä¸€æ¬¡è¿›åº¦
        if (idx + 1) % 10000 == 0:
            print(f"  å·²å¤„ç† {idx + 1}/{total_samples} ä¸ªæ ·æœ¬ ({(idx + 1) / total_samples * 100:.1f}%)")
    
    print(f"  âœ“ å†å²ä¿¡æ¯æ·»åŠ å®Œæˆ")
    return samples_with_history


class DynamicPaddingDataset(Dataset):
    """
    ä¼˜åŒ–ç‰ˆæ•°æ®é›†ï¼šä¸åšpaddingï¼Œè¿”å›åŸå§‹é•¿åº¦çš„tensor
    paddingå°†åœ¨collate_fnä¸­åŠ¨æ€è¿›è¡Œ
    """
    def __init__(self, samples, tokenizer, max_length=32768, use_profile=True, use_history=True, use_context=True, verbose=False, use_detailed_template=True, max_context_turns=15, template_filename=None, require_token_type_ids=None, custom_prompt_builder=None):
        # ä½¿ç”¨ç»å¯¹è·¯å¾„å¯¼å…¥ï¼Œç¡®ä¿ä½¿ç”¨å½“å‰ç›®å½•çš„æ¨¡å—
        import sys
        from pathlib import Path
        current_dir = str(Path(__file__).parent.absolute())
        if current_dir not in sys.path:
            sys.path.insert(0, current_dir)
        
        # âœ… ä¼˜å…ˆä½¿ç”¨å¤–éƒ¨ä¼ å…¥çš„ prompt æ„å»ºå‡½æ•°ï¼ˆç”¨äºç‰¹æ®Šæ•°æ®é›†ï¼Œå¦‚é—®å·ï¼‰
        if custom_prompt_builder is not None:
            print("âœ… ä½¿ç”¨å¤–éƒ¨ä¼ å…¥çš„è‡ªå®šä¹‰ Prompt æ„å»ºå‡½æ•°")
            self.build_training_prompt = custom_prompt_builder
        # âœ… æ ¹æ® use_detailed_template é€‰æ‹© prompt æ„å»ºå‡½æ•°
        elif use_detailed_template:
            # ä½¿ç”¨è¯¦ç»†æ¨¡æ¿ï¼ˆæ ‡å‡† markdown æ ¼å¼ï¼Œä½¿ç”¨ {VAR_NAME} å ä½ç¬¦ï¼‰
            from prompt_builder import build_training_prompt
            print("ä½¿ç”¨è¯¦ç»† Prompt æ¨¡æ¿ (prompt_builder)")
            self.build_training_prompt = build_training_prompt
        else:
            # ä½¿ç”¨ç®€çŸ­æ¨¡æ¿
            # ä¼˜å…ˆä½¿ç”¨ data_loader_more_data.pyï¼ˆåŒ…å« [ANSWER] æ ‡ç­¾ï¼Œä¸æ¨ç†è„šæœ¬ä¸€è‡´ï¼‰
            # å¦‚æœå¤±è´¥ï¼Œåˆ™ä» data_loader.py å¯¼å…¥ï¼ˆå¤‡ç”¨ç‰ˆæœ¬ï¼‰
            try:
                from data_loader_more_data import build_simple_training_prompt as build_training_prompt
                print("âœ… ä½¿ç”¨ç®€çŸ­ Prompt æ¨¡æ¿ (data_loader_more_data.build_simple_training_prompt - åŒ…å« [ANSWER] æ ‡ç­¾)")
                self.build_training_prompt = build_training_prompt
            except ImportError:
                from data_loader import build_simple_training_prompt as build_training_prompt
                print("âš ï¸  ä½¿ç”¨ç®€çŸ­ Prompt æ¨¡æ¿ (data_loader.build_simple_training_prompt - å¤‡ç”¨ç‰ˆæœ¬ï¼Œä¸åŒ…å« [ANSWER] æ ‡ç­¾)")
                self.build_training_prompt = build_training_prompt
        
        self.samples = samples
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.use_profile = use_profile
        self.use_history = use_history
        self.use_context = use_context
        self.use_detailed_template = use_detailed_template  # æ˜¯å¦ä½¿ç”¨è¯¦ç»†æ¨¡æ¿
        self.max_context_turns = max_context_turns  # æ–°å¢ï¼šæœ€å¤§ä¿ç•™çš„ context è½®æ¬¡æ•°
        self.template_filename = template_filename  # æ–°å¢ï¼šæ¨¡æ¿æ–‡ä»¶å
        self.verbose = verbose  # æ˜¯å¦è¾“å‡ºè¯¦ç»†æ—¥å¿—
        
        # âœ… è‡ªåŠ¨æ£€æµ‹æ˜¯å¦éœ€è¦ token_type_idsï¼ˆç”¨äº Gemma3 æ¨¡å‹ï¼‰
        if require_token_type_ids is None:
            # æ ¹æ® tokenizer è‡ªåŠ¨åˆ¤æ–­
            model_type = getattr(tokenizer, 'name_or_path', '').lower()
            self.require_token_type_ids = 'gemma' in model_type
            if self.require_token_type_ids and verbose:
                print(f"âœ… æ£€æµ‹åˆ° Gemma æ¨¡å‹ï¼Œå°†æ·»åŠ  token_type_ids")
        else:
            self.require_token_type_ids = require_token_type_ids
        
        # æˆªæ–­ç»Ÿè®¡
        self.truncation_stats = {
            'total_samples': 0,
            'truncated_samples': 0,
            'truncated_turns': 0,
            # å†å²è®°å½•ç»Ÿè®¡
            'total_history_items': 0,
            'truncated_history_items': 0,
            'samples_with_history': 0,
            'samples_with_history_truncated': 0
        }
        
        # ç”¨äºè®°å½•ç¬¬ä¸€æ¬¡æˆªæ–­çš„æ ·æœ¬ä¿¡æ¯ï¼ˆè°ƒè¯•ç”¨ï¼‰
        self.first_truncation_logged = False

    def __len__(self):
        return len(self.samples)
    
    def get_truncation_stats(self):
        """è·å–æˆªæ–­ç»Ÿè®¡ä¿¡æ¯"""
        if self.truncation_stats['total_samples'] == 0:
            return {
                'truncation_rate': 0.0,
                'avg_truncated_turns': 0.0,
                'total_samples': 0,
                'truncated_samples': 0,
                # å†å²è®°å½•ç»Ÿè®¡
                'history_truncation_rate': 0.0,
                'total_history_items': 0,
                'truncated_history_items': 0,
                'samples_with_history': 0,
                'samples_with_history_truncated': 0
            }
        
        truncation_rate = self.truncation_stats['truncated_samples'] / self.truncation_stats['total_samples']
        avg_truncated_turns = (self.truncation_stats['truncated_turns'] / self.truncation_stats['truncated_samples'] 
                               if self.truncation_stats['truncated_samples'] > 0 else 0)
        
        # è®¡ç®—å†å²è®°å½•æˆªæ–­ç‡
        history_truncation_rate = 0.0
        if self.truncation_stats['total_history_items'] > 0:
            history_truncation_rate = self.truncation_stats['truncated_history_items'] / self.truncation_stats['total_history_items']
        
        return {
            'truncation_rate': truncation_rate,
            'avg_truncated_turns': avg_truncated_turns,
            'total_samples': self.truncation_stats['total_samples'],
            'truncated_samples': self.truncation_stats['truncated_samples'],
            # å†å²è®°å½•ç»Ÿè®¡
            'history_truncation_rate': history_truncation_rate,
            'total_history_items': self.truncation_stats['total_history_items'],
            'truncated_history_items': self.truncation_stats['truncated_history_items'],
            'samples_with_history': self.truncation_stats['samples_with_history'],
            'samples_with_history_truncated': self.truncation_stats['samples_with_history_truncated']
        }

    def __getitem__(self, idx):
        sample = self.samples[idx]
        
        # ç»Ÿè®¡å†å²è®°å½•ä¿¡æ¯
        original_history = sample.get('history', []) if self.use_history else []
        has_history = len(original_history) > 0
        original_history_count = len(original_history)
        
        if has_history:
            self.truncation_stats['samples_with_history'] += 1
            self.truncation_stats['total_history_items'] += original_history_count
        
        # 1. æ„å»ºæ¶ˆæ¯
        # âœ… æ ¹æ®æ¨¡æ¿ç±»å‹ï¼Œä¼ é€’ä¸åŒçš„å‚æ•°
        if self.use_detailed_template:
            # è¯¦ç»†æ¨¡æ¿éœ€è¦é¢å¤–çš„å‚æ•°
            messages, target_answer = self.build_training_prompt(
                context=sample['context'],
                next_question=sample['next_question'],
                user_profile=sample.get('user_profile') if self.use_profile else None,
                task_description=sample.get('task_description'),
                history=original_history,
                use_profile=self.use_profile,
                use_history=self.use_history,
                use_context=self.use_context,
                use_detailed_template=self.use_detailed_template,
                max_context_turns=self.max_context_turns,
                tokenizer=self.tokenizer,
                template_filename=self.template_filename  # âœ… ä¼ é€’æ¨¡æ¿æ–‡ä»¶å
            )
        else:
            # ç®€çŸ­æ¨¡æ¿ - âœ… æ·»åŠ  tokenizer å’Œ max_length ç”¨äºåŠ¨æ€é•¿åº¦è°ƒæ•´
            messages, target_answer = self.build_training_prompt(
                context=sample['context'],
                next_question=sample['next_question'],
                user_profile=sample.get('user_profile') if self.use_profile else None,
                task_description=sample.get('task_description'),
                history=original_history,
                use_profile=self.use_profile,
                use_history=self.use_history,
                use_context=self.use_context,
                tokenizer=self.tokenizer,         # âœ… ä¼ é€’ tokenizer
                max_length=self.max_length,       # âœ… ä¼ é€’ max_length
                min_target_tokens=64,             # âœ… é¢„ç•™ 64 tokens ç»™ target
                user_hash=sample.get('user_hash')  # âœ… ä¼ é€’ user_hashï¼ˆå§‹ç»ˆåŒ…å«ï¼‰
            )


        # æ£€æŸ¥å†å²è®°å½•æ˜¯å¦è¢«æˆªæ–­ï¼ˆåœ¨ prompt_builder ä¸­é™åˆ¶ä¸ºå‰5ä¸ªï¼‰
        if has_history and original_history_count > 5:
            truncated_history_count = original_history_count - 5
            self.truncation_stats['truncated_history_items'] += truncated_history_count
            self.truncation_stats['samples_with_history_truncated'] += 1


        # 2. ç”Ÿæˆå®Œæ•´æ–‡æœ¬
        # âœ… ç‰¹æ®Šå¤„ç† Gemma3 æ¨¡å‹çš„ chat template
        is_gemma = 'gemma' in getattr(self.tokenizer, 'name_or_path', '').lower()
        
        if is_gemma:
            # Gemma3 ä¸æ”¯æŒ system roleï¼Œéœ€è¦æ‰‹åŠ¨æ„å»ºæ ¼å¼
            # Gemma3 æ ¼å¼: <bos><start_of_turn>user\n{content}<end_of_turn>\n<start_of_turn>model\n{response}<end_of_turn>
            if messages and messages[0].get('role') == 'system':
                system_content = messages[0]['content']
                # å°† system content ä½œä¸º user çš„ç¬¬ä¸€æ¡æ¶ˆæ¯
                full_prompt = f"<start_of_turn>user\n{system_content}<end_of_turn>\n<start_of_turn>model\n"
                full_text = full_prompt + target_answer + "<end_of_turn>"
            else:
                # å¦‚æœæ²¡æœ‰ system messageï¼Œç›´æ¥ç”Ÿæˆ
                full_prompt = "<start_of_turn>model\n"
                full_text = full_prompt + target_answer + "<end_of_turn>"
        else:
            # Qwen å’Œå…¶ä»–æ¨¡å‹ä½¿ç”¨æ ‡å‡†çš„ chat template
            full_prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)
            # ä¿®æ­£ï¼šåº”è¯¥ç”Ÿæˆassistantè§’è‰²çš„å›å¤ï¼ˆç›®æ ‡ç”¨æˆ·ï¼‰
            generation_suffix = "<|im_start|>assistant\n"
            full_prompt = full_prompt.strip() + generation_suffix
            im_end_token = "<|im_end|>"
            full_text = full_prompt + target_answer + im_end_token
        
        # âœ… ç¬¬äºŒå±‚ä¿æŠ¤ï¼šå¦‚æœä»ç„¶è¶…é•¿ï¼Œé€æ­¥ä»å‰å¾€ååˆ é™¤å¯¹è¯è½®æ¬¡
        im_end_token = "<end_of_turn>" if is_gemma else "<|im_end|>"
        target_with_end = target_answer + im_end_token
        target_tokens = len(self.tokenizer.encode(target_with_end, add_special_tokens=False))
        min_buffer = 64
        
        full_length = len(self.tokenizer.encode(full_text, add_special_tokens=False))
        is_truncated = False
        removed_turns = 0
        
        if full_length > self.max_length:
            is_truncated = True
            
            # å…è®¸çš„æœ€å¤§ prompt é•¿åº¦
            max_prompt_tokens = self.max_length - target_tokens - min_buffer
            
            # å¦‚æœæœ‰ RECENT_DIALOGUE éƒ¨åˆ†ï¼Œé€æ­¥ä»å‰å¾€ååˆ é™¤æ—§å¯¹è¯
            if len(messages) > 0 and messages[0].get('role') == 'system':
                system_content = messages[0]['content']
                
                if '[RECENT_DIALOGUE]' in system_content:
                    # è§£æ dialogue éƒ¨åˆ†
                    parts = system_content.split('[RECENT_DIALOGUE]')
                    if len(parts) > 1:
                        prefix = parts[0].strip()  # Profile + Task
                        dialogue_section = parts[1].strip()
                        
                        # æå–å¯¹è¯è¡Œï¼ˆè·³è¿‡ "Predict the user's next message:"ï¼‰
                        dialogue_lines = []
                        for line in dialogue_section.split('\n'):
                            line = line.strip()
                            if line and not line.startswith('Predict') and not line.startswith('ï¼ˆå‰é¢çœç•¥'):
                                if line.startswith('User:') or line.startswith('Assistant:'):
                                    dialogue_lines.append(line)
                        
                        # ä»å‰å¾€åé€æ­¥åˆ é™¤å¯¹è¯è½®æ¬¡ï¼Œç›´åˆ°é•¿åº¦åˆé€‚
                        while dialogue_lines and full_length > self.max_length:
                            # åˆ é™¤æœ€æ—§çš„ä¸€è½®ï¼ˆç¬¬ä¸€ä¸ªï¼‰
                            dialogue_lines.pop(0)
                            removed_turns += 1
                            
                            # é‡å»º system message
                            if removed_turns > 0 and dialogue_lines:
                                new_dialogue = f"\n[RECENT_DIALOGUE]\nï¼ˆå‰é¢çœç•¥äº† {removed_turns} è½®å¯¹è¯ï¼‰\n" + "\n".join(dialogue_lines)
                            elif dialogue_lines:
                                new_dialogue = "\n[RECENT_DIALOGUE]\n" + "\n".join(dialogue_lines)
                            else:
                                new_dialogue = ""
                            
                            new_system = prefix + new_dialogue + "\n\nPredict the user's next message:"
                            messages[0]['content'] = new_system
                            
                            # é‡æ–°ç”Ÿæˆå¹¶æµ‹è¯•é•¿åº¦
                            if is_gemma:
                                full_prompt = f"<start_of_turn>user\n{messages[0]['content']}<end_of_turn>\n<start_of_turn>model\n"
                                full_text = full_prompt + target_answer + "<end_of_turn>"
                            else:
                                full_prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)
                                generation_suffix = "<|im_start|>assistant\n"
                                full_prompt = full_prompt.strip() + generation_suffix
                                full_text = full_prompt + target_answer + im_end_token
                            
                            full_length = len(self.tokenizer.encode(full_text, add_special_tokens=False))
        
        # æ›´æ–°æˆªæ–­ç»Ÿè®¡
        self.truncation_stats['total_samples'] += 1
        if is_truncated:
            self.truncation_stats['truncated_samples'] += 1
            self.truncation_stats['truncated_turns'] += removed_turns
            
            # ç¬¬ä¸€æ¬¡é‡åˆ°æˆªæ–­æ—¶è¾“å‡ºæ—¥å¿—
            if not self.first_truncation_logged and self.verbose:
                self.first_truncation_logged = True
                print(f"\nâš ï¸  ç¬¬äºŒå±‚ä¿æŠ¤ï¼šé€æ­¥åˆ é™¤æ—§å¯¹è¯ (æ ·æœ¬#{idx}):")
                print(f"  åˆ é™¤äº† {removed_turns} è½®å¯¹è¯ï¼ˆä»æœ€æ—§çš„å¼€å§‹ï¼‰")
                print(f"  è°ƒæ•´åé•¿åº¦: {full_length} tokens")
                print(f"  æœ€å¤§é•¿åº¦: {self.max_length} tokens")
                print(f"  Target é•¿åº¦: {target_tokens} tokens (å·²å®Œæ•´ä¿ç•™)")
                print(f"  (åç»­æˆªæ–­å°†ä¸å†è¾“å‡ºè¯¦ç»†ä¿¡æ¯)\n")

        # 3. ç¼–ç  - å…³é”®ï¼šä¸åšpaddingï¼
        encoded = self.tokenizer(
            full_text,
            truncation=True,
            max_length=self.max_length,
            padding=False,  # å…³é”®æ”¹åŠ¨ï¼šä¸padding
            return_tensors='pt'
        )
        
        input_ids = encoded['input_ids'].squeeze()
        attention_mask = encoded['attention_mask'].squeeze()

        # 4. è®¡ç®—labels
        target_ids = self.tokenizer.encode(target_answer, add_special_tokens=False)
        prompt_ids = self.tokenizer.encode(full_prompt, add_special_tokens=False)
        actual_prompt_len = len(prompt_ids)

        labels = input_ids.clone()
        safe_prompt_len = min(actual_prompt_len, len(input_ids) - 1)
        labels[:safe_prompt_len] = -100
        
        # å±è”½padding tokenï¼ˆè™½ç„¶ç°åœ¨æ²¡æœ‰paddingï¼Œä½†ä¸ºäº†å…¼å®¹æ€§ä¿ç•™ï¼‰
        labels[input_ids == self.tokenizer.pad_token_id] = -100

        result = {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'labels': labels,
            'actual_length': len(input_ids)  # è®°å½•å®é™…é•¿åº¦ï¼Œç”¨äºè°ƒè¯•
        }
        
        # âœ… åªæœ‰ Gemma3 æ¨¡å‹éœ€è¦æ—¶æ‰æ·»åŠ  token_type_ids
        if self.require_token_type_ids:
            token_type_ids = torch.zeros_like(input_ids)
            result['token_type_ids'] = token_type_ids
        
        return result


def dynamic_padding_collate_fn(examples, tokenizer):
    """
    åŠ¨æ€Paddingçš„collateå‡½æ•°
    å…³é”®ä¼˜åŒ–ï¼šåªpaddingåˆ°batchå†…æœ€é•¿æ ·æœ¬çš„é•¿åº¦ï¼Œè€Œä¸æ˜¯å›ºå®šçš„max_length
    """
    # æ‰¾åˆ°batchä¸­æœ€é•¿çš„åºåˆ—é•¿åº¦
    max_length_in_batch = max(ex['input_ids'].shape[0] for ex in examples)
    
    # æ‰“å°batchä¿¡æ¯ï¼ˆç”¨äºè°ƒè¯•ï¼‰
    lengths = [ex['input_ids'].shape[0] for ex in examples]
    if random.random() < 0.05:  # 5%çš„æ¦‚ç‡æ‰“å°ï¼Œé¿å…åˆ·å±
        print(f"[Batch Info] Lengths: {lengths}, Max: {max_length_in_batch}, Avg: {sum(lengths)/len(lengths):.0f}")
    
    batch = {}
    
    # åŠ¨æ€paddingæ¯ä¸ªå­—æ®µ
    padded_input_ids = []
    padded_attention_mask = []
    padded_labels = []
    padded_token_type_ids = []  # âœ… æ·»åŠ  token_type_ids
    
    for ex in examples:
        seq_len = ex['input_ids'].shape[0]
        pad_len = max_length_in_batch - seq_len
        
        # Padding input_ids
        padded_input_ids.append(
            torch.cat([
                ex['input_ids'],
                torch.full((pad_len,), tokenizer.pad_token_id, dtype=torch.long)
            ])
        )
        
        # Padding attention_mask
        padded_attention_mask.append(
            torch.cat([
                ex['attention_mask'],
                torch.zeros(pad_len, dtype=torch.long)
            ])
        )
        
        # Padding labels
        padded_labels.append(
            torch.cat([
                ex['labels'],
                torch.full((pad_len,), -100, dtype=torch.long)
            ])
        )
        
        # âœ… Padding token_type_ids
        if 'token_type_ids' in ex:
            padded_token_type_ids.append(
                torch.cat([
                    ex['token_type_ids'],
                    torch.zeros(pad_len, dtype=torch.long)
                ])
            )
    
    batch['input_ids'] = torch.stack(padded_input_ids)
    batch['attention_mask'] = torch.stack(padded_attention_mask)
    batch['labels'] = torch.stack(padded_labels)
    
    # âœ… æ·»åŠ  token_type_ids åˆ° batch
    if padded_token_type_ids:
        batch['token_type_ids'] = torch.stack(padded_token_type_ids)
    
    # æ·»åŠ å…¶ä»–å…ƒä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
    if 'actual_length' in examples[0]:
        batch['actual_length'] = [ex['actual_length'] for ex in examples]
    
    return batch


class AblationTrainerWithDynamicPadding(AblationTrainer):
    """å¸¦æ—©åœ + åŠ¨æ€Paddingçš„è®­ç»ƒå™¨"""
    
    def train(
        self,
        train_samples: List[Dict[str, Any]],
        val_samples: Optional[List[Dict[str, Any]]] = None,
        max_epochs: int = 10,
        early_stopping_patience: int = 3,
        early_stopping_threshold: float = 0.00001
    ):
        """è®­ç»ƒæ¨¡å‹ï¼ˆå¸¦æ—©åœ + åŠ¨æ€Paddingï¼‰"""
        train_config = self.config.get('training', {})
        
        # åˆ›å»ºæ•°æ®é›†ï¼ˆä½¿ç”¨åŠ¨æ€Paddingç‰ˆæœ¬ï¼‰
        print("åˆ›å»ºè®­ç»ƒæ•°æ®é›†ï¼ˆåŠ¨æ€Paddingæ¨¡å¼ï¼‰...")
        train_dataset = DynamicPaddingDataset(
            samples=train_samples,
            tokenizer=self.tokenizer,
            max_length=train_config.get('max_length', 4096),
            use_profile=self.use_profile,
            use_history=self.use_history,
            use_context=self.use_context,
            max_context_turns=train_config.get('max_context_turns', 15)  # æ–°å¢ï¼šä» config è¯»å–
        )
        
        val_dataset = None
        if val_samples:
            print("åˆ›å»ºéªŒè¯æ•°æ®é›†ï¼ˆåŠ¨æ€Paddingæ¨¡å¼ï¼‰...")
            val_dataset = DynamicPaddingDataset(
                samples=val_samples,
                tokenizer=self.tokenizer,
                max_length=train_config.get('max_length', 4096),
                use_profile=self.use_profile,
                use_history=self.use_history,
                use_context=self.use_context,
                max_context_turns=train_config.get('max_context_turns', 15)  # æ–°å¢
            )
        
        # æ•°æ®æ•´ç†å™¨ï¼ˆåŠ¨æ€Paddingï¼‰
        def collate_fn(examples):
            return dynamic_padding_collate_fn(examples, self.tokenizer)
        
        # è®¡ç®—æ¯ä¸ªepochçš„æ­¥æ•°å’Œè¯„ä¼°æ­¥æ•°
        steps_per_epoch = len(train_dataset) // (train_config.get('batch_size', 1) * train_config.get('gradient_accumulation_steps', 16))
        eval_steps_value = max(1, steps_per_epoch // 2) if val_dataset else None
        
        # è°ƒæ•´ save_steps
        save_steps_value = train_config.get('save_steps', 500)
        if val_dataset and eval_steps_value and save_steps_value % eval_steps_value != 0:
            save_steps_value = ((save_steps_value + eval_steps_value - 1) // eval_steps_value) * eval_steps_value
            print(f"è°ƒæ•´ save_steps ä¸º {save_steps_value}ï¼ˆeval_steps={eval_steps_value} çš„æ•´æ•°å€ï¼‰")
        
        # å­¦ä¹ ç‡æ£€æŸ¥
        learning_rate = train_config.get('learning_rate', 1e-5)
        if learning_rate > 1e-5:
            print(f"è­¦å‘Š: å­¦ä¹ ç‡ {learning_rate} å¯èƒ½è¿‡å¤§")
        print(f"ä½¿ç”¨å­¦ä¹ ç‡: {learning_rate}")
        
        # è®­ç»ƒå‚æ•°
        training_args = TrainingArguments(
            output_dir=self.output_dir,
            num_train_epochs=max_epochs,
            per_device_train_batch_size=train_config.get('batch_size', 2),
            per_device_eval_batch_size=train_config.get('eval_batch_size', 2),
            gradient_accumulation_steps=train_config.get('gradient_accumulation_steps', 8),
            learning_rate=learning_rate,
            weight_decay=train_config.get('weight_decay', 0.01),
            warmup_steps=train_config.get('warmup_steps', 100),
            logging_steps=train_config.get('logging_steps', 10),
            save_steps=save_steps_value,
            eval_steps=eval_steps_value,
            eval_strategy="steps" if val_dataset else "no",
            save_total_limit=train_config.get('save_total_limit', 3),
            load_best_model_at_end=True if val_dataset else False,
            metric_for_best_model="eval_loss",
            greater_is_better=False,
            fp16=False,
            bf16=True,
            dataloader_pin_memory=False,
            gradient_checkpointing=True,
            optim="adamw_torch",
            max_grad_norm=0.5,
            report_to="wandb" if os.environ.get('WANDB_PROJECT') else "none",
            ddp_find_unused_parameters=False,
        )
        
        # åˆ›å»ºæ—©åœå›è°ƒ
        early_stopping = EarlyStoppingCallback(
            early_stopping_patience=early_stopping_patience,
            early_stopping_threshold=early_stopping_threshold
        )
        
        # åˆ›å»º Trainerï¼ˆä½¿ç”¨ç»Ÿä¸€çš„ CustomTrainerWithAnswerWeightï¼‰
        trainer = CustomTrainerWithAnswerWeight(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=val_dataset,
            data_collator=collate_fn,  # ä½¿ç”¨åŠ¨æ€paddingçš„collate_fn
            processing_class=self.tokenizer,
            callbacks=[early_stopping] if val_dataset else [],
            tokenizer=self.tokenizer,
            is_main_process=True,  # å•å¡è®­ç»ƒï¼Œæ€»æ˜¯ä¸»è¿›ç¨‹
            rank=0,
            debug_steps=3,
        )
        
        # å¼€å§‹è®­ç»ƒ
        print("="*80)
        print("ğŸš€ å¼€å§‹è®­ç»ƒï¼ˆåŠ¨æ€Batch Paddingä¼˜åŒ–ç‰ˆï¼‰")
        print("="*80)
        print(f"è®­ç»ƒæ ·æœ¬æ•°: {len(train_dataset)}")
        if val_dataset:
            print(f"éªŒè¯æ ·æœ¬æ•°: {len(val_dataset)}")
        print(f"ä½¿ç”¨é…ç½®: profile={self.use_profile}, history={self.use_history}, context={self.use_context}")
        print(f"æœ€å¤§åºåˆ—é•¿åº¦: {train_config.get('max_length', 4096)} (åŠ¨æ€padding)")
        print(f"æœ€å¤§è½®æ¬¡: {max_epochs}")
        print(f"æ—©åœè€å¿ƒå€¼: {early_stopping_patience}")
        print("="*80)
        
        trainer.train()
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        print(f"ä¿å­˜æœ€ç»ˆæ¨¡å‹åˆ° {self.output_dir}")
        try:
            trainer.save_model()
            self.tokenizer.save_pretrained(self.output_dir)
            print("âœ“ æ¨¡å‹ä¿å­˜æˆåŠŸ")
        except Exception as e:
            print(f"è­¦å‘Š: ä¿å­˜æ¨¡å‹æ—¶å‡ºé”™: {e}")
        
        # è¾“å‡ºæˆªæ–­ç»Ÿè®¡
        if hasattr(train_dataset, 'get_truncation_stats'):
            stats = train_dataset.get_truncation_stats()
            print("\n" + "="*80)
            print("ğŸ“Š è®­ç»ƒæ•°æ®æˆªæ–­ç»Ÿè®¡:")
            print(f"  æ€»æ ·æœ¬æ•°: {stats['total_samples']}")
            print(f"  è¢«æˆªæ–­æ ·æœ¬æ•°: {stats['truncated_samples']}")
            print(f"  Context æˆªæ–­ç‡: {stats['truncation_rate']:.2%}")
            print(f"  å¹³å‡æˆªæ–­è½®æ¬¡: {stats['avg_truncated_turns']:.2f}")
            
            # å¦‚æœä½¿ç”¨äº†å†å²è®°å½•ï¼Œè¾“å‡ºå†å²è®°å½•ç»Ÿè®¡
            if stats['samples_with_history'] > 0:
                print("\n  ğŸ“š å†å²è®°å½•ç»Ÿè®¡:")
                print(f"    åŒ…å«å†å²è®°å½•çš„æ ·æœ¬æ•°: {stats['samples_with_history']}")
                print(f"    å†å²è®°å½•æ€»æ¡ç›®æ•°: {stats['total_history_items']}")
                print(f"    è¢«æˆªæ–­çš„å†å²æ¡ç›®æ•°: {stats['truncated_history_items']}")
                print(f"    å†å²è®°å½•æˆªæ–­ç‡: {stats['history_truncation_rate']:.2%}")
                print(f"    åŒ…å«è¢«æˆªæ–­å†å²çš„æ ·æœ¬æ•°: {stats['samples_with_history_truncated']}")
                if stats['samples_with_history'] > 0:
                    history_sample_rate = stats['samples_with_history_truncated'] / stats['samples_with_history']
                    print(f"    æ ·æœ¬çº§å†å²æˆªæ–­ç‡: {history_sample_rate:.2%}")
            print("="*80)
        
        print("è®­ç»ƒå®Œæˆï¼")


def main():
    parser = argparse.ArgumentParser(description='æ¶ˆèå®éªŒè®­ç»ƒï¼ˆåŠ¨æ€Paddingä¼˜åŒ–ç‰ˆï¼‰')
    parser.add_argument('--config', type=str,
                       default='/data/lingyu.li/parallel-post-train/ablation/config.json',
                       help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--ablation_config', type=str, required=True,
                       choices=['profile_and_history_and_context', 'profile_and_history', 'profile_and_context', 
                               'history_and_context', 'profile_only', 'history_only', 'context_only'],
                       help='æ¶ˆèå®éªŒé…ç½®')
    parser.add_argument('--val_ratio', type=float, default=0.1,
                       help='éªŒè¯é›†æ¯”ä¾‹')
    parser.add_argument('--gpu', type=int, default=1,
                       help='ä½¿ç”¨çš„GPUç¼–å·ï¼ˆé»˜è®¤ï¼š1ï¼‰')
    parser.add_argument('--max_epochs', type=int, default=50,
                       help='æœ€å¤§è®­ç»ƒè½®æ¬¡ï¼ˆé»˜è®¤ï¼š50ï¼‰')
    parser.add_argument('--early_stopping_patience', type=int, default=3,
                       help='æ—©åœè€å¿ƒå€¼ï¼ˆé»˜è®¤ï¼š3ï¼‰')
    parser.add_argument('--early_stopping_threshold', type=float, default=0.001,
                       help='æ—©åœé˜ˆå€¼ï¼ˆé»˜è®¤ï¼š0.001ï¼‰')
    parser.add_argument('--output_dir', type=str, default=None,
                       help='æ¨¡å‹è¾“å‡ºç›®å½•')
    parser.add_argument('--wandb_project', type=str, default=None,
                       help='Weights & Biasesé¡¹ç›®åç§°')
    parser.add_argument('--wandb_run_name', type=str, default=None,
                       help='Weights & Biasesè¿è¡Œåç§°')
    
    args = parser.parse_args()
    
    # é…ç½® Weights & Biases
    if args.wandb_project:
        try:
            import wandb
            os.environ['WANDB_PROJECT'] = args.wandb_project
            if args.wandb_run_name:
                os.environ['WANDB_NAME'] = args.wandb_run_name
            print(f"âœ“ å·²å¯ç”¨ Weights & Biases ç›‘æ§")
            print(f"  é¡¹ç›®: {args.wandb_project}")
            if args.wandb_run_name:
                print(f"  è¿è¡Œåç§°: {args.wandb_run_name}")
        except ImportError:
            print("è­¦å‘Š: wandb æœªå®‰è£…")
            args.wandb_project = None
    
    # è®¾ç½®GPU
    physical_gpu_id = args.gpu
    os.environ['CUDA_VISIBLE_DEVICES'] = str(physical_gpu_id)
    print(f"=" * 60)
    print(f"GPU è®¾ç½®: ç‰©ç†GPU {physical_gpu_id}")
    print(f"=" * 60)
    
    # éªŒè¯GPU
    if torch.cuda.is_available():
        print(f"CUDA å¯ç”¨ï¼ŒGPU æ•°é‡: {torch.cuda.device_count()}")
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"GPU åç§°: {gpu_name}")
        print(f"GPU æ€»å†…å­˜: {gpu_memory:.2f} GB")
    else:
        print("è­¦å‘Š: CUDA ä¸å¯ç”¨")
    
    # åŠ è½½é…ç½®
    with open(args.config, 'r', encoding='utf-8') as f:
        config = json.load(f)
    
    # è·å–æ¶ˆèé…ç½®
    ablation_config = config['ablation_configs'][args.ablation_config]
    use_profile = ablation_config.get('use_profile', True)
    use_history = ablation_config.get('use_history', True)
    use_context = ablation_config.get('use_context', True)
    config_name = ablation_config['name']
    
    print("=" * 60)
    print(f"æ¶ˆèå®éªŒï¼ˆåŠ¨æ€Paddingä¼˜åŒ–ç‰ˆï¼‰: {config_name}")
    print(f"ä½¿ç”¨é…ç½®: profile={use_profile}, history={use_history}, context={use_context}")
    print("=" * 60)
    
    # åŠ è½½è®­ç»ƒæ•°æ®
    print("åŠ è½½è®­ç»ƒæ•°æ®...")
    train_path = config['data']['train_path']
    train_data = load_train_data(train_path)
    
    if not train_data:
        print(f"é”™è¯¯: æ— æ³•åŠ è½½è®­ç»ƒæ•°æ®")
        return
    
    # æå–è®­ç»ƒæ ·æœ¬
    all_samples = extract_training_samples(train_data, debug=True)
    print(f"æå–äº† {len(all_samples)} ä¸ªè®­ç»ƒæ ·æœ¬")
    
    # æ·»åŠ å†å²ä¿¡æ¯
    if use_history:
        print("æ·»åŠ å†å²ä¿¡æ¯...")
        all_samples = add_history_to_samples(all_samples, all_samples)
    
    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    train_samples, val_samples = split_train_val(all_samples, args.val_ratio)
    print(f"è®­ç»ƒé›†: {len(train_samples)} ä¸ªæ ·æœ¬")
    print(f"éªŒè¯é›†: {len(val_samples)} ä¸ªæ ·æœ¬")
    
    # è·å–æ¨¡å‹é…ç½®
    model_config = config['model']
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    if args.output_dir:
        output_dir = args.output_dir
        os.makedirs(output_dir, exist_ok=True)
        print(f"ä½¿ç”¨æŒ‡å®šçš„è¾“å‡ºç›®å½•: {output_dir}")
    else:
        checkpoint_dir = model_config['checkpoint_dir']
        dataset_name = os.path.basename(os.path.dirname(train_path))
        output_dir = os.path.join(checkpoint_dir, f"{dataset_name}_ablation_{config_name}_dynamic_padding")
        
        try:
            os.makedirs(output_dir, exist_ok=True)
            print(f"è¾“å‡ºç›®å½•: {output_dir}")
        except (OSError, IOError) as e:
            print(f"è­¦å‘Š: æ— æ³•åˆ›å»ºç›®å½•: {e}")
            local_checkpoint_dir = os.path.join(os.path.expanduser("~"), "checkpoints")
            output_dir = os.path.join(local_checkpoint_dir, f"{dataset_name}_ablation_{config_name}_dynamic_padding")
            os.makedirs(output_dir, exist_ok=True)
            print(f"ä½¿ç”¨æœ¬åœ°ç›®å½•: {output_dir}")
    
    # åˆ›å»ºè®­ç»ƒå™¨
    model_path = model_config['path']
    trainer = AblationTrainerWithDynamicPadding(
        model_path=model_path,
        output_dir=output_dir,
        config=config,
        use_profile=use_profile,
        use_history=use_history,
        use_context=use_context
    )
    
    # å¼€å§‹è®­ç»ƒ
    trainer.train(
        train_samples, 
        val_samples,
        max_epochs=args.max_epochs,
        early_stopping_patience=args.early_stopping_patience,
        early_stopping_threshold=args.early_stopping_threshold
    )
    
    print(f"\nâœ… è®­ç»ƒå®Œæˆï¼æ¨¡å‹ä¿å­˜åœ¨: {output_dir}")


if __name__ == '__main__':
    main()
