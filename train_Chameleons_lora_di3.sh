#!/bin/bash

# Chameleons LoRA шонч╗ГшДЪцЬм
# ф╜┐чФи LoRA х╛ош░Г 30B цибхЮЛя╝Мшонч╗ГщАЯх║жцПРхНЗ 3-5 хАН

echo "================================================================================"
echo "Chameleons LoRA шонч╗Г (30B цибхЮЛ)"
echo "================================================================================"
echo "ф╝ШхК┐:"
echo "  тЪб шонч╗ГщАЯх║ж: 3-5x хКащАЯ (28чзТ/batch тЖТ 5-8чзТ/batch)"
echo "  ЁЯТ╛ цШ╛хнШхНачФи: хЗПх░С 50-70%"
echo "  тП▒я╕П  шонч╗ГцЧ╢щЧ┤: 12.5хдй тЖТ 2-3хдй"
echo "  ЁЯУК хПпшонч╗ГхПВцХ░: 30B тЖТ 50-200M (<1%)"
echo ""
echo "цХ░цНощЫЖ: sampled_data/Chameleons/train_di3.json (~16,963 ца╖цЬм)"
echo "DeepSpeed: ZeRO-2 (LoRA цШ╛хнШхНачФих░Пя╝МцЧащЬА ZeRO-3)"
echo "================================================================================"
echo ""

# щЕНч╜о
CONFIG="config_Chameleons_30B_lora_di3.json"
DEEPSPEED="ds_config_zero2.json"
ABLATION="context_only"
OUTPUT_DIR="outputs/Chameleons_context_30B_lora_di3"
PROJECT="Qwen3_30B-Chameleons-LoRA"
RUN_NAME="context_lora_r64_di3"
MASTER_PORT=29503

# цгАцЯещЕНч╜оцЦЗф╗╢
if [ ! -f "$CONFIG" ]; then
    echo "тЭМ щФЩшпп: щЕНч╜оцЦЗф╗╢ф╕НхнШхЬи: $CONFIG"
    echo "шп╖хЕИш┐РшбМ: python create_lora_config.py"
    exit 1
fi

if [ ! -f "$DEEPSPEED" ]; then
    echo "тЭМ щФЩшпп: DeepSpeed щЕНч╜оцЦЗф╗╢ф╕НхнШхЬи: $DEEPSPEED"
    exit 1
fi

if [ ! -f "sampled_data/Chameleons/train_di3.json" ]; then
    echo "тЭМ щФЩшпп: щЗЗца╖цХ░цНощЫЖф╕НхнШхЬи"
    echo "шп╖хЕИш┐РшбМ:"
    echo "  python sample_dataset_data_item_level.py \\"
    echo "      /mnt/parallel/GIDigitalTwinBench/RealSelf/Chameleons/train.json \\"
    echo "      sampled_data/Chameleons/train_di3.json \\"
    echo "      --max_data_items 3 --seed 42"
    exit 1
fi

echo "тЬЕ щЕНч╜оцгАцЯещАЪш┐З"
echo ""
echo "х╝АхзЛшонч╗Г..."
echo ""

# шонч╗ГхС╜ф╗д
torchrun \
    --nproc_per_node=8 \
    --master_port=${MASTER_PORT} \
    train_distributed_Chameleons.py \
    --config ${CONFIG} \
    --deepspeed ${DEEPSPEED} \
    --ablation_config ${ABLATION} \
    --output_dir ${OUTPUT_DIR} \
    --max_epochs 50 \
    --val_ratio 0.1 \
    --wandb_project ${PROJECT} \
    --wandb_run_name ${RUN_NAME} \
    --prompt_style simple

echo ""
echo "================================================================================"
echo "шонч╗ГхоМцИРя╝Б"
echo "================================================================================"
echo "ш╛УхЗ║чЫох╜Х: ${OUTPUT_DIR}"
echo ""
echo "LoRA щАВщЕНхЩих╖▓ф┐ЭхнШя╝МхПпчФиф║О:"
echo "  1. ч╗зч╗ншонч╗Г"
echo "  2. хРИх╣╢хИ░хЯ║чбАцибхЮЛ: python merge_lora_weights.py"
echo "  3. цОичРЖ: хКаш╜╜ base model + LoRA adapter"
echo "================================================================================"
