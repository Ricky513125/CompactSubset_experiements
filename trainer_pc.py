"""
è®­ç»ƒå™¨æ¨¡å— - ä¼˜åŒ–ç‰ˆ
é€‚é…æ¶ˆèå®éªŒï¼Œæ”¯æŒä¸¥æ ¼çš„è§’è‰²æ§åˆ¶ä¸æ—¥å¿—ç›‘æ§
æ·»åŠ  Emoji è¿‡æ»¤åŠŸèƒ½
"""
import os
import re
import time
import json
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
)
from typing import List, Dict, Any, Optional, Tuple
import sys
from pathlib import Path

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„ï¼Œç¡®ä¿èƒ½å¯¼å…¥æ¨¡å—
sys.path.insert(0, str(Path(__file__).parent))

# âœ… ä½¿ç”¨æ–°çš„ç®€çŸ­ prompt æ„å»ºå‡½æ•°ï¼ˆç°åœ¨åœ¨åŒä¸€ç›®å½•ä¸‹ï¼‰
try:
    from data_loader_more_data import build_simple_training_prompt
    print("ä½¿ç”¨ç®€çŸ­ prompt æ„å»ºå‡½æ•° (data_loader_more_data)")
except ImportError as e:
    print(f"âš ï¸ æ— æ³•å¯¼å…¥ data_loader_more_data: {e}")
    print("âš ï¸ å›é€€åˆ°è¯¦ç»† prompt æ„å»ºå‡½æ•°")
    from prompt_builder_LovinkDialogue import build_training_prompt as build_simple_training_prompt

# å¯¼å…¥ emoji è¿‡æ»¤æ¨¡å—
try:
    from emoji_filter import contains_emoji
except ImportError:
    print("è­¦å‘Š: æ— æ³•å¯¼å…¥ emoji_filterï¼Œå°†è·³è¿‡ emoji è¿‡æ»¤")
    def contains_emoji(text):
        return False


class AblationDataset(Dataset):
    def __init__(self, samples, tokenizer, max_length=32768, use_profile=True, use_history=True, use_context=True, filter_emoji=True):
        """
        Args:
            samples: è®­ç»ƒæ ·æœ¬åˆ—è¡¨
            tokenizer: tokenizerå®ä¾‹
            max_length: æœ€å¤§åºåˆ—é•¿åº¦
            use_profile: æ˜¯å¦ä½¿ç”¨ profile
            use_history: æ˜¯å¦ä½¿ç”¨ history
            use_context: æ˜¯å¦ä½¿ç”¨ context
            filter_emoji: æ˜¯å¦åœ¨æ•°æ®é›†å±‚é¢å†æ¬¡è¿‡æ»¤ emojiï¼ˆåŒé‡ä¿é™©ï¼Œé»˜è®¤Trueï¼‰
        """
        # Emoji è¿‡æ»¤ï¼ˆåŒé‡ä¿é™©ï¼‰
        if filter_emoji:
            original_count = len(samples)
            filtered_samples = []
            emoji_count = 0
            
            for sample in samples:
                target_text = sample.get('next_question', '')
                if contains_emoji(target_text):
                    emoji_count += 1
                    continue
                filtered_samples.append(sample)
            
            self.samples = filtered_samples
            
            if emoji_count > 0:
                print(f"\n{'='*80}")
                print(f"ğŸš« Dataset å±‚ Emoji äºŒæ¬¡è¿‡æ»¤:")
                print(f"  åŸå§‹æ ·æœ¬æ•°: {original_count}")
                print(f"  é¢å¤–è¿‡æ»¤ emoji æ ·æœ¬æ•°: {emoji_count}")
                print(f"  æœ€ç»ˆæ ·æœ¬æ•°: {len(self.samples)}")
                print(f"  é¢å¤–è¿‡æ»¤æ¯”ä¾‹: {emoji_count / original_count * 100:.2f}%")
                print(f"{'='*80}\n")
        else:
            self.samples = samples
        
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.use_profile = use_profile
        self.use_history = use_history
        self.use_context = use_context
        
        # æˆªæ–­ç»Ÿè®¡
        self.truncation_stats = {
            'total_samples': 0,
            'truncated_samples': 0,
            'truncated_turns': 0
        }
        
        # Emoji ç»Ÿè®¡
        self.emoji_stats = {
            'checked_samples': 0,
            'emoji_found': 0
        }

    def __len__(self):
        return len(self.samples)
    
    def get_truncation_stats(self):
        """è·å–æˆªæ–­ç»Ÿè®¡ä¿¡æ¯"""
        if self.truncation_stats['total_samples'] == 0:
            return {
                'truncation_rate': 0.0,
                'avg_truncated_turns': 0.0,
                'total_samples': 0,
                'truncated_samples': 0
            }
        
        truncation_rate = self.truncation_stats['truncated_samples'] / self.truncation_stats['total_samples']
        avg_truncated_turns = (self.truncation_stats['truncated_turns'] / self.truncation_stats['truncated_samples'] 
                               if self.truncation_stats['truncated_samples'] > 0 else 0)
        
        return {
            'truncation_rate': truncation_rate,
            'avg_truncated_turns': avg_truncated_turns,
            'total_samples': self.truncation_stats['total_samples'],
            'truncated_samples': self.truncation_stats['truncated_samples']
        }

    def __getitem__(self, idx):
        sample = self.samples[idx]
        
        # è¿è¡Œæ—¶ emoji æ£€æµ‹ï¼ˆæœ€åä¸€é“é˜²çº¿ï¼Œç†è®ºä¸Šä¸åº”è¯¥è§¦å‘ï¼‰
        target_text = sample.get('next_question', '')
        self.emoji_stats['checked_samples'] += 1
        
        if contains_emoji(target_text):
            self.emoji_stats['emoji_found'] += 1
            # å¦‚æœåœ¨è¿è¡Œæ—¶å‘ç° emojiï¼Œè®°å½•è­¦å‘Šä½†ç»§ç»­è®­ç»ƒ
            # ï¼ˆå› ä¸ºå·²ç»åœ¨ __init__ æ—¶è¿‡æ»¤è¿‡äº†ï¼Œè¿™é‡Œåº”è¯¥ä¸ä¼šè§¦å‘ï¼‰
            if self.emoji_stats['emoji_found'] <= 3:  # åªæ‰“å°å‰3æ¬¡
                print(f"âš ï¸  è­¦å‘Š: åœ¨è¿è¡Œæ—¶æ£€æµ‹åˆ° emojiï¼ˆæ ·æœ¬ #{idx}ï¼‰: {target_text[:50]}...")
        
        # 1. åˆå§‹æ„å»º - âœ… ä½¿ç”¨ç®€çŸ­ prompt æ„å»ºå‡½æ•°
        messages, target_answer = build_simple_training_prompt(
            context=sample['context'],
            next_question=sample['next_question'],
            user_profile=sample.get('user_profile') if self.use_profile else None,
            task_description=sample.get('task_description'),
            history=sample.get('history') if self.use_history else None,
            use_profile=self.use_profile,
            use_history=self.use_history,
            use_context=self.use_context
        )

        # è®°å½•åŸå§‹æ¶ˆæ¯é•¿åº¦
        original_message_count = len(messages)
        is_truncated = False
        truncated_turns = 0
        
        # --- æ ¸å¿ƒä¼˜åŒ–ï¼šåŠ¨æ€è£å‰ªå†å²ä»¥é˜²æ­¢æˆªæ–­ ---
        # å¦‚æœæ¶ˆæ¯å¤ªé•¿ï¼Œå¾ªç¯åˆ é™¤ messages ä¸­æœ€æ—©çš„å¯¹è¯è½®æ¬¡ï¼ˆä¿ç•™ system æç¤ºè¯ï¼‰
        # ç´¢å¼• 0 æ˜¯ systemï¼Œ1 å’Œ 2 æ˜¯æœ€æ—©çš„ä¸€å¯¹ user/assistant
        while len(self.tokenizer.apply_chat_template(messages, tokenize=True)) > (self.max_length - 512):
            if len(messages) > 2:
                messages.pop(1) # å¼¹å‡ºæœ€æ—©çš„å¯¹è¯
                is_truncated = True
                truncated_turns += 1
            else:
                break
        
        # æ›´æ–°æˆªæ–­ç»Ÿè®¡
        self.truncation_stats['total_samples'] += 1
        if is_truncated:
            self.truncation_stats['truncated_samples'] += 1
            self.truncation_stats['truncated_turns'] += truncated_turns

        # 2. ç”Ÿæˆ Prompt (æ‰‹åŠ¨æ·»åŠ å¼•å¯¼ç¬¦)
        full_prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)
        # ä¿®æ­£ï¼šåº”è¯¥ç”Ÿæˆassistantè§’è‰²çš„å›å¤ï¼ˆç›®æ ‡ç”¨æˆ·ï¼‰
        generation_suffix = "<|im_start|>assistant\n"

        # 3. ç»„åˆæˆçœŸæ­£çš„ Prompt
        full_prompt = full_prompt.strip() + generation_suffix
        # ç¡®ä¿ä¸åŒ…å«ç­”æ¡ˆï¼Œä½¿ç”¨ <|im_end|> ä½œä¸ºç»“æŸæ ‡è®°ï¼ˆè®©æ¨¡å‹å­¦ä¼šåœ¨æ­£ç¡®ä½ç½®åœæ­¢ï¼‰
        im_end_token = "<|im_end|>"
        full_text = full_prompt + target_answer + im_end_token

        # 3. ç¼–ç 
        encoded = self.tokenizer(
            full_text,
            truncation=True,
            max_length=self.max_length,
            padding='max_length',
            return_tensors='pt'
        )
        
        input_ids = encoded['input_ids'].squeeze()
        attention_mask = encoded['attention_mask'].squeeze()

        # --- æ ¸å¿ƒä¼˜åŒ–ï¼šé«˜ç²¾åº¦è®¡ç®— Prompt é•¿åº¦ ---
        # æˆ‘ä»¬ä¸ç›´æ¥ encode(full_prompt)ï¼Œè€Œæ˜¯é€šè¿‡å¯»æ‰¾ target çš„èµ·å§‹ token æ¥ç¡®å®š
        target_ids = self.tokenizer.encode(target_answer, add_special_tokens=False)
        
        # å¯»æ‰¾åˆ†ç•Œç‚¹ï¼šåœ¨ input_ids ä¸­æ‰¾åˆ°ç¬¬ä¸€ä¸ªä¸å±äº prompt çš„ä½ç½®
        # æˆ‘ä»¬å¯ä»¥å…ˆ encode ä¸€ä¸ªå®Œå…¨æ²¡å¸¦ç‰¹æ®Šå­—ç¬¦çš„ prompt
        prompt_ids = self.tokenizer.encode(full_prompt, add_special_tokens=False)
        actual_prompt_len = len(prompt_ids)

        labels = input_ids.clone()
        
        # å±è”½ Promptï¼šç¡®ä¿ä¸ä¼šè¶Šç•Œ
        safe_prompt_len = min(actual_prompt_len, self.max_length - 1)
        labels[:safe_prompt_len] = -100
        
        # å±è”½ Padding
        labels[input_ids == self.tokenizer.pad_token_id] = -100

        # --- å±è”½ç‰¹æ®Š Token (ä¿ç•™ EOS å’Œ <|im_end|>) ---
        # è·å– <|im_end|> çš„ token IDï¼Œç¡®ä¿å®ƒè¢«åŒ…å«åœ¨æŸå¤±è®¡ç®—ä¸­
        im_end_token = "<|im_end|>"
        im_end_id = None
        try:
            # å°è¯•è·å– <|im_end|> çš„ token ID
            im_end_ids = self.tokenizer.encode(im_end_token, add_special_tokens=False)
            if im_end_ids:
                im_end_id = im_end_ids[0]  # é€šå¸¸ <|im_end|> æ˜¯ä¸€ä¸ªå•ç‹¬çš„ token
                # è°ƒè¯•ä¿¡æ¯ï¼ˆåªåœ¨ç¬¬ä¸€æ¬¡æ‰“å°ï¼‰
                if not hasattr(self, '_im_end_logged'):
                    print(f"âœ“ <|im_end|> token ID: {im_end_id}ï¼Œå°†è¢«åŒ…å«åœ¨æŸå¤±è®¡ç®—ä¸­")
                    self._im_end_logged = True
        except Exception as e:
            if not hasattr(self, '_im_end_error_logged'):
                print(f"è­¦å‘Š: æ— æ³•è·å– <|im_end|> token ID: {e}")
                self._im_end_error_logged = True
        
        special_ids = set(self.tokenizer.all_special_ids)
        eos_id = self.tokenizer.eos_token_id
        # ä¿ç•™ EOS å’Œ <|im_end|> tokenï¼Œè®©æ¨¡å‹å­¦ä¼šåœ¨æ­£ç¡®ä½ç½®åœæ­¢
        tokens_to_keep = {eos_id}
        if im_end_id is not None:
            tokens_to_keep.add(im_end_id)
        
        for tid in special_ids:
            if tid not in tokens_to_keep:
                labels[labels == tid] = -100
        
        # éªŒè¯ <|im_end|> æ˜¯å¦åœ¨ labels ä¸­ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        if im_end_id is not None and (labels == im_end_id).any():
            if not hasattr(self, '_im_end_verified'):
                print(f"âœ“ ç¡®è®¤: <|im_end|> token (ID: {im_end_id}) å·²åŒ…å«åœ¨æŸå¤±è®¡ç®—ä¸­")
                self._im_end_verified = True

        # 4. æœ€ç»ˆéªŒè¯ï¼šé˜²æ­¢ NaN
        if (labels != -100).sum() == 0:
            # æŒ½æ•‘é€»è¾‘ï¼šå¦‚æœå…¨è¢«å±è”½äº†ï¼ˆè¯´æ˜æˆªæ–­å¤ªä¸¥é‡ï¼‰ï¼Œå¼ºè¡Œæš´éœ²æœ€å 32 ä¸ª token 
            # è¿™ç§æƒ…å†µé€šå¸¸å‘ç”Ÿåœ¨ç­”æ¡ˆæé•¿æˆ–æˆªæ–­åˆšå¥½åˆ‡åœ¨äº†ç­”æ¡ˆå¼€å¤´
            labels[-32:] = input_ids[-32:]
            labels[labels == self.tokenizer.pad_token_id] = -100

        return {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'labels': labels
        }


class CustomTrainer(Trainer):
    """å¸¦å®æ—¶æ—¥å¿—çš„è‡ªå®šä¹‰è®­ç»ƒå™¨"""
    
    def __init__(self, *args, verbose_logging=False, log_file_path=None, **kwargs):
        super().__init__(*args, **kwargs)
        self.verbose_logging = verbose_logging
        self.log_file_path = log_file_path
        self.log_entry_count = 0
        
        if self.log_file_path:
            os.makedirs(os.path.dirname(self.log_file_path), exist_ok=True)
            self.log_file = open(self.log_file_path, 'w', encoding='utf-8')
            self.log_file.write("[\n")

    def __del__(self):
        if hasattr(self, 'log_file') and self.log_file:
            try:
                self.log_file.write("\n]")
                self.log_file.close()
            except: pass

    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
        outputs = model(**inputs)
        loss = outputs.loss if hasattr(outputs, 'loss') else None
        
        if loss is None and "labels" in inputs:
            logits = outputs.get("logits")
            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)
            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = inputs["labels"][..., 1:].contiguous()
            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))

        if self.verbose_logging and (self.state.global_step % self.args.logging_steps == 0):
            self._log_details(inputs, outputs, loss.item())

        return (loss, outputs) if return_outputs else loss

    def clean_output_text(self, text: str) -> str:
        # ç§»é™¤æ€è€ƒè¿‡ç¨‹
        text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)
        text = text.replace('<think>', '').replace('</think>', '')
        return text.strip()

    def _log_details(self, inputs, outputs, loss_val):
        """è®°å½•è®­ç»ƒç»†èŠ‚ï¼šå¯¹æ¯” Target å’Œæ¨¡å‹çš„é¢„æµ‹ (Argmax)"""
        try:
            batch_idx = 0
            ids = inputs['input_ids'][batch_idx]
            lbs = inputs['labels'][batch_idx]
            logits = outputs.get("logits")[batch_idx]
            
            # ç»Ÿè®¡ä¿¡æ¯
            total_tokens = len(ids)
            valid_label_count = (lbs != -100).sum().item()
            
            # è§£ç  Target
            target_ids = [t.item() for t in lbs if t != -100]
            target_text = self.tokenizer.decode(target_ids, skip_special_tokens=True)
            
            # è§£ç é¢„æµ‹ (å¯»æ‰¾ label æœ‰æ•ˆä½å¯¹åº”çš„é¢„æµ‹ä½)
            pred_ids_all = logits.argmax(dim=-1)
            valid_pos = (lbs != -100).nonzero(as_tuple=True)[0]
            pred_ids = [pred_ids_all[p-1].item() for p in valid_pos if p > 0]
            predict_text = self.tokenizer.decode(pred_ids, skip_special_tokens=True)
            
            # è®¡ç®—å‡†ç¡®åŒ¹é…çš„ token æ•°é‡
            correct_tokens = 0
            for i, pos in enumerate(valid_pos):
                if pos > 0 and i < len(pred_ids):
                    if lbs[pos].item() == pred_ids_all[pos-1].item():
                        correct_tokens += 1
            
            token_accuracy = correct_tokens / valid_label_count if valid_label_count > 0 else 0
            
            # æ‰“å°è¯¦ç»†ä¿¡æ¯
            print(f"\n{'='*100}")
            print(f"[Step {self.state.global_step}] è®­ç»ƒæ—¥å¿—")
            print(f"{'='*100}")
            print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
            print(f"  Loss: {loss_val:.4f}")
            print(f"  æ€» Token æ•°: {total_tokens}")
            print(f"  æœ‰æ•ˆæ ‡ç­¾æ•°: {valid_label_count} (è®­ç»ƒæ¯”ä¾‹: {valid_label_count/total_tokens:.2%})")
            print(f"  Token å‡†ç¡®ç‡: {token_accuracy:.2%} ({correct_tokens}/{valid_label_count})")
            print(f"\nğŸ¯ é¢„æµ‹ç›®æ ‡ (Target):")
            print(f"  é•¿åº¦: {len(target_text)} å­—ç¬¦")
            if len(target_text) <= 200:
                print(f"  å®Œæ•´å†…å®¹: {target_text}")
            else:
                print(f"  å‰100å­—: {target_text[:100]}")
                print(f"  å100å­—: {target_text[-100:]}")
            
            print(f"\nğŸ¤– æ¨¡å‹é¢„æµ‹ (Prediction):")
            print(f"  é•¿åº¦: {len(predict_text)} å­—ç¬¦")
            if len(predict_text) <= 200:
                print(f"  å®Œæ•´å†…å®¹: {predict_text}")
            else:
                print(f"  å‰100å­—: {predict_text[:100]}")
                print(f"  å100å­—: {predict_text[-100:]}")
            
            # ç®€å•çš„æ–‡æœ¬ç›¸ä¼¼åº¦æç¤º
            if target_text == predict_text:
                print(f"\nâœ… å®Œå…¨åŒ¹é…ï¼")
            elif target_text[:50] == predict_text[:50]:
                print(f"\nâš ï¸  å‰50å­—åŒ¹é…ï¼Œåç»­æœ‰å·®å¼‚")
            else:
                print(f"\nâŒ é¢„æµ‹ä¸ç›®æ ‡å·®å¼‚è¾ƒå¤§")
            
            print(f"{'='*100}\n")

            # ä¿å­˜åˆ°æ—¥å¿—æ–‡ä»¶
            if hasattr(self, 'log_file'):
                log_data = {
                    "step": self.state.global_step,
                    "loss": loss_val,
                    "stats": {
                        "total_tokens": total_tokens,
                        "valid_labels": valid_label_count,
                        "training_ratio": f"{valid_label_count/total_tokens:.2%}",
                        "token_accuracy": f"{token_accuracy:.2%}",
                        "correct_tokens": correct_tokens
                    },
                    "target": {
                        "text": target_text,
                        "length": len(target_text)
                    },
                    "prediction": {
                        "text": predict_text,
                        "length": len(predict_text)
                    },
                    "match_status": "full" if target_text == predict_text else "partial" if target_text[:50] == predict_text[:50] else "different"
                }
                if self.log_entry_count > 0: self.log_file.write(",\n")
                self.log_file.write(json.dumps(log_data, ensure_ascii=False, indent=2))
                self.log_file.flush()
                self.log_entry_count += 1
        except Exception as e:
            print(f"âŒ Log Error: {e}")
            import traceback
            traceback.print_exc()


class AblationTrainer:
    """æ¶ˆèå®éªŒä¸»æ§ç±»"""
    
    def __init__(self, model_path: str, output_dir: str, config: Dict[str, Any], 
                 use_profile: bool = True, use_history: bool = True, use_context: bool = True, log_file_path: Optional[str] = None):
        self.model_path = model_path
        self.output_dir = output_dir
        self.config = config
        self.use_profile = use_profile
        self.use_history = use_history
        self.use_context = use_context
        self.log_file_path = log_file_path

        # 1. åŠ è½½ Tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # 2. åŠ è½½æ¨¡å‹
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model = AutoModelForCausalLM.from_pretrained(
            model_path, 
            torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,
            trust_remote_code=True
        ).to(self.device)
        
        if hasattr(self.model, 'gradient_checkpointing_enable'):
            self.model.gradient_checkpointing_enable()

    def train(self, train_samples: List[Dict[str, Any]], val_samples: Optional[List[Dict[str, Any]]] = None):
        train_config = self.config.get('training', {})
        
        train_dataset = AblationDataset(
            train_samples, self.tokenizer, 
            max_length=train_config.get('max_length', 32768),
            use_profile=self.use_profile, use_history=self.use_history, use_context=self.use_context
        )

        training_args = TrainingArguments(
            output_dir=self.output_dir,
            num_train_epochs=train_config.get('num_epochs', 3),
            per_device_train_batch_size=train_config.get('batch_size', 1),
            gradient_accumulation_steps=train_config.get('gradient_accumulation_steps', 16),
            learning_rate=train_config.get('learning_rate', 2e-5),
            logging_steps=10,
            save_steps=100,
            bf16=torch.cuda.is_bf16_supported(),
            fp16=not torch.cuda.is_bf16_supported(),
            report_to="none",
            remove_unused_columns=False
        )

        trainer = CustomTrainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
            tokenizer=self.tokenizer,
            verbose_logging=True,
            log_file_path=self.log_file_path
        )

        print(f"ğŸš€ å¼€å§‹è®­ç»ƒ: Profile={self.use_profile}, History={self.use_history}, Context={self.use_context}")
        trainer.train()
        
        # ä¿å­˜
        trainer.save_model(self.output_dir)
        self.tokenizer.save_pretrained(self.output_dir)
        
        # è¾“å‡ºæˆªæ–­ç»Ÿè®¡
        if hasattr(train_dataset, 'get_truncation_stats'):
            stats = train_dataset.get_truncation_stats()
            print("\n" + "="*80)
            print("ğŸ“Š è®­ç»ƒæ•°æ®æˆªæ–­ç»Ÿè®¡:")
            print(f"  æ€»æ ·æœ¬æ•°: {stats['total_samples']}")
            print(f"  è¢«æˆªæ–­æ ·æœ¬æ•°: {stats['truncated_samples']}")
            print(f"  æˆªæ–­ç‡: {stats['truncation_rate']:.2%}")
            print(f"  å¹³å‡æˆªæ–­è½®æ¬¡: {stats['avg_truncated_turns']:.2f}")
            print("="*80)
        
        # è¾“å‡º emoji ç»Ÿè®¡
        if hasattr(train_dataset, 'emoji_stats'):
            emoji_stats = train_dataset.emoji_stats
            print("\n" + "="*80)
            print("ğŸš« è®­ç»ƒè¿‡ç¨‹ Emoji æ£€æµ‹ç»Ÿè®¡:")
            print(f"  æ£€æŸ¥çš„æ ·æœ¬æ•°: {emoji_stats['checked_samples']}")
            print(f"  è¿è¡Œæ—¶å‘ç° emoji æ•°: {emoji_stats['emoji_found']}")
            if emoji_stats['emoji_found'] > 0:
                print(f"  âš ï¸  è­¦å‘Š: æœ‰ {emoji_stats['emoji_found']} ä¸ªæ ·æœ¬åœ¨è¿è¡Œæ—¶æ£€æµ‹åˆ° emoji")
                print(f"     è¿™è¡¨æ˜æ•°æ®è¿‡æ»¤å¯èƒ½å­˜åœ¨é—æ¼ï¼Œè¯·æ£€æŸ¥")
            else:
                print(f"  âœ“ å®Œç¾: è¿è¡Œæ—¶æœªæ£€æµ‹åˆ°ä»»ä½• emoji")
            print("="*80)