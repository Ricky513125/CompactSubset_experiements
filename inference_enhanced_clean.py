"""
å¢å¼ºç‰ˆæ¨ç†æ¸…ç†æ¨¡å—
ä¸“é—¨è§£å†³ï¼š
1. æ—¥æ–‡ä¸­å¤¹æ‚è‹±æ–‡
2. é‡å¤ç”Ÿæˆ
3. Assistant/GSç­‰å…ƒæ•°æ®
4. è¯­è¨€æ··ä¹±
5. Emoji è¡¨æƒ…ç¬¦å·æ¸…é™¤ âœ…
"""
import re
from typing import List

# å¯¼å…¥ emoji æ£€æµ‹å’Œæ¸…é™¤åŠŸèƒ½
try:
    from emoji_filter import contains_emoji
except ImportError:
    # å¦‚æœæ— æ³•å¯¼å…¥ï¼Œæä¾›ä¸€ä¸ªç®€å•çš„å¤‡ç”¨å®ç°
    def contains_emoji(text):
        emoji_pattern = re.compile(
            "[\U0001F600-\U0001F64F\U0001F300-\U0001F5FF\U0001F680-\U0001F6FF"
            "\U0001F1E0-\U0001F1FF\U0001F900-\U0001F9FF\U0001FA00-\U0001FAFF"
            "\U00002600-\U000026FF\U00002700-\U000027BF\U0000FE00-\U0000FE0F"
            "\U0001F004-\U0001F0CF\U0001F170-\U0001F251]+"
        )
        return bool(emoji_pattern.search(text))


def remove_excessive_interjections(text: str, max_repeats: int = 2) -> str:
    """
    æ¸…ç†è¿‡åº¦é‡å¤çš„è¯­æ°”è¯å’Œæ‹Ÿå£°è¯
    
    Args:
        text: è¾“å…¥æ–‡æœ¬
        max_repeats: å…è®¸çš„æœ€å¤§è¿ç»­é‡å¤æ¬¡æ•°ï¼ˆé»˜è®¤2æ¬¡ï¼‰
        
    Returns:
        æ¸…ç†åçš„æ–‡æœ¬
    """
    if not text:
        return text
    
    # ç¬¬ä¸€æ­¥ï¼šå¤„ç†ä»»ä½•å•å­—ç¬¦çš„è¿‡åº¦é‡å¤ï¼ˆé€šç”¨è§„åˆ™ï¼‰
    # å°† 3ä¸ªæˆ–æ›´å¤šçš„è¿ç»­ç›¸åŒå­—ç¬¦ -> æœ€å¤š2ä¸ª
    cleaned = re.sub(r'(.)\1{2,}', r'\1\1', text)  # âœ… å“ˆå“ˆå“ˆå“ˆå“ˆ -> å“ˆå“ˆ
    
    # ç¬¬äºŒæ­¥ï¼šå¤„ç†ç‰¹å®šè¯­æ°”è¯ï¼ˆæ›´ä¸¥æ ¼ï¼‰
    interjection_patterns = [
        (r'(å“ˆå“ˆ)+å“ˆ', 'å“ˆå“ˆ'),     # å“ˆå“ˆå“ˆ -> å“ˆå“ˆï¼ˆæ›´ä¸¥æ ¼ï¼‰
        (r'(å˜»){2,}', ''),          # å˜»å˜» -> åˆ é™¤
        (r'(å˜¿){2,}', ''),          # å˜¿å˜¿ -> åˆ é™¤
        (r'(å‘œ){2,}', ''),          # å‘œå‘œ -> åˆ é™¤
        (r'(å“‡){2,}', ''),          # å“‡å“‡ -> åˆ é™¤
        (r'(å•Š){2,}', ''),          # å•Šå•Š -> åˆ é™¤
        (r'(å™œ){1,}', ''),          # å™œ -> åˆ é™¤
        (r'(å—·){1,}', ''),          # å—· -> åˆ é™¤
        (r'(å˜Ÿ){1,}', ''),          # å˜Ÿ -> åˆ é™¤
        (r'(æ‹‰){2,}', ''),          # æ‹‰æ‹‰ -> åˆ é™¤
        (r'(å•¦){2,}', ''),          # å•¦å•¦ -> åˆ é™¤
        (r'(å“¦){2,}', ''),          # å“¦å“¦ -> åˆ é™¤
        (r'(å‘µ){2,}', ''),          # å‘µå‘µ -> åˆ é™¤
        (r'(å—¯){2,}', ''),          # å—¯å—¯ -> åˆ é™¤
        (r'(å“¼){2,}', ''),          # å“¼å“¼ -> åˆ é™¤
        (r'(å—¨){2,}', ''),          # å—¨å—¨ -> åˆ é™¤
        (r'(å“Ÿ){2,}', ''),          # å“Ÿå“Ÿ -> åˆ é™¤
        (r'(å–‚){2,}', ''),          # å–‚å–‚ -> åˆ é™¤
        (r'(å“){2,}', ''),          # å“å“ -> åˆ é™¤
        (r'(å‘¦){2,}', ''),          # å‘¦å‘¦ -> åˆ é™¤
    ]
    
    for pattern, replacement in interjection_patterns:
        cleaned = re.sub(pattern, replacement, cleaned)
    
    # ç¬¬ä¸‰æ­¥ï¼šåˆ é™¤æ•´ä¸ªæ— æ„ä¹‰çš„æ‹Ÿå£°è¯åºåˆ—
    meaningless_sequences = [
        r'å‘œå“‡\w*',
        r'å—·å—·\w*',
        r'å˜Ÿå™œ\w*',
        r'å•¦å•¦\w*',
        r'æ‹‰æ‹‰\w*',
        r'å‘œå‘œ\w*',
        r'æˆ‘çš„å¤©\w*',
    ]
    
    for pattern in meaningless_sequences:
        cleaned = re.sub(pattern, '', cleaned)
    
    # ç¬¬å››æ­¥ï¼šæ¸…ç†è‹±æ–‡é‡å¤ï¼ˆå¦‚ "hahahaha", "hihihi"ï¼‰
    cleaned = re.sub(r'\b([a-z]{2,})\1{2,}\b', r'\1', cleaned, flags=re.IGNORECASE)  # hahaha -> ha
    cleaned = re.sub(r'\b(ha|hi|hey|lol|hehe){3,}\b', '', cleaned, flags=re.IGNORECASE)  # åˆ é™¤è¿‡åº¦é‡å¤çš„è‹±æ–‡ç¬‘å£°
    
    # æ¸…ç†å¯èƒ½äº§ç”Ÿçš„å¤šä½™ç©ºæ ¼å’Œæ ‡ç‚¹
    cleaned = re.sub(r'\s+', ' ', cleaned).strip()
    cleaned = re.sub(r'\s+([ï¼Œã€‚ï¼ï¼Ÿã€])', r'\1', cleaned)  # æ ‡ç‚¹å‰çš„ç©ºæ ¼
    
    return cleaned


def remove_unicode_replacement_chars(text: str) -> str:
    """
    æ¸…é™¤Unicodeæ›¿æ¢å­—ç¬¦ï¼ˆï¿½ï¼‰å’Œå…¶ä»–æ— æ•ˆå­—ç¬¦
    è¿™äº›å­—ç¬¦é€šå¸¸æ˜¯ç”±äºemoji tokenè§£ç å¤±è´¥äº§ç”Ÿçš„
    
    Args:
        text: è¾“å…¥æ–‡æœ¬
        
    Returns:
        æ¸…é™¤åçš„æ–‡æœ¬
    """
    if not text:
        return text
    
    # ç§»é™¤ Unicode æ›¿æ¢å­—ç¬¦ï¼ˆU+FFFD: ï¿½ï¼‰
    cleaned = text.replace('\ufffd', '')
    cleaned = cleaned.replace('ï¿½', '')
    
    # ç§»é™¤å…¶ä»–å¸¸è§çš„æ— æ•ˆå­—ç¬¦
    invalid_chars = [
        '\x00', '\x01', '\x02', '\x03', '\x04', '\x05', '\x06', '\x07',
        '\x08', '\x0b', '\x0c', '\x0e', '\x0f', '\x10', '\x11', '\x12',
        '\x13', '\x14', '\x15', '\x16', '\x17', '\x18', '\x19', '\x1a',
        '\x1b', '\x1c', '\x1d', '\x1e', '\x1f'
    ]
    for char in invalid_chars:
        cleaned = cleaned.replace(char, '')
    
    # æ¸…ç†å¤šä½™ç©ºæ ¼
    cleaned = re.sub(r'\s+', ' ', cleaned).strip()
    
    return cleaned


def remove_all_emoji(text: str) -> str:
    """
    å¼ºåˆ¶æ¸…é™¤æ–‡æœ¬ä¸­çš„æ‰€æœ‰ emoji è¡¨æƒ…ç¬¦å·
    è¿™æ˜¯æœ€åä¸€é“é˜²çº¿ï¼Œç¡®ä¿è¾“å‡ºä¸­å®Œå…¨æ²¡æœ‰ emoji
    
    Args:
        text: è¾“å…¥æ–‡æœ¬
        
    Returns:
        æ¸…é™¤ emoji åçš„æ–‡æœ¬
    """
    if not text:
        return text
    
    # Emoji Unicode èŒƒå›´ï¼ˆæ‰©å±•è¦†ç›–ï¼‰
    emoji_pattern = re.compile(
        "["
        "\U0001F600-\U0001F64F"  # è¡¨æƒ…ç¬¦å·ï¼ˆğŸ˜€-ğŸ™ï¼‰
        "\U0001F300-\U0001F5FF"  # ç¬¦å·å’Œå›¾æ ‡
        "\U0001F680-\U0001F6FF"  # äº¤é€šå’Œåœ°å›¾ç¬¦å·
        "\U0001F1E0-\U0001F1FF"  # å›½æ——
        "\U0001F900-\U0001F9FF"  # è¡¥å……ç¬¦å·å’Œå›¾æ ‡
        "\U0001FA00-\U0001FA6F"  # æ‰©å±•A
        "\U0001FA70-\U0001FAFF"  # æ‰©å±•B
        "\U00002600-\U000026FF"  # æ‚é¡¹ç¬¦å·ï¼ˆåŒ…å«â¤ï¸â­ï¼‰
        "\U00002700-\U000027BF"  # è£…é¥°ç¬¦å·ï¼ˆåŒ…å«âœ¨âœ…ç­‰ï¼‰
        "\U0000FE00-\U0000FE0F"  # å˜ä½“é€‰æ‹©å™¨ï¼ˆemojiå˜ä½“ï¼‰
        "\U0001F004-\U0001F0CF"  # éº»å°†å’Œæ‰‘å…‹ç‰Œ
        "\U0001F170-\U0001F251"  # å°é—­å­—ç¬¦
        "]+",
        flags=re.UNICODE
    )
    
    # æ¸…é™¤æ‰€æœ‰åŒ¹é…çš„ emoji
    cleaned_text = emoji_pattern.sub('', text)
    
    # æ¸…ç†å¯èƒ½äº§ç”Ÿçš„å¤šä½™ç©ºæ ¼
    cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()
    
    return cleaned_text


def detect_language(text: str) -> str:
    """æ£€æµ‹æ–‡æœ¬çš„ä¸»è¦è¯­è¨€"""
    # æ—¥æ–‡å­—ç¬¦ï¼ˆå¹³å‡åã€ç‰‡å‡åã€æ±‰å­—ï¼‰
    japanese_chars = len(re.findall(r'[\u3040-\u309f\u30a0-\u30ff\u4e00-\u9fff]', text))
    # ä¸­æ–‡å­—ç¬¦
    chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
    # è‹±æ–‡å•è¯
    english_words = len(re.findall(r'\b[a-zA-Z]+\b', text))
    
    total = len(text)
    if total == 0:
        return "unknown"
    
    if japanese_chars > total * 0.3:  # 30%ä»¥ä¸Šæ—¥æ–‡å­—ç¬¦
        return "japanese"
    elif chinese_chars > total * 0.3:
        return "chinese"
    elif english_words > 5:
        return "english"
    return "mixed"


def remove_english_from_japanese(text: str) -> str:
    """
    ä»æ—¥è¯­æ–‡æœ¬ä¸­ç§»é™¤è‹±æ–‡å•è¯ï¼ˆä¿ç•™å¿…è¦çš„è‹±æ–‡ç¼©å†™å’Œä¸“æœ‰åè¯ï¼‰
    """
    # ä¿ç•™çš„è‹±æ–‡è¯ï¼ˆå¸¸è§ç¼©å†™å’Œä¸“æœ‰åè¯ï¼‰
    keep_words = {
        'AI', 'IT', 'PC', 'TV', 'DVD', 'CD', 'USB', 'WiFi', 'LINE', 'Twitter', 
        'Facebook', 'YouTube', 'Google', 'iOS', 'Android', 'OK', 'NG',
        'SNS', 'DM', 'PM', 'AM', 'vs', 'etc', 'App', 'Web', 'A', 'B', 'C'
    }
    
    def replace_english(match):
        word = match.group(0)
        # å¦‚æœæ˜¯ä¿ç•™è¯ï¼Œä¸æ›¿æ¢
        if word in keep_words or word.upper() in keep_words:
            return word
        # å¦‚æœæ˜¯å•ä¸ªå­—æ¯ï¼Œå¯èƒ½æ˜¯ç¼©å†™ï¼Œä¿ç•™
        if len(word) <= 1:
            return word
        # å¦åˆ™ç§»é™¤
        return ''
    
    # åŒ¹é…æ‰€æœ‰è‹±æ–‡å•è¯ï¼ˆåŒ…æ‹¬2ä¸ªå­—æ¯çš„ï¼‰
    text = re.sub(r'\b[a-zA-Z]+\b', replace_english, text)
    
    # æ¸…ç†å¤šä½™çš„ç©ºæ ¼
    text = re.sub(r'\s{2,}', ' ', text)
    text = text.strip()
    
    return text


def remove_duplicates(text: str) -> str:
    """
    ç§»é™¤é‡å¤çš„å¥å­æˆ–çŸ­è¯­
    """
    # æŒ‰å¥å­åˆ†å‰²ï¼ˆæ—¥è¯­å¥å·ã€é—®å·ã€æ„Ÿå¹å·ï¼‰
    sentences = re.split(r'([ã€‚ï¼ï¼Ÿ\n])', text)
    
    # é‡ç»„å¥å­ï¼ˆåŒ…å«æ ‡ç‚¹ï¼‰
    full_sentences = []
    for i in range(0, len(sentences) - 1, 2):
        if i + 1 < len(sentences):
            full_sentences.append(sentences[i] + sentences[i + 1])
        else:
            full_sentences.append(sentences[i])
    
    # å¦‚æœæ²¡æœ‰åˆ†å‰²å‡ºå¥å­ï¼ˆå³åŸæ–‡æ²¡æœ‰ã€‚ï¼ï¼Ÿ\nï¼‰ï¼Œç›´æ¥è¿”å›åŸæ–‡
    if not full_sentences and sentences:
        full_sentences = [sentences[0]]
    
    # å»é‡ï¼ˆä¿ç•™ç¬¬ä¸€æ¬¡å‡ºç°ï¼‰
    seen = set()
    unique_sentences = []
    for sent in full_sentences:
        sent_clean = sent.strip()
        if sent_clean and sent_clean not in seen:
            seen.add(sent_clean)
            unique_sentences.append(sent)
    
    result = ''.join(unique_sentences)
    
    # å¦‚æœç»“æœä¸ºç©ºï¼Œè¿”å›åŸæ–‡
    if not result.strip():
        return text
    
    # é¢å¤–å¤„ç†ï¼šæ£€æµ‹çŸ­è¯­çº§åˆ«çš„é‡å¤ï¼ˆå¦‚"ãŠã¯ã‚ˆã†ï¼ãŠã¯ã‚ˆã†ï¼"ï¼‰
    # æ£€æµ‹2-10ä¸ªå­—çš„é‡å¤æ¨¡å¼
    for length in range(10, 1, -1):
        pattern = r'(.{' + str(length) + r'})\1+'
        result = re.sub(pattern, r'\1', result)
    
    return result


def remove_metadata_and_roles(text: str) -> str:
    """
    ç§»é™¤Assistantã€GSã€userç­‰å…ƒæ•°æ®å’Œè§’è‰²æ ‡è¯†
    """
    # ç§»é™¤è§’è‰²æ ‡è¯†æ¨¡å¼
    role_patterns = [
        r'\b[Aa]ssistant\s*[:\ï¼š]?\s*',  # Assistant: æˆ– assistant:
        r'\b[Uu]ser\s*[:\ï¼š]?\s*',       # User: æˆ– user:
        r'\bGS\s*[:\ï¼š]?\s*',            # GS:
        r'\b[Bb]ot\s*[:\ï¼š]?\s*',        # Bot:
        r'\b[Aa][Ii]\s*[:\ï¼š]?\s*',      # AI:
        r'\bå›ç­”è€…\s*[:\ï¼š]?\s*',         # å›ç­”è€…:
        r'\bè³ªå•è€…\s*[:\ï¼š]?\s*',         # è³ªå•è€…:
        r'^\s*[>\-\*]\s*',               # å¼€å¤´çš„ >, -, * ç­‰
    ]
    
    for pattern in role_patterns:
        text = re.sub(pattern, '', text)
    
    # ç§»é™¤ç‰¹æ®Šæ ‡è®°
    special_markers = [
        r'<\|im_start\|>.*?\n',
        r'<\|im_end\|>',
        r'<\|user\|>',
        r'<\|assistant\|>',
        r'<think>.*?</think>',
        r'\[INST\].*?\[/INST\]',
        r'<<SYS>>.*?<</SYS>>',
    ]
    
    for pattern in special_markers:
        text = re.sub(pattern, '', text, flags=re.DOTALL)
    
    # ç§»é™¤å…ƒæ•°æ®æ‹¬å·å†…å®¹
    metadata_patterns = [
        r'\(ä»¥ä¸‹.*?\)',
        r'ï¼ˆä»¥ä¸‹.*?ï¼‰',
        r'\(æ³¨[ï¼š:].*?\)',
        r'ï¼ˆæ³¨[ï¼š:].*?ï¼‰',
        r'\([Nn]ote[ï¼š:].*?\)',
        r'\\Assistant.*?:',  # åæ–œæ è½¬ä¹‰çš„Assistant
    ]
    
    for pattern in metadata_patterns:
        text = re.sub(pattern, '', text, flags=re.DOTALL)
    
    return text.strip()


def clean_language_contamination(text: str, target_language: str = "japanese") -> str:
    """
    æ¸…ç†è¯­è¨€æ±¡æŸ“ï¼Œç¡®ä¿ç›®æ ‡è¯­è¨€çš„çº¯å‡€åº¦
    æ³¨æ„ï¼šåªæ¸…ç†æ˜æ˜¾çš„æ±¡æŸ“ï¼Œä¸è¦è¿‡åº¦æ¸…ç†
    """
    if target_language == "japanese":
        # ç§»é™¤æ˜æ˜¾çš„çº¯ä¸­æ–‡çŸ­è¯­ï¼ˆä¸åŒ…æ‹¬æ—¥è¯­æ±‰å­—ï¼‰
        # åªåˆ é™¤æ˜æ˜¾çš„ä¸­æ–‡è§£é‡Šæ€§è¯æ±‡
        chinese_only_phrases = [
            'åˆ†æå¦‚ä¸‹', 'å»ºè®®å¦‚ä¸‹', 'å›ç­”å¦‚ä¸‹', 'é—®é¢˜æ˜¯', 'å¦‚æœæœ‰æœºä¼š',
            'è¿™ä¸ªåœ°æ–¹', 'é‚£ä¸ªåœ°æ–¹', 'ä»€ä¹ˆæ—¶å€™', 'æ€ä¹ˆæ ·', 'ä¸ºä»€ä¹ˆ',  
        ]
        
        for phrase in chinese_only_phrases:
            text = text.replace(phrase, '')
        
        # ä¸å†ä½¿ç”¨æ¿€è¿›çš„æ­£åˆ™åˆ é™¤ï¼Œé¿å…è¯¯åˆ 
        # åªåˆ é™¤æ˜æ˜¾æ˜¯çº¯è‹±æ–‡çš„é•¿å¥ï¼ˆä½†ä¿ç•™æ—¥æ–‡å†…å®¹ï¼‰
        # è¿™é‡Œæš‚æ—¶ç¦ç”¨ï¼Œå› ä¸ºå¤ªå®¹æ˜“è¯¯åˆ 
        
    return text.strip()


def extract_first_valid_sentence(text: str, max_length: int = 512) -> str:
    """
    æå–ç¬¬ä¸€å¥æœ‰æ•ˆçš„ã€å¹²å‡€çš„å›å¤
    """
    # æŒ‰å¥å­åˆ†å‰²
    sentences = re.split(r'([ã€‚ï¼ï¼Ÿ\n])', text)
    
    # æ‰¾åˆ°ç¬¬ä¸€å¥æœ‰æ•ˆçš„å¥å­ï¼ˆé•¿åº¦åˆç†ä¸”ä¸æ˜¯åƒåœ¾ï¼‰
    for i in range(0, len(sentences) - 1, 2):
        if i + 1 < len(sentences):
            sentence = sentences[i] + sentences[i + 1]
        else:
            sentence = sentences[i]
        
        sentence = sentence.strip()
        
        # è¿‡æ»¤æ¡ä»¶
        if len(sentence) < 3:  # å¤ªçŸ­
            continue
        if re.match(r'^[\s\*\-\.]+$', sentence):  # åªæœ‰ç¬¦å·
            continue
        if 'absence' in sentence.lower():  # åŒ…å«æ˜æ˜¾é”™è¯¯
            continue
        
        # æ‰¾åˆ°æœ‰æ•ˆå¥å­
        return sentence[:max_length]
    
    # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œè¿”å›åŸæ–‡çš„å‰ä¸€éƒ¨åˆ†
    return text[:max_length].strip()


def enhanced_clean_model_output(
    text: str,
    max_length: int = 512,
    is_japanese_task: bool = False,
    remove_english: bool = True,
    remove_repeats: bool = True,
    remove_emoji: bool = True,  # âœ… æ–°å¢ï¼šæ˜¯å¦å¼ºåˆ¶æ¸…é™¤ emojiï¼ˆé»˜è®¤Trueï¼‰
    debug: bool = False
) -> str:
    """
    å¢å¼ºç‰ˆè¾“å‡ºæ¸…ç†å‡½æ•°
    
    Args:
        text: åŸå§‹æ¨¡å‹è¾“å‡º
        max_length: æœ€å¤§è¾“å‡ºé•¿åº¦
        is_japanese_task: æ˜¯å¦ä¸ºæ—¥è¯­ä»»åŠ¡
        remove_english: æ˜¯å¦ç§»é™¤è‹±æ–‡ï¼ˆä»…åœ¨æ—¥è¯­ä»»åŠ¡ä¸­ï¼‰
        remove_repeats: æ˜¯å¦ç§»é™¤é‡å¤
        remove_emoji: æ˜¯å¦å¼ºåˆ¶æ¸…é™¤ emojiï¼ˆé»˜è®¤Trueï¼Œä½œä¸ºæœ€åä¸€é“é˜²çº¿ï¼‰
        debug: æ˜¯å¦æ‰“å°è°ƒè¯•ä¿¡æ¯
    
    Returns:
        æ¸…ç†åçš„æ–‡æœ¬
    """
    if not text:
        return ""
    
    original_text = text
    if debug:
        print(f"[DEBUG] åŸå§‹è¾“å…¥: {text[:100]}...")
    
    # 1. ç§»é™¤å…ƒæ•°æ®å’Œè§’è‰²æ ‡è¯†
    text = remove_metadata_and_roles(text)
    
    # 2. åŸºç¡€æ¸…ç†ï¼ˆç§»é™¤æ€è€ƒè¿‡ç¨‹ç­‰ï¼‰
    text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)
    text = text.replace('<think>', '').replace('</think>', '')
    
    # 3. ç§»é™¤åœæ­¢æ ‡è®°åçš„å†…å®¹
    stop_markers = [
        r'\nï¼ˆ', r'\n\(',
        r'\n[*]{2,}',
        r'\nâ€»',
        r'\nå•é¡Œ[ï¼š:]',
        r'\næœ€ç»ˆç”Ÿæˆ',
        r'\nåˆ†æ',
        r'\nå»ºè®®',
        r'\n\s*---',
        r'\n[A-Z]\)',
        r'RealPersonaChat',
        r'\\Assistant',  # ç‰¹åˆ«å¤„ç†
    ]
    
    combined_stop_pattern = '|'.join(stop_markers)
    match = re.search(combined_stop_pattern, text)
    if match:
        text = text[:match.start()]
    if debug:
        print(f"[DEBUG] Step 3 - ç§»é™¤åœæ­¢æ ‡è®°å: {repr(text[:100])}")
        print(f"[DEBUG] Step 3 - é•¿åº¦: {len(text)}")
    
    # 4. ç§»é™¤é‡å¤ï¼ˆåœ¨æå–ç¬¬ä¸€å¥ä¹‹å‰ï¼‰
    if remove_repeats:
        text = remove_duplicates(text)
    if debug:
        print(f"[DEBUG] Step 4 - ç§»é™¤é‡å¤å: {repr(text[:100])}")
        print(f"[DEBUG] Step 4 - é•¿åº¦: {len(text)}")
    
    # 5. æå–ç¬¬ä¸€å¥æœ‰æ•ˆå†…å®¹
    text = extract_first_valid_sentence(text, max_length)
    if debug:
        print(f"[DEBUG] Step 5 - æå–ç¬¬ä¸€å¥: {repr(text)}")
        print(f"[DEBUG] Step 5 - é•¿åº¦: {len(text)}")
    
    # 6. é’ˆå¯¹æ—¥è¯­ä»»åŠ¡çš„ç‰¹æ®Šå¤„ç†ï¼ˆåœ¨æå–ç¬¬ä¸€å¥ä¹‹åï¼‰
    if is_japanese_task:
        if debug:
            print(f"[DEBUG] Step 6a - å¼€å§‹æ—¥è¯­å¤„ç†, remove_english={remove_english}")
        # ç§»é™¤è‹±æ–‡å•è¯
        if remove_english:
            before_remove = text
            text = remove_english_from_japanese(text)
            if debug:
                print(f"[DEBUG] Step 6b - ç§»é™¤è‹±æ–‡å‰: {before_remove}")
                print(f"[DEBUG] Step 6c - ç§»é™¤è‹±æ–‡å: {text}")
                print(f"[DEBUG] Step 6c - åŒ…å«experience: {'experience' in text}")
        
        # ç§»é™¤ä¸­æ–‡å’Œè‹±æ–‡æ±¡æŸ“
        before_clean = text
        text = clean_language_contamination(text, "japanese")
        if debug:
            print(f"[DEBUG] Step 6d - æ¸…ç†æ±¡æŸ“å‰: {before_clean}")
            print(f"[DEBUG] Step 6e - æ¸…ç†æ±¡æŸ“å: {text}")
            print(f"[DEBUG] Step 6e - åŒ…å«experience: {'experience' in text}")
    
    # 7. æœ€åæ¸…ç†
    text = text.strip()
    if debug:
        print(f"[DEBUG] Step 7 - æœ€åæ¸…ç†: {text}")
        print(f"[DEBUG] Step 7 - texté•¿åº¦: {len(text)}")
    
    # 8. âœ… æ¸…ç† Unicode æ›¿æ¢å­—ç¬¦ï¼ˆä¹±ç  ï¿½ï¼‰
    before_unicode_cleanup = text
    text = remove_unicode_replacement_chars(text)
    if debug and text != before_unicode_cleanup:
        print(f"[DEBUG] Step 8 - æ¸…ç†ä¹±ç å­—ç¬¦")
        print(f"[DEBUG] Step 8a - æ¸…ç†å‰: {before_unicode_cleanup}")
        print(f"[DEBUG] Step 8b - æ¸…ç†å: {text}")
    
    # 9. âœ… æ¸…ç†è¿‡åº¦é‡å¤çš„è¯­æ°”è¯
    before_interjection_removal = text
    text = remove_excessive_interjections(text)
    if debug and text != before_interjection_removal:
        print(f"[DEBUG] Step 9 - æ¸…ç†è¯­æ°”è¯")
        print(f"[DEBUG] Step 9a - æ¸…ç†å‰: {before_interjection_removal}")
        print(f"[DEBUG] Step 9b - æ¸…ç†å: {text}")
    
    # 10. âœ… å¼ºåˆ¶æ¸…é™¤ emojiï¼ˆæœ€åä¸€é“é˜²çº¿ï¼‰
    if remove_emoji:
        before_emoji_removal = text
        had_emoji = contains_emoji(text)
        text = remove_all_emoji(text)
        if debug and had_emoji:
            print(f"[DEBUG] Step 10 - æ£€æµ‹åˆ° emojiï¼")
            print(f"[DEBUG] Step 10a - æ¸…é™¤å‰: {before_emoji_removal}")
            print(f"[DEBUG] Step 10b - æ¸…é™¤å: {text}")
        elif debug:
            print(f"[DEBUG] Step 10 - æœªæ£€æµ‹åˆ° emojiï¼Œè·³è¿‡æ¸…é™¤")
    
    # 11. å…œåº•ï¼šå¦‚æœæ¸…ç†åå¤ªçŸ­ï¼Œè¿”å›åŸæ–‡çš„ä¸€éƒ¨åˆ†ï¼ˆä¹Ÿæ¸…é™¤ä¹±ç ã€è¯­æ°”è¯å’Œemojiï¼‰
    if len(text) < 3 and len(original_text) > 5:
        if debug:
            print(f"[DEBUG] Step 11 - è§¦å‘å…œåº•é€»è¾‘ï¼è¿”å›åŸæ–‡")
        fallback_text = original_text[:max_length].strip()
        # å³ä½¿æ˜¯å…œåº•é€»è¾‘ï¼Œä¹Ÿè¦æ¸…é™¤ä¹±ç ã€è¯­æ°”è¯å’Œ emoji
        fallback_text = remove_unicode_replacement_chars(fallback_text)
        fallback_text = remove_excessive_interjections(fallback_text)
        if remove_emoji:
            fallback_text = remove_all_emoji(fallback_text)
        return fallback_text
    
    if debug:
        print(f"[DEBUG] Step 11 - æ­£å¸¸è¿”å›æ¸…ç†åçš„text")
    return text


def test_enhanced_clean():
    """æµ‹è¯•å¢å¼ºç‰ˆæ¸…ç†å‡½æ•°"""
    
    test_cases = [
        {
            "input": "ãŠã‚ã‚ã€å£°å„ªã•ã‚“ã®è¦–ç‚¹ã‹ã‚‰ã®èˆå° experience è€ƒãˆã¦ã¿ãŸã‚‰ã€ã‚ˆã‚Šãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ãªæ„Ÿã˜ã«èã“ãˆã¾ã™ã‚ˆã­â€¦",
            "expected": "ãŠã‚ã‚ã€å£°å„ªã•ã‚“ã®è¦–ç‚¹ã‹ã‚‰ã®èˆå°è€ƒãˆã¦ã¿ãŸã‚‰ã€ã‚ˆã‚Šãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ãªæ„Ÿã˜ã«èã“ãˆã¾ã™ã‚ˆã­â€¦",
            "desc": "ç§»é™¤è‹±æ–‡å•è¯"
        },
        {
            "input": "ãŠã¯ã‚ˆã†ã”ã–ã„ã¾ã—ãŸï¼ï¼\nãŠã¯ã‚ˆã†ã”ã–ã„ã¾ã—ãŸï¼ï¼\nãŠã¯ã‚ˆã†ã”ã–ã„ã¾ã—ãŸï¼",
            "expected": "ãŠã¯ã‚ˆã†ã”ã–ã„ã¾ã—ãŸï¼ï¼",
            "desc": "ç§»é™¤é‡å¤å¥å­"
        },
        {
            "input": "ãŠã‚ã‚ã€åƒè‘‰ really ç†±ã‹ã£ãŸã§ã™ã­â€¦",
            "expected": "ãŠã‚ã‚ã€åƒè‘‰ç†±ã‹ã£ãŸã§ã™ã­â€¦",
            "desc": "ç§»é™¤æ—¥æ–‡ä¸­çš„è‹±æ–‡"
        },
        {
            "input": "å¾³å³¶çœŒã«ã¯é˜¿æ³¢èˆä¼ã¨ã‹ã‚ã‚‹ã˜ã‚ƒãªã„ã§ã™ã‹ã€‚ãã‚Œé£Ÿã‚ãªã„ã¨æã ã‚ˆï¼\\Assistant GS: ã‚ˆãè€³ã«ã—ã¾ã—ãŸã‘ã©...",
            "expected": "å¾³å³¶çœŒã«ã¯é˜¿æ³¢èˆä¼ã¨ã‹ã‚ã‚‹ã˜ã‚ƒãªã„ã§ã™ã‹ã€‚",
            "desc": "ç§»é™¤Assistantæ ‡è¯†"
        },
        {
            "input": "å¾·å³¶çœŒã«ã¯è¡Œã£ã¦ã¿ãŸã„ã¨æ€ã„ã¾ã™ã€‚å››å›½ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ã‚‚ã‚ã‚Šã‚ã„ãªã®ã§ã€ãã¡ã‚‰æ–¹é¢ã¸è¡ŒããŸã„ãªãã¨æ€ã£ãŸã‚Šã—ã¾ã™ã€‚",
            "expected": "å¾·å³¶çœŒã«ã¯è¡Œã£ã¦ã¿ãŸã„ã¨æ€ã„ã¾ã™ã€‚",
            "desc": "æå–ç¬¬ä¸€å¥"
        },
        {
            "input": "è°¢è°¢ä½ çš„å¸®åŠ©ï¼â¤ï¸â¤ï¸â¤ï¸",
            "expected": "è°¢è°¢ä½ çš„å¸®åŠ©ï¼",
            "desc": "âœ… æ¸…é™¤ emoji è¡¨æƒ…ç¬¦å·"
        },
        {
            "input": "å¤ªæ£’äº†ğŸ˜ŠğŸ˜ŠğŸ˜Šéå¸¸å¥½ğŸ‘ğŸ‘",
            "expected": "å¤ªæ£’äº†éå¸¸å¥½",
            "desc": "âœ… æ¸…é™¤å¤šä¸ªä¸åŒçš„ emoji"
        },
        {
            "input": "ãŠã¯ã‚ˆã†ã”ã–ã„ã¾ã™ğŸ˜€âœ¨",
            "expected": "ãŠã¯ã‚ˆã†ã”ã–ã„ã¾ã™",
            "desc": "âœ… æ¸…é™¤æ—¥æ–‡ä¸­çš„ emoji"
        }
    ]
    
    print("=" * 80)
    print("å¢å¼ºç‰ˆæ¸…ç†å‡½æ•°æµ‹è¯•")
    print("=" * 80)
    
    for i, case in enumerate(test_cases, 1):
        result = enhanced_clean_model_output(
            case["input"],
            max_length=512,
            is_japanese_task=True,
            remove_english=True,
            remove_repeats=True
        )
        
        print(f"\næµ‹è¯• {i}: {case['desc']}")
        print(f"è¾“å…¥: {case['input'][:80]}...")
        print(f"è¾“å‡º: {result}")
        print(f"é¢„æœŸ: {case['expected']}")
        print(f"é€šè¿‡: {'âœ…' if case['expected'] in result or result in case['expected'] else 'âŒ'}")
    
    print("\n" + "=" * 80)


if __name__ == '__main__':
    test_enhanced_clean()
