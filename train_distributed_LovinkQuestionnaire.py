import json
import argparse
import os
import sys
from pathlib import Path
import random
import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler

# æ³¨é‡Šæ‰çˆ¶ç›®å½•è·¯å¾„ï¼Œç»Ÿä¸€ä½¿ç”¨å½“å‰ç›®å½•ï¼ˆprompt_improvement/Lovink/ï¼‰ä¸‹çš„æ–‡ä»¶
# sys.path.insert(0, str(Path(__file__).parent.parent))
# from data_loader import load_train_data, extract_training_samples, get_user_only_history # æ—§ç‰ˆæœ¬ å¤æ‚çš„è®­ç»ƒprompt 
# from data_loader_more_data import load_train_data, extract_training_samples, get_user_only_history # æ–°ç‰ˆæœ¬ ç®€çŸ­çš„è®­ç»ƒprompt

# âœ… ä½¿ç”¨ä¸“é—¨çš„ LovinkQuestionnaire æ•°æ®åŠ è½½å™¨
from data_loader_lovink_questionnaire import (
    load_questionnaire_data as load_train_data,
    extract_questionnaire_samples as extract_training_samples,
    add_questionnaire_history_to_samples,
    split_train_val
)

# ä»é€šç”¨æ¨¡å—å¯¼å…¥ DynamicPaddingDataset å’Œ collate_fn
from train_with_dynamic_padding import DynamicPaddingDataset, dynamic_padding_collate_fn
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    EarlyStoppingCallback,
    TrainingArguments,
    Trainer
)
from typing import List, Dict, Any, Optional
import torch.nn as nn


def check_flash_attention_support():
    """æ£€æŸ¥ç³»ç»Ÿæ˜¯å¦æ”¯æŒ FlashAttention 2"""
    try:
        import flash_attn
        flash_version = getattr(flash_attn, '__version__', 'unknown')
        print(f"FlashAttention å·²å®‰è£…ï¼Œç‰ˆæœ¬: {flash_version}")
        return True
    except ImportError:
        print("è­¦å‘Š: FlashAttention æœªå®‰è£…")
        return False


def setup_distributed():
    """åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ"""
    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:
        rank = int(os.environ["RANK"])
        world_size = int(os.environ['WORLD_SIZE'])
        local_rank = int(os.environ['LOCAL_RANK'])
    elif 'SLURM_PROCID' in os.environ:
        rank = int(os.environ['SLURM_PROCID'])
        world_size = int(os.environ['SLURM_NTASKS'])
        local_rank = rank % torch.cuda.device_count()
    else:
        print('è­¦å‘Š: æœªæ£€æµ‹åˆ°åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒå˜é‡ï¼Œä½¿ç”¨å•å¡è®­ç»ƒ')
        rank = 0
        world_size = 1
        local_rank = 0
    
    torch.cuda.set_device(local_rank)
    
    if world_size > 1:
        dist.init_process_group(
            backend='nccl',
            init_method='env://',
            world_size=world_size,
            rank=rank
        )
    
    return rank, world_size, local_rank


def cleanup_distributed():
    """æ¸…ç†åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ"""
    if dist.is_initialized():
        dist.destroy_process_group()


def sample_per_user(
    all_samples: List[Dict[str, Any]],
    max_samples_per_user: int = 2,
    random_seed: int = 42
) -> List[Dict[str, Any]]:
    """
    å¯¹æ¯ä¸ªç”¨æˆ·çš„æ ·æœ¬è¿›è¡Œé‡‡æ ·
     é’ˆå¯¹é—®å·æ•°æ®çš„ç‰¹æ®Šå¤„ç†ï¼š
    - éšæœºé€‰æ‹© max_samples_per_user æ¡ä½œä¸ºé¢„æµ‹ç›®æ ‡
    - å…¶ä»–æ‰€æœ‰é—®ç­”éƒ½ä½œä¸ºå†å²ä¿¡æ¯
    
    Args:
        all_samples: æ‰€æœ‰è®­ç»ƒæ ·æœ¬
        max_samples_per_user: æ¯ä¸ªç”¨æˆ·æœ€å¤šä¿ç•™å¤šå°‘ä¸ªæ ·æœ¬ä½œä¸ºé¢„æµ‹ç›®æ ‡
        random_seed: éšæœºç§å­ï¼ˆä¿è¯å¯å¤ç°ï¼‰
    
    Returns:
        é‡‡æ ·åçš„æ ·æœ¬åˆ—è¡¨ï¼ˆæ¯ä¸ªæ ·æœ¬éƒ½åŒ…å«è¯¥ç”¨æˆ·çš„å…¶ä»–é—®ç­”ä½œä¸ºå†å²ï¼‰
    """
    random.seed(random_seed)
    
    # æŒ‰ç”¨æˆ·åˆ†ç»„
    user_samples = {}
    for sample in all_samples:
        user_hash = sample.get('user_hash', 'unknown')
        if user_hash not in user_samples:
            user_samples[user_hash] = []
        user_samples[user_hash].append(sample)
    
    # å¯¹æ¯ä¸ªç”¨æˆ·çš„æ ·æœ¬è¿›è¡Œé‡‡æ ·
    sampled_samples = []
    total_history_items = 0
    
    for user_hash, samples in user_samples.items():
        if len(samples) <= max_samples_per_user:
            # æ ·æœ¬æ•°ä¸è¶…è¿‡é™åˆ¶ï¼Œå…¨éƒ¨ä¿ç•™ï¼ˆä½†ä¸æ·»åŠ å†å²ï¼‰
            sampled_samples.extend(samples)
        else:
            # âœ… éšæœºé€‰æ‹© max_samples_per_user ä¸ªä½œä¸ºé¢„æµ‹ç›®æ ‡
            sampled_indices = random.sample(range(len(samples)), max_samples_per_user)
            sampled_indices_set = set(sampled_indices)
            
            # âœ… æ„å»ºå†å²ï¼šæ‰€æœ‰æœªè¢«é€‰ä¸­çš„é—®ç­”
            history_samples = [samples[i] for i in range(len(samples)) if i not in sampled_indices_set]
            total_history_items += len(history_samples)
            
            # âœ… ä¸ºæ¯ä¸ªè¢«é€‰ä¸­çš„æ ·æœ¬æ·»åŠ å†å²æ ‡è®°
            for idx in sampled_indices:
                selected_sample = samples[idx]
                # å°†å†å²æ ·æœ¬çš„ä¿¡æ¯å­˜å‚¨èµ·æ¥ï¼Œåç»­ä¼šè¢« add_questionnaire_history_to_samples å¤„ç†
                selected_sample['_other_samples_for_history'] = history_samples
                sampled_samples.append(selected_sample)
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print(f"\n{'='*50}")
    print(f"é—®å·æ ·æœ¬é‡‡æ ·ç»Ÿè®¡ï¼ˆé¢„æµ‹ç›®æ ‡ vs å†å²ï¼‰:")
    print(f"  åŸå§‹æ ·æœ¬æ•°: {len(all_samples)}")
    print(f"  ç”¨æˆ·æ•°: {len(user_samples)}")
    print(f"  æ¯ç”¨æˆ·é€‰ä¸ºé¢„æµ‹ç›®æ ‡çš„æ ·æœ¬æ•°: {max_samples_per_user}")
    print(f"  é‡‡æ ·åæ ·æœ¬æ•°ï¼ˆé¢„æµ‹ç›®æ ‡ï¼‰: {len(sampled_samples)}")
    print(f"  å…¶ä»–æ ·æœ¬ä½œä¸ºå†å²: {len(all_samples) - len(sampled_samples)}")
    print(f"  å¹³å‡æ¯ä¸ªæ ·æœ¬çš„å†å²æ¡ç›®: {total_history_items / len(sampled_samples) if sampled_samples else 0:.1f}")
    print(f"  é‡‡æ ·æ¯”ä¾‹: {len(sampled_samples) / len(all_samples) * 100:.1f}%")
    print(f"{'='*50}\n")
    
    return sampled_samples


def main():
    parser = argparse.ArgumentParser(description='åˆ†å¸ƒå¼æ¶ˆèå®éªŒè®­ç»ƒï¼ˆFlashAttention 2 + åŠ¨æ€Paddingï¼‰- LovinkQuestionnaire')
    parser.add_argument('--config', type=str,
                       default='config_LovinkQuestionnaire.json',
                       help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--ablation_config', type=str, required=True,
                       choices=['profile_and_history_and_context', 'profile_and_history', 'profile_and_context', 
                               'history_and_context', 'profile_only', 'history_only', 'context_only'],
                       help='æ¶ˆèå®éªŒé…ç½®')
    parser.add_argument('--val_ratio', type=float, default=0.1,
                       help='éªŒè¯é›†æ¯”ä¾‹')
    parser.add_argument('--max_epochs', type=int, default=50,
                       help='æœ€å¤§è®­ç»ƒè½®æ¬¡ï¼ˆé»˜è®¤ï¼š50ï¼‰')
    parser.add_argument('--early_stopping_patience', type=int, default=3,
                       help='æ—©åœè€å¿ƒå€¼ï¼ˆé»˜è®¤ï¼š3ï¼‰')
    parser.add_argument('--early_stopping_threshold', type=float, default=0.001,
                       help='æ—©åœé˜ˆå€¼ï¼ˆé»˜è®¤ï¼š0.001ï¼‰')
    parser.add_argument('--output_dir', type=str, default=None,
                       help='æ¨¡å‹è¾“å‡ºç›®å½•')
    parser.add_argument('--local_rank', type=int, default=-1,
                       help='æœ¬åœ°è¿›ç¨‹rankï¼ˆç”± torch.distributed.launch è‡ªåŠ¨è®¾ç½®ï¼‰')
    parser.add_argument('--wandb_project', type=str, default='Qwen3-LovinkQuestionnaire',
                       help='Weights & Biasesé¡¹ç›®åç§°ï¼ˆé»˜è®¤ï¼šQwen3-LovinkQuestionnaireï¼‰')
    parser.add_argument('--wandb_run_name', type=str, default=None,
                       help='Weights & Biasesè¿è¡Œåç§°ï¼ˆé»˜è®¤ï¼šè‡ªåŠ¨ç”Ÿæˆï¼‰')
    parser.add_argument('--disable_flash_attn', action='store_true',
                       help='ç¦ç”¨FlashAttention 2ï¼Œä½¿ç”¨æ ‡å‡†attention')
    parser.add_argument('--deepspeed', type=str, default=None,
                       help='DeepSpeedé…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰')
    
    # æ–°å¢ï¼šPrompt æ¨¡æ¿æ§åˆ¶å‚æ•°
    parser.add_argument('--prompt_style', type=str, default='simple',
                       choices=['simple', 'detailed', 'lovink'],
                       help='Prompt é£æ ¼ï¼šsimple=ç®€æ´æ ‡ç­¾æ ¼å¼ï¼ˆé»˜è®¤ï¼‰ï¼Œdetailed=è¯¦ç»†æ¨¡æ¿ï¼Œlovink=Lovinké£æ ¼')
    parser.add_argument('--template_filename', type=str, default=None,
                       help='æŒ‡å®šæ¨¡æ¿æ–‡ä»¶åï¼ˆä»…å½“ prompt_style=detailed æ—¶ç”Ÿæ•ˆï¼‰')
    
    # æ–°å¢ï¼šæ¯ç”¨æˆ·é‡‡æ ·å‚æ•°
    parser.add_argument('--max_samples_per_user', type=int, default=None,
                       help='æ¯ä¸ªç”¨æˆ·æœ€å¤šä¿ç•™å¤šå°‘ä¸ªæ ·æœ¬ï¼ˆç”¨äºå‡å°‘è®­ç»ƒæ•°æ®é‡ï¼‰')
    parser.add_argument('--sample_seed', type=int, default=42,
                       help='é‡‡æ ·éšæœºç§å­ï¼ˆé»˜è®¤ï¼š42ï¼Œä¿è¯å¯å¤ç°ï¼‰')
    
    # æ–°å¢ï¼šå†å²ç­–ç•¥å‚æ•°
    parser.add_argument('--history_strategy', type=str, default='random',
                       choices=['recent', 'random', 'relevant', 'all_previous', 'fixed_ratio', 'fixed_count', 'none'],
                       help='å†å²é€‰æ‹©ç­–ç•¥ï¼šrecent=æœ€è¿‘çš„å†å²ï¼Œrandom=éšæœºé€‰æ‹©ï¼ˆé»˜è®¤ï¼‰ï¼Œall_previous=æ‰€æœ‰ä¹‹å‰çš„ï¼Œfixed_ratio=å›ºå®šæ¯”ä¾‹ï¼Œfixed_count=å›ºå®šæ•°é‡ï¼Œnone=ä¸ä½¿ç”¨å†å²')
    parser.add_argument('--history_ratio', type=float, default=1.0,
                       help='å†å²ä½¿ç”¨æ¯”ä¾‹ï¼ˆ0.0-1.0ï¼‰ï¼Œç”¨äºæ§åˆ¶ä½¿ç”¨å¤šå°‘æ¯”ä¾‹çš„å†å²è®°å½•')
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ
    rank, world_size, local_rank = setup_distributed()
    
    # åªåœ¨ä¸»è¿›ç¨‹æ‰“å°ä¿¡æ¯
    is_main_process = (rank == 0)
    
    # é…ç½® Weights & Biases (åªåœ¨ä¸»è¿›ç¨‹)
    if args.wandb_project:
        try:
            import wandb
            os.environ['WANDB_PROJECT'] = args.wandb_project
            if args.wandb_run_name:
                os.environ['WANDB_NAME'] = args.wandb_run_name
            if is_main_process:
                print(f"âœ“ å·²å¯ç”¨ Weights & Biases ç›‘æ§")
        except ImportError:
            if is_main_process:
                print("è­¦å‘Š: wandb æœªå®‰è£…")
            args.wandb_project = None
    
    if is_main_process:
        print(f"=" * 80)
        print(f"åˆ†å¸ƒå¼è®­ç»ƒè®¾ç½®ï¼ˆFlashAttention 2 + åŠ¨æ€Paddingï¼‰:")
        print(f"  World Size (æ€»è¿›ç¨‹æ•°): {world_size}")
        print(f"  Rank (è¿›ç¨‹ID): {rank}")
        print(f"  Local Rank (æœ¬åœ°GPU ID): {local_rank}")
        print(f"  ä½¿ç”¨ {world_size} å¼ GPUè¿›è¡Œå¹¶è¡Œè®­ç»ƒ")
        print(f"  ä¼˜åŒ–ç­–ç•¥: FlashAttention 2 + åŠ¨æ€Batch Padding")
        if args.deepspeed:
            print(f"  DeepSpeedé…ç½®: {args.deepspeed}")
        print(f"=" * 80)
    
    # æ£€æŸ¥ FlashAttention æ”¯æŒ
    use_flash_attn = False
    if not args.disable_flash_attn and is_main_process:
        use_flash_attn = check_flash_attention_support()
    
    # å¹¿æ’­ use_flash_attn åˆ°æ‰€æœ‰è¿›ç¨‹
    if world_size > 1:
        use_flash_attn_tensor = torch.tensor([use_flash_attn], dtype=torch.bool, device=f'cuda:{local_rank}')
        dist.broadcast(use_flash_attn_tensor, src=0)
        use_flash_attn = use_flash_attn_tensor.item()
    
    # éªŒè¯GPUæ˜¯å¦å¯ç”¨
    if torch.cuda.is_available():
        if is_main_process:
            print(f"CUDA å¯ç”¨ï¼Œæ€»GPUæ•°é‡: {torch.cuda.device_count()}")
            print(f"å½“å‰è¿›ç¨‹ä½¿ç”¨ GPU: {local_rank}")
            gpu_name = torch.cuda.get_device_name(local_rank)
            gpu_memory = torch.cuda.get_device_properties(local_rank).total_memory / 1024**3
            print(f"GPU åç§°: {gpu_name}")
            print(f"GPU æ€»å†…å­˜: {gpu_memory:.2f} GB")
            
            # æ£€æŸ¥GPUè®¡ç®—èƒ½åŠ›
            compute_capability = torch.cuda.get_device_capability(local_rank)
            print(f"GPU è®¡ç®—èƒ½åŠ›: {compute_capability[0]}.{compute_capability[1]}")
            if compute_capability[0] >= 8:  # A100/H100
                print("âœ“ GPUæ”¯æŒFlashAttention 2ä¼˜åŒ–")
            else:
                print("GPUè®¡ç®—èƒ½åŠ›è¾ƒä½ï¼ŒFlashAttention 2æ€§èƒ½å¯èƒ½å—é™")
    else:
        print("é”™è¯¯: CUDA ä¸å¯ç”¨")
        cleanup_distributed()
        return
    
    # åŠ è½½é…ç½®ï¼ˆä¼˜å…ˆä½¿ç”¨å½“å‰ç›®å½•ï¼Œæ”¯æŒç»å¯¹è·¯å¾„ï¼‰
    if os.path.isabs(args.config):
        config_path = args.config
    else:
        # ä¼˜å…ˆæŸ¥æ‰¾å½“å‰ç›®å½•
        local_config = Path(__file__).parent / args.config
        if local_config.exists():
            config_path = str(local_config)
        else:
            # å›é€€åˆ°çˆ¶ç›®å½•ï¼ˆå‘åå…¼å®¹ï¼‰
            config_path = os.path.join(Path(__file__).parent.parent, args.config)
    
    with open(config_path, 'r', encoding='utf-8') as f:
        config = json.load(f)
    
    # è·å–æ¶ˆèé…ç½®
    ablation_config = config['ablation_configs'][args.ablation_config]
    use_profile = ablation_config.get('use_profile', True)
    use_history = ablation_config.get('use_history', True)
    use_context = ablation_config.get('use_context', True)
    config_name = ablation_config['name']
    
    if is_main_process:
        print("=" * 80)
        print(f"æ¶ˆèå®éªŒï¼ˆFlashAttn2 + åŠ¨æ€Paddingï¼‰: {config_name}")
        print(f"ä½¿ç”¨é…ç½®: profile={use_profile}, history={use_history}, context={use_context}")
        print(f"FlashAttention 2: {'å¯ç”¨' if use_flash_attn else 'ç¦ç”¨'}")
        print("=" * 80)
    
    # åŠ è½½è®­ç»ƒæ•°æ®
    if is_main_process:
        print("åŠ è½½è®­ç»ƒæ•°æ®...")
    train_path = config['data']['train_path']
    train_data = load_train_data(train_path)
    
    if not train_data:
        print(f"é”™è¯¯: æ— æ³•åŠ è½½è®­ç»ƒæ•°æ®")
        cleanup_distributed()
        return
    
    # æå–è®­ç»ƒæ ·æœ¬
    all_samples = extract_training_samples(train_data, debug=is_main_process)
    if is_main_process:
        print(f"æå–äº† {len(all_samples)} ä¸ªè®­ç»ƒæ ·æœ¬")
    
    # æ–°å¢ï¼šæ¯ç”¨æˆ·é‡‡æ ·ï¼ˆå¦‚æœæŒ‡å®šäº† max_samples_per_userï¼‰
    if args.max_samples_per_user is not None:
        if is_main_process:
            print(f"\nå¯¹æ¯ä¸ªç”¨æˆ·è¿›è¡Œé‡‡æ ·ï¼ˆæ¯ç”¨æˆ·æœ€å¤š {args.max_samples_per_user} ä¸ªæ ·æœ¬ï¼‰...")
        all_samples = sample_per_user(
            all_samples,
            max_samples_per_user=args.max_samples_per_user,
            random_seed=args.sample_seed
        )
    
    # æ·»åŠ å†å²ä¿¡æ¯
    if use_history:
        if is_main_process:
            print("æ·»åŠ å†å²ä¿¡æ¯...")
            print(f"  å†å²ç­–ç•¥: {args.history_strategy}")
            print(f"  å†å²æ¯”ä¾‹: {args.history_ratio:.1%}")
        
        # ä½¿ç”¨ä¸“é—¨çš„é—®å·å†å²æ·»åŠ å‡½æ•°
        all_samples = add_questionnaire_history_to_samples(
            samples=all_samples,
            history_strategy=args.history_strategy,
            history_ratio=args.history_ratio,
            seed=args.sample_seed
        )
    
    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    train_samples, val_samples = split_train_val(all_samples, args.val_ratio)
    if is_main_process:
        print(f"è®­ç»ƒé›†: {len(train_samples)} ä¸ªæ ·æœ¬")
        print(f"éªŒè¯é›†: {len(val_samples)} ä¸ªæ ·æœ¬")
        print(f"æ¯ä¸ªGPUå®é™…å¤„ç†çº¦ {len(train_samples) // world_size} ä¸ªè®­ç»ƒæ ·æœ¬")
    
    # è·å–æ¨¡å‹é…ç½®
    model_config = config['model']
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    if args.output_dir:
        output_dir = args.output_dir
    else:
        checkpoint_dir = model_config['checkpoint_dir']
        dataset_name = os.path.basename(os.path.dirname(train_path))
        flash_suffix = "flashattn2" if use_flash_attn else "standard"
        output_dir = os.path.join(checkpoint_dir, f"{dataset_name}_ablation_{config_name}_{flash_suffix}_dynamic_distributed")
    
    # åªåœ¨ä¸»è¿›ç¨‹åˆ›å»ºç›®å½•å’Œæ—¥å¿—æ–‡ä»¶
    training_log_path = None
    if is_main_process:
        try:
            os.makedirs(output_dir, exist_ok=True)
            print(f"è¾“å‡ºç›®å½•: {output_dir}")
            
            # åˆ›å»ºè®­ç»ƒæ—¥å¿—æ–‡ä»¶
            training_log_path = os.path.join(output_dir, "training_samples_log.txt")
            print(f"è®­ç»ƒæ—¥å¿—: {training_log_path}")
        except (OSError, IOError) as e:
            print(f"è­¦å‘Š: æ— æ³•åˆ›å»ºè¾“å‡ºç›®å½•: {e}")
    
    # ç­‰å¾…ä¸»è¿›ç¨‹åˆ›å»ºå®Œç›®å½•
    if world_size > 1:
        dist.barrier()
    
    # åŠ è½½æ¨¡å‹å’Œtokenizer
    model_path = model_config['path']
    if is_main_process:
        print(f"åŠ è½½æ¨¡å‹: {model_path}")
        if use_flash_attn:
            print("  ä½¿ç”¨ FlashAttention 2 å®ç°...")
    
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # âœ… å…ˆè·å– train_configï¼ˆåœ¨æ•°æ®åˆ†æä¹‹å‰éœ€è¦ï¼‰
    train_config = config.get('training', {})
    
    # ============================================================================
    # è®¡ç®—è®­ç»ƒé›†çš„æœ€å¤§è¾“å…¥é•¿åº¦ï¼ˆåœ¨ä¸»è¿›ç¨‹ä¸­ï¼‰
    # ============================================================================
    if is_main_process:
        print("\n" + "="*80)
        print("ğŸ“Š åˆ†æè®­ç»ƒæ•°æ®é•¿åº¦åˆ†å¸ƒ")
        print("="*80)
        
        # å¯¼å…¥promptæ„å»ºå‡½æ•°
        # âœ… ä½¿ç”¨ä¸“é—¨çš„é—®å· prompt æ„å»ºå‡½æ•°
        from data_loader_lovink_questionnaire import build_simple_training_prompt
        
        # é‡‡æ ·éƒ¨åˆ†æ•°æ®è¿›è¡Œåˆ†æï¼ˆé¿å…å¤ªæ…¢ï¼‰
        sample_size = min(100, len(train_samples))
        sampled_indices = random.sample(range(len(train_samples)), sample_size)
        
        lengths = []
        max_length_sample = None
        max_length = 0
        
        print(f"æ­£åœ¨åˆ†æ {sample_size} ä¸ªæ ·æœ¬...")
        for idx in sampled_indices:
            sample = train_samples[idx]
            
            # æ„å»ºprompt
            try:
                messages, target_answer = build_simple_training_prompt(
                    context=sample['context'],
                    next_question=sample['next_question'],
                    user_profile=sample.get('user_profile') if use_profile else None,
                    task_description=sample.get('task_description'),
                    history=sample.get('history', []) if use_history else [],
                    use_profile=use_profile,
                    use_history=use_history,
                    use_context=use_context,
                    tokenizer=tokenizer,
                    max_length=train_config.get('max_length', 4096),
                    min_target_tokens=64,
                    user_hash=sample.get('user_hash')
                )
                
                # ç”Ÿæˆå®Œæ•´æ–‡æœ¬
                full_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)
                generation_suffix = "<|im_start|>assistant\n"
                full_prompt = full_prompt.strip() + generation_suffix
                im_end_token = "<|im_end|>"
                full_text = full_prompt + target_answer + im_end_token
                
                # è®¡ç®—é•¿åº¦
                token_ids = tokenizer.encode(full_text, add_special_tokens=False)
                length = len(token_ids)
                lengths.append(length)
                
                # è®°å½•æœ€é•¿çš„æ ·æœ¬
                if length > max_length:
                    max_length = length
                    max_length_sample = {
                        'idx': idx,
                        'length': length,
                        'user_hash': sample.get('user_hash', 'unknown'),
                        'context_turns': len(sample.get('context', [])),
                        'history_items': len(sample.get('history', []))
                    }
            except Exception as e:
                print(f"  è­¦å‘Š: æ ·æœ¬ {idx} å¤„ç†å¤±è´¥: {e}")
                continue
        
        if lengths:
            import numpy as np
            lengths_array = np.array(lengths)
            
            print(f"\nè®­ç»ƒæ•°æ®é•¿åº¦ç»Ÿè®¡ï¼ˆåŸºäº {len(lengths)} ä¸ªæ ·æœ¬ï¼‰:")
            print(f"  æœ€å°é•¿åº¦: {lengths_array.min()}")
            print(f"  æœ€å¤§é•¿åº¦: {lengths_array.max()}")
            print(f"  å¹³å‡é•¿åº¦: {lengths_array.mean():.1f}")
            print(f"  ä¸­ä½æ•°é•¿åº¦: {np.median(lengths_array):.1f}")
            print(f"  æ ‡å‡†å·®: {lengths_array.std():.1f}")
            print(f"\né•¿åº¦åˆ†å¸ƒ:")
            print(f"  < 1024 tokens:  {(lengths_array < 1024).sum()} ({(lengths_array < 1024).sum()/len(lengths)*100:.1f}%)")
            print(f"  < 2048 tokens:  {(lengths_array < 2048).sum()} ({(lengths_array < 2048).sum()/len(lengths)*100:.1f}%)")
            print(f"  < 4096 tokens:  {(lengths_array < 4096).sum()} ({(lengths_array < 4096).sum()/len(lengths)*100:.1f}%)")
            print(f"  < 8192 tokens:  {(lengths_array < 8192).sum()} ({(lengths_array < 8192).sum()/len(lengths)*100:.1f}%)")
            print(f"  >= 8192 tokens: {(lengths_array >= 8192).sum()} ({(lengths_array >= 8192).sum()/len(lengths)*100:.1f}%)")
            
            if max_length_sample:
                print(f"\næœ€é•¿æ ·æœ¬ä¿¡æ¯:")
                print(f"  ç´¢å¼•: {max_length_sample['idx']}")
                print(f"  é•¿åº¦: {max_length_sample['length']} tokens")
                print(f"  ç”¨æˆ·å“ˆå¸Œ: {max_length_sample['user_hash']}")
                print(f"  ä¸Šä¸‹æ–‡è½®æ¬¡: {max_length_sample['context_turns']}")
                print(f"  å†å²æ¡ç›®æ•°: {max_length_sample['history_items']}")
            
            # æ ¹æ®æ•°æ®åˆ†å¸ƒç»™å‡ºé…ç½®å»ºè®®
            configured_max_length = train_config.get('max_length', 4096)
            percentile_95 = np.percentile(lengths_array, 95)
            print(f"\né…ç½®å»ºè®®:")
            print(f"  å½“å‰é…ç½®çš„ max_length: {configured_max_length}")
            print(f"  95åˆ†ä½æ•°é•¿åº¦: {percentile_95:.0f}")
            if percentile_95 > configured_max_length:
                print(f"  âš ï¸  è­¦å‘Š: 95%çš„æ•°æ®è¶…è¿‡é…ç½®çš„max_lengthï¼Œå¯èƒ½å¯¼è‡´å¤§é‡æˆªæ–­")
                print(f"  å»ºè®®è°ƒæ•´ max_length è‡³å°‘åˆ° {int(percentile_95)}")
            elif percentile_95 < configured_max_length * 0.7:
                print(f"  â„¹ï¸  æç¤º: 95%çš„æ•°æ®é•¿åº¦è¿œå°äºmax_lengthï¼Œå¯ä»¥è€ƒè™‘é™ä½ä»¥èŠ‚çœæ˜¾å­˜")
            else:
                print(f"  âœ“ max_length è®¾ç½®åˆç†")
        
        print("="*80 + "\n")
    
    # ç­‰å¾…ä¸»è¿›ç¨‹å®Œæˆåˆ†æ
    if world_size > 1:
        dist.barrier()
    
    # åŠ è½½æ¨¡å‹åˆ°æŒ‡å®šGPUï¼ˆä½¿ç”¨FlashAttention 2ï¼‰
    model_kwargs = {
        'torch_dtype': torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,
        'trust_remote_code': True,
    }
    
    # å¦‚æœæ”¯æŒä¸”æœªç¦ç”¨ï¼Œåˆ™ä½¿ç”¨FlashAttention 2
    if use_flash_attn:
        model_kwargs['attn_implementation'] = 'flash_attention_2'
    
    try:
        model = AutoModelForCausalLM.from_pretrained(model_path, **model_kwargs)
        if is_main_process:
            if use_flash_attn:
                print("âœ“ æ¨¡å‹å·²åŠ è½½ï¼ˆFlashAttention 2ï¼‰")
            else:
                print("âœ“ æ¨¡å‹å·²åŠ è½½ï¼ˆæ ‡å‡†Attentionï¼‰")
    except Exception as e:
        if is_main_process:
            print(f"åŠ è½½FlashAttention 2å¤±è´¥: {e}")
            print("   å›é€€åˆ°æ ‡å‡†attention...")
        # å›é€€åˆ°æ ‡å‡†attention
        model_kwargs.pop('attn_implementation', None)
        model = AutoModelForCausalLM.from_pretrained(model_path, **model_kwargs)
        use_flash_attn = False
    
    # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
    if hasattr(model, 'gradient_checkpointing_enable'):
        model.gradient_checkpointing_enable()
        if is_main_process:
            print("âœ“ æ¢¯åº¦æ£€æŸ¥ç‚¹å·²å¯ç”¨")
    
    # å°†æ¨¡å‹ç§»åˆ°å¯¹åº”çš„GPU
    model = model.to(local_rank)
    
    # åˆ›å»ºæ•°æ®é›†ï¼ˆä½¿ç”¨åŠ¨æ€Paddingç‰ˆæœ¬ï¼‰
    # train_config å·²åœ¨å‰é¢å®šä¹‰ï¼ˆæ•°æ®åˆ†æé˜¶æ®µï¼‰
    if is_main_process:
        print("åˆ›å»ºè®­ç»ƒæ•°æ®é›†ï¼ˆåŠ¨æ€Paddingæ¨¡å¼ï¼‰...")
    
    # âœ… æ ¹æ®å‘½ä»¤è¡Œå‚æ•°å†³å®šä½¿ç”¨å“ªç§ prompt é£æ ¼
    use_detailed_template = (args.prompt_style != 'simple')
    template_filename = args.template_filename if args.prompt_style == 'detailed' else None
    
    if is_main_process:
        print(f"Prompt é£æ ¼: {args.prompt_style}")
        if args.prompt_style == 'simple':
            print("   ä½¿ç”¨ç®€æ´æ ‡ç­¾æ ¼å¼ï¼ˆ[USER_PROFILE] [DIM_XXX=score] ...ï¼‰")
        elif args.prompt_style == 'detailed':
            if template_filename:
                print(f"   ä½¿ç”¨è¯¦ç»†æ¨¡æ¿: {template_filename} (æ ‡å‡† {{VAR_NAME}} æ ¼å¼)")
            else:
                print("   ä½¿ç”¨è¯¦ç»†æ¨¡æ¿ï¼ˆé»˜è®¤é¡ºåºæŸ¥æ‰¾ï¼‰")
        elif args.prompt_style == 'lovink':
            print("   ä½¿ç”¨ Lovink é£æ ¼æ¨¡æ¿")
    
    train_dataset = DynamicPaddingDataset(
        samples=train_samples,
        tokenizer=tokenizer,
        max_length=train_config.get('max_length', 4096),
        use_profile=use_profile,
        use_history=use_history,
        use_context=use_context,
        verbose=is_main_process,  # åªåœ¨ä¸»è¿›ç¨‹è¾“å‡ºè¯¦ç»†æ—¥å¿—
        use_detailed_template=use_detailed_template,
        template_filename=template_filename
    )
    
    val_dataset = None
    if val_samples:
        if is_main_process:
            print("åˆ›å»ºéªŒè¯æ•°æ®é›†ï¼ˆåŠ¨æ€Paddingæ¨¡å¼ï¼‰...")
        val_dataset = DynamicPaddingDataset(
            samples=val_samples,
            tokenizer=tokenizer,
            max_length=train_config.get('max_length', 4096),
            use_profile=use_profile,
            use_history=use_history,
            use_context=use_context,
            use_detailed_template=use_detailed_template,
            template_filename=template_filename
        )
    
    # æ•°æ®æ•´ç†å™¨ï¼ˆä½¿ç”¨åŠ¨æ€Paddingç‰ˆæœ¬ï¼‰
    def collate_fn(examples):
        return dynamic_padding_collate_fn(examples, tokenizer)
    
    # åœ¨ä¸»è¿›ç¨‹ä¸­æ‰“å°å‡ ä¸ªæ ·æœ¬ç¤ºä¾‹ï¼ˆç”¨äºè°ƒè¯•å’ŒéªŒè¯ï¼‰
    if is_main_process and training_log_path:
        print("\n" + "=" * 80)
        print("ğŸ“ æ ·æœ¬ç¤ºä¾‹ï¼ˆå‰5ä¸ªè®­ç»ƒæ ·æœ¬ï¼‰")
        print("=" * 80)
        
        # åŒæ—¶å†™å…¥æ—¥å¿—æ–‡ä»¶
        with open(training_log_path, 'w', encoding='utf-8') as log_file:
            log_file.write("=" * 80 + "\n")
            log_file.write(f"è®­ç»ƒé…ç½®: {config_name}\n")
            log_file.write(f"æ•°æ®é›†: {train_path}\n")
            log_file.write(f"æ€»æ ·æœ¬æ•°: {len(train_samples)}\n")
            log_file.write(f"Max Length: {train_config.get('max_length', 4096)}\n")
            log_file.write(f"FlashAttention 2: {'å¯ç”¨' if use_flash_attn else 'ç¦ç”¨'}\n")
            log_file.write("=" * 80 + "\n\n")
            
            num_samples_to_show = min(5, len(train_samples))
            for i in range(num_samples_to_show):
                sample = train_samples[i]
                
                # æ§åˆ¶å°è¾“å‡º
                print(f"\n--- æ ·æœ¬ {i+1} ---")
                
                # æ—¥å¿—æ–‡ä»¶è¾“å‡º
                log_file.write(f"\n{'=' * 80}\n")
                log_file.write(f"æ ·æœ¬ {i+1}\n")
                log_file.write(f"{'=' * 80}\n\n")
                
                # æ˜¾ç¤ºè§’è‰²æ˜ å°„çš„context
                context_info = f"Context ({len(sample['context'])}è½®):"
                print(context_info)
                log_file.write(context_info + "\n")
                
                for j, turn in enumerate(sample['context']):
                    role = turn['role']
                    content = turn['content']
                    role_desc = "user(å¯¹è¯è€…)" if role == "user" else "assistant(ç›®æ ‡ç”¨æˆ·)"
                    
                    # æ§åˆ¶å°åªæ˜¾ç¤ºå‰5è½®ï¼Œä¸”æˆªæ–­
                    if j < 5:
                        print(f"  {j+1}. {role_desc:25s}: {content[:60]}...")
                    
                    # æ—¥å¿—æ–‡ä»¶æ˜¾ç¤ºå®Œæ•´å†…å®¹
                    log_file.write(f"  {j+1}. {role_desc}:\n")
                    log_file.write(f"     {content}\n\n")
                
                if len(sample['context']) > 5:
                    print(f"  ... (è¿˜æœ‰ {len(sample['context']) - 5} è½®)")
                
                # æ˜¾ç¤ºè¦é¢„æµ‹çš„target
                target = sample['next_question']
                print(f"\nTarget (æ¨¡å‹è¦ç”Ÿæˆçš„):")
                print(f"  assistant(ç›®æ ‡ç”¨æˆ·): {target[:100]}...")
                
                log_file.write(f"\nTarget (æ¨¡å‹è¦ç”Ÿæˆçš„):\n")
                log_file.write(f"  assistant(ç›®æ ‡ç”¨æˆ·):\n")
                log_file.write(f"     {target}\n\n")
                
                # æ˜¾ç¤ºprofileä¿¡æ¯
                if sample.get('user_profile'):
                    profile = sample['user_profile']
                    print(f"\nProfile:")
                    log_file.write(f"Profile:\n")
                    
                    for key in ['name', 'age', 'gender', 'profession', 'residence']:
                        if key in profile:
                            info = f"  {key.capitalize()}: {profile[key]}"
                            print(info)
                            log_file.write(info + "\n")
                
                # ä½¿ç”¨datasetçš„__getitem__æ¥è·å–ç¼–ç åçš„ä¿¡æ¯
                try:
                    encoded_sample = train_dataset[i]
                    input_length = len(encoded_sample['input_ids'])
                    valid_labels = (encoded_sample['labels'] != -100).sum().item()
                    actual_length = encoded_sample.get('actual_length', input_length)
                    
                    encoding_info = [
                        f"\nç¼–ç ä¿¡æ¯:",
                        f"  è¾“å…¥é•¿åº¦: {input_length} tokens",
                        f"  å®é™…é•¿åº¦: {actual_length} tokens",
                        f"  æœ‰æ•ˆæ ‡ç­¾æ•°: {valid_labels} tokens",
                        f"  è®­ç»ƒæ¯”ä¾‹: {valid_labels/input_length:.2%}"
                    ]
                    
                    for line in encoding_info:
                        print(line)
                        log_file.write(line + "\n")
                    
                    # æ£€æŸ¥æ˜¯å¦è¢«æˆªæ–­
                    if hasattr(train_dataset, 'truncation_stats'):
                        stats = train_dataset.get_truncation_stats()
                        if stats['truncated_samples'] > 0:
                            truncation_info = f"  âš ï¸  å·²æœ‰ {stats['truncated_samples']} ä¸ªæ ·æœ¬è¢«æˆªæ–­"
                            print(truncation_info)
                            log_file.write(truncation_info + "\n")
                    
                except Exception as e:
                    error_msg = f"\nç¼–ç ä¿¡æ¯: æ— æ³•è·å– ({e})"
                    print(error_msg)
                    log_file.write(error_msg + "\n")
                
                log_file.write("\n")
        
        print(f"\nâœ“ æ ·æœ¬è¯¦æƒ…å·²ä¿å­˜åˆ°: {training_log_path}")
        print("=" * 80)
    
    # è®¡ç®—è®­ç»ƒæ­¥æ•°
    steps_per_epoch = len(train_dataset) // (world_size * train_config.get('batch_size', 2) * train_config.get('gradient_accumulation_steps', 8))
    eval_steps_value = max(1, steps_per_epoch // 2) if val_dataset else None
    save_steps_value = train_config.get('save_steps', 500)
    
    if val_dataset and eval_steps_value and save_steps_value % eval_steps_value != 0:
        save_steps_value = ((save_steps_value + eval_steps_value - 1) // eval_steps_value) * eval_steps_value
        if is_main_process:
            print(f"è°ƒæ•´ save_steps ä¸º {save_steps_value}ï¼ˆeval_steps={eval_steps_value} çš„æ•´æ•°å€ï¼‰")
    
    # è®­ç»ƒå‚æ•°ï¼ˆåˆ†å¸ƒå¼ + FlashAttention 2 + åŠ¨æ€Paddingï¼‰
    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=args.max_epochs,
        per_device_train_batch_size=train_config.get('batch_size', 2),
        per_device_eval_batch_size=train_config.get('eval_batch_size', 2),
        gradient_accumulation_steps=train_config.get('gradient_accumulation_steps', 8),
        learning_rate=train_config.get('learning_rate', 1e-5),
        weight_decay=train_config.get('weight_decay', 0.01),
        warmup_steps=train_config.get('warmup_steps', 100),
        logging_steps=train_config.get('logging_steps', 10),
        save_steps=save_steps_value,
        eval_steps=eval_steps_value,
        eval_strategy="steps" if val_dataset else "no",
        save_total_limit=train_config.get('save_total_limit', 3),
        load_best_model_at_end=True if val_dataset else False,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        fp16=False,
        bf16=True,  # FlashAttention 2 ä¸ BF16 é…åˆæ•ˆæœæ›´å¥½
        dataloader_pin_memory=False,
        gradient_checkpointing=True,
        optim="adamw_torch",
        max_grad_norm=0.5,
        report_to="wandb" if args.wandb_project else "none",
        # åˆ†å¸ƒå¼è®­ç»ƒå…³é”®å‚æ•°
        local_rank=local_rank,
        ddp_find_unused_parameters=False,
        ddp_backend="nccl",
        dataloader_num_workers=2,
        save_on_each_node=False,
        logging_first_step=True,
        # DeepSpeedé…ç½®ï¼ˆå¯é€‰ï¼‰
        deepspeed=args.deepspeed,
    )
    
    # åˆ›å»ºæ—©åœå›è°ƒ
    callbacks = []
    if val_dataset:
        early_stopping = EarlyStoppingCallback(
            early_stopping_patience=args.early_stopping_patience,
            early_stopping_threshold=args.early_stopping_threshold
        )
        callbacks.append(early_stopping)
    
    # åˆ›å»ºè‡ªå®šä¹‰Trainerï¼ˆå¸¦æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥å’Œè¯¦ç»†æ—¥å¿—ï¼‰
    class CustomTrainer(Trainer):
        def __init__(self, *args, **kwargs):
            super().__init__(*args, **kwargs)
            # åˆ›å»ºè®­ç»ƒè¿›åº¦æ—¥å¿—æ–‡ä»¶
            if is_main_process:
                self.progress_log_file = os.path.join(output_dir, "training_logs", "training_progress.txt")
                os.makedirs(os.path.dirname(self.progress_log_file), exist_ok=True)
                with open(self.progress_log_file, 'w', encoding='utf-8') as f:
                    f.write("=" * 100 + "\n")
                    f.write("è®­ç»ƒè¿›åº¦æ—¥å¿—\n")
                    f.write("=" * 100 + "\n\n")
            else:
                self.progress_log_file = None
        
        def log(self, logs: Dict[str, float], start_time: Optional[float] = None, **kwargs) -> None:
            """
            é‡å†™logæ–¹æ³•ï¼Œä¿®æ­£æ¢¯åº¦ç´¯ç§¯å¯¼è‡´çš„train_lossæ˜¾ç¤ºé—®é¢˜ï¼Œå¹¶æ·»åŠ è¯¦ç»†æ—¥å¿—
            """
            if "loss" in logs:
                # ä¿®æ­£train_lossï¼šé™¤ä»¥æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
                logs["loss"] = logs["loss"] / self.args.gradient_accumulation_steps
            
            # è®°å½•è¯¦ç»†æ—¥å¿—ï¼ˆå‰50æ­¥å’Œæ¯100æ­¥ï¼‰
            if is_main_process and self.progress_log_file:
                step = self.state.global_step
                if step <= 50 or step % 100 == 0:
                    with open(self.progress_log_file, 'a', encoding='utf-8') as f:
                        f.write(f"\n{'=' * 80}\n")
                        f.write(f"Step {step} | Epoch {self.state.epoch:.2f}\n")
                        f.write(f"{'=' * 80}\n")
                        for key, value in logs.items():
                            if isinstance(value, (int, float)):
                                f.write(f"  {key}: {value:.6f}\n")
                            else:
                                f.write(f"  {key}: {value}\n")
                        f.write("\n")
            
            # è°ƒç”¨çˆ¶ç±»çš„logæ–¹æ³•ï¼Œä¼ é€’æ‰€æœ‰é¢å¤–å‚æ•°
            if start_time is not None:
                super().log(logs, start_time, **kwargs)
            else:
                super().log(logs, **kwargs)
        
        def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
            """è®¡ç®—æŸå¤±ï¼ˆå¸¦æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥å’Œbatchæ—¥å¿—ï¼‰"""
            # è®°å½•å‰3ä¸ªbatchçš„è¯¦ç»†ä¿¡æ¯
            if is_main_process and self.state.global_step <= 3 and self.progress_log_file:
                with open(self.progress_log_file, 'a', encoding='utf-8') as f:
                    f.write(f"\n{'=' * 100}\n")
                    f.write(f"Batch è¯¦ç»†ä¿¡æ¯ - Step {self.state.global_step}\n")
                    f.write(f"{'=' * 100}\n")
                    f.write(f"Batch size: {inputs['input_ids'].shape[0]}\n")
                    f.write(f"Sequence lengths: {inputs['input_ids'].shape[1]}\n")
                    
                    # æ˜¾ç¤ºç¬¬ä¸€ä¸ªæ ·æœ¬çš„ä¿¡æ¯
                    if inputs['input_ids'].shape[0] > 0:
                        first_input_ids = inputs['input_ids'][0]
                        first_labels = inputs['labels'][0]
                        first_attention_mask = inputs['attention_mask'][0]
                        
                        f.write(f"\nç¬¬ä¸€ä¸ªæ ·æœ¬:\n")
                        f.write(f"  Input length: {len(first_input_ids)}\n")
                        f.write(f"  Valid labels: {(first_labels != -100).sum().item()}\n")
                        f.write(f"  Attention tokens: {first_attention_mask.sum().item()}\n")
                        
                        # è§£ç æ›´å¤štokensä»¥æŸ¥çœ‹å®é™…å†…å®¹
                        try:
                            seq_len = len(first_input_ids)
                            
                            f.write(f"\n  è§£ç çš„è¾“å…¥ (å‰500 tokens):\n")
                            f.write(f"  {tokenizer.decode(first_input_ids[:500], skip_special_tokens=False)}\n")
                            f.write(f"  ...\n")
                            
                            # å¦‚æœå¤Ÿé•¿ï¼Œæ‰“å°ä¸­é—´éƒ¨åˆ†
                            if seq_len > 1000:
                                f.write(f"\n  è§£ç çš„è¾“å…¥ (ç¬¬500-1000 tokens):\n")
                                f.write(f"  {tokenizer.decode(first_input_ids[500:1000], skip_special_tokens=False)}\n")
                                f.write(f"  ...\n")
                            
                            f.write(f"\n  è§£ç çš„è¾“å…¥ (å500 tokens):\n")
                            f.write(f"  {tokenizer.decode(first_input_ids[-500:], skip_special_tokens=False)}\n\n")
                            
                            # è§£ç æ ‡ç­¾ï¼ˆå®Œæ•´æ˜¾ç¤ºï¼Œä¸æˆªæ–­ï¼‰
                            valid_label_mask = first_labels != -100
                            if valid_label_mask.any():
                                valid_labels = first_labels[valid_label_mask]
                                f.write(f"  è§£ç çš„æ ‡ç­¾ (å®Œæ•´æœ‰æ•ˆéƒ¨åˆ†ï¼Œå…±{len(valid_labels)}ä¸ªtokens):\n")
                                f.write(f"  {tokenizer.decode(valid_labels, skip_special_tokens=False)}\n")
                        except Exception as e:
                            f.write(f"  è§£ç å¤±è´¥: {e}\n")
                    
                    f.write("\n")
            
            # ç§»é™¤actual_lengthå­—æ®µï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            actual_lengths = inputs.pop('actual_length', None)
            
            outputs = model(**inputs)
            logits = outputs.get("logits")
            labels = inputs.get("labels")
            
            # æ£€æŸ¥å¹¶æ¸…ç†logitsä¸­çš„nan/inf
            if logits is not None and logits.numel() > 0:
                # å¿«é€Ÿé‡‡æ ·æ£€æŸ¥
                check_size = min(1000, logits.numel() // 2)
                if logits.numel() > check_size * 2:
                    head_values = logits.view(-1)[:check_size]
                    tail_values = logits.view(-1)[-check_size:]
                    has_issue = torch.isnan(head_values).any() or torch.isnan(tail_values).any() or \
                                torch.isinf(head_values).any() or torch.isinf(tail_values).any()
                else:
                    has_issue = torch.isnan(logits).any() or torch.isinf(logits).any()
                
                if has_issue:
                    if rank == 0:
                        print(f"è­¦å‘Š: [GPU {rank}] Step {self.state.global_step} æ£€æµ‹åˆ°nan/infï¼Œæ­£åœ¨æ¸…ç†...")
                    logits = torch.where(
                        torch.isnan(logits) | torch.isinf(logits),
                        torch.tensor(0.0, device=logits.device, dtype=logits.dtype),
                        logits
                    )
                    logits = torch.clamp(logits, min=-50.0, max=50.0)
            
            # è®¡ç®—æŸå¤±
            if hasattr(outputs, 'loss') and outputs.loss is not None:
                loss = outputs.loss
            elif labels is not None:
                valid_labels_count = (labels != -100).sum().item()
                
                if valid_labels_count == 0:
                    if rank == 0:
                        print(f"è­¦å‘Š: [GPU {rank}] Step {self.state.global_step} æ²¡æœ‰æœ‰æ•ˆçš„labels")
                    loss = torch.tensor(2.0, device=logits.device, requires_grad=True)
                else:
                    loss_fct = nn.CrossEntropyLoss(ignore_index=-100, reduction='mean')
                    shift_logits = logits[..., :-1, :].contiguous()
                    shift_labels = labels[..., 1:].contiguous()
                    
                    loss = loss_fct(
                        shift_logits.view(-1, shift_logits.size(-1)),
                        shift_labels.view(-1)
                    )
            else:
                loss = torch.tensor(2.0, device=logits.device, requires_grad=True)
            
            # æ£€æŸ¥æŸå¤±å€¼
            if loss is not None and torch.is_tensor(loss):
                if loss.dim() > 0:
                    loss = loss.mean()
                
                if torch.isnan(loss) or torch.isinf(loss):
                    if rank == 0:
                        print(f"è­¦å‘Š: [GPU {rank}] Step {self.state.global_step} lossä¸ºnan/inf")
                    loss = torch.tensor(2.0, device=logits.device, requires_grad=True)
                elif loss.item() > 1e6:
                    if rank == 0:
                        print(f"è­¦å‘Š: [GPU {rank}] Step {self.state.global_step} lossè¿‡å¤§")
                    loss = torch.clamp(loss, max=100.0)
            
            # å®šæœŸæ¸…ç†CUDAç¼“å­˜
            if self.state.global_step % 10 == 0:
                torch.cuda.empty_cache()
            
            if return_outputs:
                return loss, outputs
            return loss
    
    # åˆ›å»º Trainer
    trainer = CustomTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        data_collator=collate_fn,  # ä½¿ç”¨åŠ¨æ€paddingçš„collate_fn
        processing_class=tokenizer,
        callbacks=callbacks,
    )
    
    # åˆ›å»ºè®­ç»ƒæ—¥å¿—æ–‡ä»¶ï¼ˆä¸»è¿›ç¨‹ï¼‰
    if is_main_process:
        log_dir = os.path.join(output_dir, "training_logs")
        os.makedirs(log_dir, exist_ok=True)
        training_log_file = os.path.join(log_dir, "detailed_training_log.txt")
        
        print(f"\nğŸ“ åˆ›å»ºè¯¦ç»†è®­ç»ƒæ—¥å¿—: {training_log_file}")
        
        with open(training_log_file, 'w', encoding='utf-8') as f:
            f.write("=" * 100 + "\n")
            f.write("è¯¦ç»†è®­ç»ƒæ—¥å¿— - å‰3ä¸ªè®­ç»ƒæ ·æœ¬\n")
            f.write("=" * 100 + "\n\n")
            
            # è®°å½•å‰3ä¸ªè®­ç»ƒæ ·æœ¬çš„è¯¦ç»†ä¿¡æ¯
            num_samples_to_log = min(3, len(train_dataset))
            for idx in range(num_samples_to_log):
                raw_sample = train_samples[idx]
                encoded_sample = train_dataset[idx]
                
                f.write(f"\n{'=' * 100}\n")
                f.write(f"è®­ç»ƒæ ·æœ¬ #{idx + 1}\n")
                f.write(f"{'=' * 100}\n\n")
                
                # 1. åŸå§‹æ ·æœ¬ä¿¡æ¯
                f.write("ã€åŸå§‹æ ·æœ¬ä¿¡æ¯ã€‘\n")
                f.write(f"User Hash: {raw_sample.get('user_hash', 'N/A')}\n")
                if raw_sample.get('user_profile'):
                    profile = raw_sample['user_profile']
                    f.write(f"User Profile: {profile.get('name', 'N/A')} (age: {profile.get('age', 'N/A')})\n")
                f.write("\n")
                
                # 2. å¯¹è¯ä¸Šä¸‹æ–‡
                f.write("ã€å¯¹è¯ä¸Šä¸‹æ–‡ Contextã€‘\n")
                context = raw_sample.get('context', [])
                for turn_idx, turn in enumerate(context[-5:], 1):  # åªæ˜¾ç¤ºæœ€å5è½®
                    role = turn.get('role', 'unknown')
                    content = turn.get('content', '')
                    f.write(f"  è½®æ¬¡{turn_idx} [{role}]: {content}\n")
                if len(context) > 5:
                    f.write(f"  ... (è¿˜æœ‰ {len(context) - 5} è½®å¯¹è¯)\n")
                f.write("\n")
                
                # 3. ç›®æ ‡è¾“å‡ºï¼ˆæ¨¡å‹è¦å­¦ä¹ ç”Ÿæˆçš„å†…å®¹ï¼‰
                f.write("ã€ç›®æ ‡è¾“å‡º Next Questionã€‘\n")
                next_question = raw_sample.get('next_question', '')
                f.write(f"{next_question}\n\n")
                
                # 4. å†å²ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
                if use_history and raw_sample.get('history'):
                    f.write("ã€å†å²ä¿¡æ¯ Historyã€‘\n")
                    history = raw_sample['history']
                    for hist_idx, hist_item in enumerate(history[:3], 1):  # åªæ˜¾ç¤ºå‰3æ¡
                        f.write(f"  å†å²{hist_idx}: {hist_item[:100]}...\n")
                    if len(history) > 3:
                        f.write(f"  ... (è¿˜æœ‰ {len(history) - 3} æ¡å†å²)\n")
                    f.write("\n")
                
                # 5. ç¼–ç åçš„ä¿¡æ¯
                f.write("ã€ç¼–ç åçš„æ•°æ®ã€‘\n")
                input_ids = encoded_sample['input_ids']
                labels = encoded_sample['labels']
                attention_mask = encoded_sample['attention_mask']
                
                f.write(f"Input IDs é•¿åº¦: {len(input_ids)}\n")
                f.write(f"Attention Mask é•¿åº¦: {len(attention_mask)}\n")
                f.write(f"Labels é•¿åº¦: {len(labels)}\n")
                
                valid_labels = (labels != -100).sum().item()
                f.write(f"æœ‰æ•ˆæ ‡ç­¾æ•°: {valid_labels}\n")
                f.write(f"è®­ç»ƒæ¯”ä¾‹: {valid_labels / len(labels):.2%}\n")
                
                # è§£ç æŸ¥çœ‹å®é™…çš„æ–‡æœ¬ï¼ˆæ›´è¯¦ç»†çš„æ‰“å°ï¼‰
                total_length = len(input_ids)
                
                # å¦‚æœåºåˆ—ä¸å¤ªé•¿ï¼ˆ< 6000 tokensï¼‰ï¼Œç›´æ¥æ‰“å°å®Œæ•´å†…å®¹
                if total_length <= 6000:
                    f.write("\nã€å®Œæ•´çš„è¾“å…¥æ–‡æœ¬ã€‘\n")
                    f.write("-" * 100 + "\n")
                    decoded_full = tokenizer.decode(input_ids, skip_special_tokens=False)
                    f.write(decoded_full + "\n")
                    f.write("-" * 100 + "\n\n")
                    f.write(f"æ€»åºåˆ—é•¿åº¦: {total_length} tokens (å·²æ‰“å°å®Œæ•´å†…å®¹)\n\n")
                else:
                    # åºåˆ—å¤ªé•¿ï¼Œåˆ†æ®µæ‰“å°
                    f.write(f"\nã€åºåˆ—å¤ªé•¿ ({total_length} tokens)ï¼Œåˆ†æ®µæ‰“å°ã€‘\n\n")
                    
                    # æ‰“å°å‰2000ä¸ªtokens
                    f.write("ã€ç¬¬1-2000 tokensã€‘\n")
                    f.write("-" * 100 + "\n")
                    decoded_input_start = tokenizer.decode(input_ids[:2000], skip_special_tokens=False)
                    f.write(decoded_input_start + "\n")
                    f.write("-" * 100 + "\n\n")
                    
                    # æ‰“å°ä¸­é—´éƒ¨åˆ†ï¼ˆç¬¬2000-4000ä¸ªtokensï¼‰
                    f.write("ã€ç¬¬2001-4000 tokensã€‘\n")
                    f.write("-" * 100 + "\n")
                    decoded_input_middle = tokenizer.decode(input_ids[2000:4000], skip_special_tokens=False)
                    f.write(decoded_input_middle + "\n")
                    f.write("-" * 100 + "\n\n")
                    
                    # å¦‚æœè¿˜æœ‰æ›´å¤šï¼Œæ‰“å°ç¬¬4000-6000
                    if total_length > 6000:
                        f.write("ã€ç¬¬4001-6000 tokensã€‘\n")
                        f.write("-" * 100 + "\n")
                        decoded_input_middle2 = tokenizer.decode(input_ids[4000:6000], skip_special_tokens=False)
                        f.write(decoded_input_middle2 + "\n")
                        f.write("-" * 100 + "\n\n")
                    
                    # æ‰“å°å2000ä¸ªtokens
                    f.write("ã€å2000 tokensã€‘\n")
                    f.write("-" * 100 + "\n")
                    decoded_input_end = tokenizer.decode(input_ids[-2000:], skip_special_tokens=False)
                    f.write(decoded_input_end + "\n")
                    f.write("-" * 100 + "\n\n")
                    
                    f.write(f"æ€»åºåˆ—é•¿åº¦: {total_length} tokens\n\n")
                
                # è§£ç æ ‡ç­¾ï¼ˆåªæ˜¾ç¤ºæœ‰æ•ˆçš„éƒ¨åˆ†ï¼‰
                valid_label_indices = (labels != -100).nonzero(as_tuple=True)[0]
                if len(valid_label_indices) > 0:
                    f.write("ã€è§£ç åçš„æ ‡ç­¾æ–‡æœ¬ (æ¨¡å‹è¦å­¦ä¹ ç”Ÿæˆçš„éƒ¨åˆ†)ã€‘\n")
                    f.write("-" * 100 + "\n")
                    valid_labels_ids = labels[valid_label_indices]
                    decoded_labels = tokenizer.decode(valid_labels_ids, skip_special_tokens=False)
                    f.write(decoded_labels + "\n")
                    f.write("-" * 100 + "\n\n")
                
                f.write("\n")
            
            f.write("=" * 100 + "\n")
            f.write("è®­ç»ƒæ ·æœ¬æ—¥å¿—è®°å½•å®Œæˆ\n")
            f.write("=" * 100 + "\n")
        
        print(f"âœ“ è®­ç»ƒæ ·æœ¬æ—¥å¿—å·²ä¿å­˜åˆ°: {training_log_file}\n")
        
        # åœ¨æ§åˆ¶å°æ˜¾ç¤ºç¬¬ä¸€ä¸ªæ ·æœ¬çš„ç®€è¦ä¿¡æ¯
        print("=" * 80)
        print("ğŸ“‹ ç¬¬ä¸€ä¸ªè®­ç»ƒæ ·æœ¬é¢„è§ˆ")
        print("=" * 80)
        
        first_sample = train_samples[0]
        print(f"User Hash: {first_sample.get('user_hash', 'N/A')}")
        
        context = first_sample.get('context', [])
        if context:
            print(f"\nContext æœ€åä¸€è½®:")
            last_turn = context[-1]
            print(f"  [{last_turn.get('role', 'unknown')}]: {last_turn.get('content', '')[:150]}...")
        
        next_question = first_sample.get('next_question', '')
        print(f"\nTarget (è¦å­¦ä¹ ç”Ÿæˆçš„):")
        print(f"  {next_question[:150]}...")
        
        first_encoded = train_dataset[0]
        print(f"\nç¼–ç ä¿¡æ¯:")
        print(f"  Input length: {len(first_encoded['input_ids'])} tokens")
        print(f"  Valid labels: {(first_encoded['labels'] != -100).sum().item()} tokens")
        print(f"  è®­ç»ƒæ¯”ä¾‹: {(first_encoded['labels'] != -100).sum().item() / len(first_encoded['labels']):.2%}")
        
        print("=" * 80 + "\n")
    
    # å¼€å§‹è®­ç»ƒ
    if is_main_process:
        print("=" * 80)
        print("å¼€å§‹åˆ†å¸ƒå¼è®­ç»ƒï¼ˆFlashAttention 2 + åŠ¨æ€Paddingï¼‰")
        print("=" * 80)
        print(f"æ€»æ ·æœ¬æ•°: {len(train_dataset)}")
        print(f"æ¯ä¸ªGPUå¤„ç†çº¦: {len(train_dataset) // world_size} ä¸ªæ ·æœ¬")
        effective_batch = train_config.get('batch_size', 2) * train_config.get('gradient_accumulation_steps', 8) * world_size
        print(f"æœ‰æ•ˆ batch size: {effective_batch}")
        print(f"é¢„è®¡æ¯ä¸ªepochæ­¥æ•°: {steps_per_epoch}")
        print(f"Max Length: {train_config.get('max_length', 4096)} (åŠ¨æ€padding)")
        print(f"Attention: {'FlashAttention 2' if use_flash_attn else 'æ ‡å‡†Attention'}")
        if args.wandb_project:
            print(f"W&B ç›‘æ§: é¡¹ç›®={args.wandb_project}, è¿è¡Œ={args.wandb_run_name or 'auto'}")
        
        # è¾“å‡ºåˆå§‹æˆªæ–­ç»Ÿè®¡ï¼ˆè®­ç»ƒå‰ï¼‰
        if hasattr(train_dataset, 'get_truncation_stats'):
            stats = train_dataset.get_truncation_stats()
            print(f"\næ•°æ®é¢„å¤„ç†æˆªæ–­ç»Ÿè®¡:")
            print(f"  å·²å¤„ç†æ ·æœ¬: {stats['total_samples']}")
            print(f"  è¢«æˆªæ–­æ ·æœ¬: {stats['truncated_samples']}")
            if stats['total_samples'] > 0:
                print(f"  æˆªæ–­ç‡: {stats['truncation_rate']:.2%}")
                if stats['truncated_samples'] > 0:
                    print(f"  å¹³å‡æˆªæ–­è½®æ¬¡: {stats['avg_truncated_turns']:.2f}")
        
        print("=" * 80)
    
    trainer.train()
    
    # è®­ç»ƒå®Œæˆï¼Œè¾“å‡ºæ—¥å¿—æ±‡æ€»
    if is_main_process:
        print("\n" + "=" * 80)
        print("ğŸ“Š è®­ç»ƒæ—¥å¿—æ±‡æ€»")
        print("=" * 80)
        
        log_dir = os.path.join(output_dir, "training_logs")
        if os.path.exists(log_dir):
            print(f"è¯¦ç»†æ—¥å¿—æ–‡ä»¶:")
            for log_file_name in os.listdir(log_dir):
                log_path = os.path.join(log_dir, log_file_name)
                file_size = os.path.getsize(log_path) / 1024  # KB
                print(f"  - {log_path} ({file_size:.1f} KB)")
        print("=" * 80 + "\n")
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹ï¼ˆåªåœ¨ä¸»è¿›ç¨‹ä¿å­˜ï¼‰
    if is_main_process:
        print(f"ä¿å­˜æœ€ç»ˆæ¨¡å‹åˆ° {output_dir}")
        try:
            trainer.save_model()
            tokenizer.save_pretrained(output_dir)
            print("âœ“ æ¨¡å‹ä¿å­˜æˆåŠŸ")
            
            # ä¿å­˜è®­ç»ƒé…ç½®ä¿¡æ¯
            config_info = {
                'flash_attention_2': use_flash_attn,
                'dynamic_padding': True,
                'gradient_checkpointing': True,
                'ablation_config': args.ablation_config,
                'config_name': config_name
            }
            with open(os.path.join(output_dir, 'training_config.json'), 'w', encoding='utf-8') as f:
                json.dump(config_info, f, indent=2, ensure_ascii=False)
            print("âœ“ è®­ç»ƒé…ç½®å·²ä¿å­˜")
            
        except Exception as e:
            print(f"è­¦å‘Š: ä¿å­˜æ¨¡å‹æ—¶å‡ºé”™: {e}")
        
        # è¾“å‡ºæˆªæ–­ç»Ÿè®¡
        if hasattr(train_dataset, 'get_truncation_stats'):
            stats = train_dataset.get_truncation_stats()
            print("\n" + "="*80)
            print(" è®­ç»ƒæ•°æ®æˆªæ–­ç»Ÿè®¡:")
            print(f"  æ€»æ ·æœ¬æ•°: {stats['total_samples']}")
            print(f"  è¢«æˆªæ–­æ ·æœ¬æ•°: {stats['truncated_samples']}")
            print(f"  æˆªæ–­ç‡: {stats['truncation_rate']:.2%}")
            print(f"  å¹³å‡æˆªæ–­è½®æ¬¡: {stats['avg_truncated_turns']:.2f}")
            print("="*80)
            
            # å°†æˆªæ–­ç»Ÿè®¡å†™å…¥æ—¥å¿—æ–‡ä»¶
            if training_log_path:
                try:
                    with open(training_log_path, 'a', encoding='utf-8') as log_file:
                        log_file.write("\n" + "="*80 + "\n")
                        log_file.write("ğŸ“Š æœ€ç»ˆè®­ç»ƒæ•°æ®æˆªæ–­ç»Ÿè®¡\n")
                        log_file.write("="*80 + "\n")
                        log_file.write(f"æ€»æ ·æœ¬æ•°: {stats['total_samples']}\n")
                        log_file.write(f"è¢«æˆªæ–­æ ·æœ¬æ•°: {stats['truncated_samples']}\n")
                        log_file.write(f"æˆªæ–­ç‡: {stats['truncation_rate']:.2%}\n")
                        log_file.write(f"å¹³å‡æˆªæ–­è½®æ¬¡: {stats['avg_truncated_turns']:.2f}\n")
                        log_file.write(f"FlashAttention 2: {'å¯ç”¨' if use_flash_attn else 'ç¦ç”¨'}\n")
                        log_file.write("="*80 + "\n")
                    print(f"âœ“ æˆªæ–­ç»Ÿè®¡å·²è¿½åŠ åˆ°: {training_log_path}")
                except Exception as e:
                    print(f"è­¦å‘Š: æ— æ³•å†™å…¥æˆªæ–­ç»Ÿè®¡åˆ°æ—¥å¿—æ–‡ä»¶: {e}")
    
    # ç­‰å¾…æ‰€æœ‰è¿›ç¨‹å®Œæˆ
    if world_size > 1:
        dist.barrier()
    
    if is_main_process:
        print(f"\n è®­ç»ƒå®Œæˆï¼æ¨¡å‹ä¿å­˜åœ¨: {output_dir}")
        if use_flash_attn:
            print(" ä½¿ç”¨äº† FlashAttention 2 åŠ é€Ÿè®­ç»ƒ")
    
    # æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ
    cleanup_distributed()


if __name__ == '__main__':
    main()
