"""
Emoji æ£€æµ‹å’Œè¿‡æ»¤æ¨¡å—
ç”¨äºè®­ç»ƒæ—¶è¿‡æ»¤åŒ…å« emoji çš„æ ·æœ¬ï¼Œä»¥åŠæ¨ç†æ—¶è·å– emoji token IDs
"""
import re
import unicodedata
from typing import List, Set, Optional, TYPE_CHECKING

if TYPE_CHECKING:
    from transformers import PreTrainedTokenizer


def contains_emoji(text: str) -> bool:
    """
    æ£€æµ‹æ–‡æœ¬ä¸­æ˜¯å¦åŒ…å« emoji è¡¨æƒ…ç¬¦å·
    
    Args:
        text: è¦æ£€æµ‹çš„æ–‡æœ¬
        
    Returns:
        bool: å¦‚æœåŒ…å«emojiè¿”å›Trueï¼Œå¦åˆ™è¿”å›False
    """
    if not text:
        return False
    
    # Emoji Unicode èŒƒå›´ï¼ˆæ¶µç›–å¤§å¤šæ•°å¸¸è§ emojiï¼‰
    emoji_pattern = re.compile(
        "["
        "\U0001F600-\U0001F64F"  # è¡¨æƒ…ç¬¦å·
        "\U0001F300-\U0001F5FF"  # ç¬¦å·å’Œå›¾æ ‡
        "\U0001F680-\U0001F6FF"  # äº¤é€šå’Œåœ°å›¾ç¬¦å·
        "\U0001F1E0-\U0001F1FF"  # å›½æ——ï¼ˆiOSï¼‰
        "\U0001F900-\U0001F9FF"  # è¡¥å……ç¬¦å·å’Œå›¾æ ‡
        "\U0001FA00-\U0001FA6F"  # æ‰©å±•A
        "\U0001FA70-\U0001FAFF"  # æ‰©å±•B
        "\U00002600-\U000026FF"  # æ‚é¡¹ç¬¦å·ï¼ˆåŒ…å«å¸¸è§ç¬¦å·å¦‚â¤ï¸â­ï¼‰
        "\U00002700-\U000027BF"  # è£…é¥°ç¬¦å·ï¼ˆåŒ…å«âœ¨âœ…âœ”ï¸ç­‰ï¼‰
        "\U0000FE00-\U0000FE0F"  # å˜ä½“é€‰æ‹©å™¨ï¼ˆemojiå˜ä½“ï¼‰
        "\U0001F004-\U0001F0CF"  # éº»å°†å’Œæ‰‘å…‹ç‰Œ
        "\U0001F170-\U0001F251"  # å°é—­å­—ç¬¦ï¼ˆè¡€å‹ã€æŒ‰é’®ç­‰ï¼‰
        "]+",
        flags=re.UNICODE
    )
    
    # æ£€æµ‹æ˜¯å¦åŒ¹é… emoji æ¨¡å¼
    return bool(emoji_pattern.search(text))


def filter_samples_with_emoji(samples: List[dict], target_key: str = 'continuation') -> tuple:
    """
    è¿‡æ»¤åŒ…å« emoji çš„è®­ç»ƒæ ·æœ¬
    
    Args:
        samples: è®­ç»ƒæ ·æœ¬åˆ—è¡¨
        target_key: è¦æ£€æµ‹çš„å­—æ®µåï¼ˆé»˜è®¤ä¸º 'continuation'ï¼Œä¹Ÿå¯ä»¥æ˜¯ 'next_question' ç­‰ï¼‰
        
    Returns:
        tuple: (è¿‡æ»¤åçš„æ ·æœ¬åˆ—è¡¨, ç»Ÿè®¡ä¿¡æ¯å­—å…¸)
    """
    filtered_samples = []
    emoji_count = 0
    total_count = len(samples)
    
    for sample in samples:
        # è·å–ç›®æ ‡æ–‡æœ¬
        target_text = sample.get(target_key, '')
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ° target_keyï¼Œå°è¯•å…¶ä»–å¯èƒ½çš„å­—æ®µ
        if not target_text and target_key == 'continuation':
            target_text = sample.get('next_question', '')
        
        # æ£€æµ‹æ˜¯å¦åŒ…å« emoji
        if contains_emoji(target_text):
            emoji_count += 1
            continue  # è·³è¿‡åŒ…å« emoji çš„æ ·æœ¬
        
        filtered_samples.append(sample)
    
    # ç»Ÿè®¡ä¿¡æ¯
    stats = {
        'total_samples': total_count,
        'emoji_samples': emoji_count,
        'filtered_samples': len(filtered_samples),
        'emoji_ratio': emoji_count / total_count if total_count > 0 else 0.0,
        'kept_ratio': len(filtered_samples) / total_count if total_count > 0 else 0.0
    }
    
    return filtered_samples, stats


def get_emoji_token_ids(tokenizer) -> Set[int]:
    """
    è·å– tokenizer ä¸­æ‰€æœ‰ emoji ç›¸å…³çš„ token IDs
    ç”¨äºæ¨ç†æ—¶çš„ logit bias
    
    Args:
        tokenizer: Hugging Face tokenizer (PreTrainedTokenizer)
        
    Returns:
        Set[int]: emoji token IDs çš„é›†åˆ
    """
    emoji_token_ids = set()
    
    # å¸¸è§çš„ emoji åˆ—è¡¨ï¼ˆå¯ä»¥æ ¹æ®éœ€è¦æ‰©å±•ï¼‰
    common_emojis = [
        # è¡¨æƒ…ç¬¦å·
        'ğŸ˜€', 'ğŸ˜', 'ğŸ˜‚', 'ğŸ¤£', 'ğŸ˜ƒ', 'ğŸ˜„', 'ğŸ˜…', 'ğŸ˜†', 'ğŸ˜‰', 'ğŸ˜Š',
        'ğŸ˜‹', 'ğŸ˜', 'ğŸ˜', 'ğŸ˜˜', 'ğŸ¥°', 'ğŸ˜—', 'ğŸ˜™', 'ğŸ˜š', 'ğŸ™‚', 'ğŸ¤—',
        'ğŸ¤©', 'ğŸ¤”', 'ğŸ¤¨', 'ğŸ˜', 'ğŸ˜‘', 'ğŸ˜¶', 'ğŸ™„', 'ğŸ˜', 'ğŸ˜£', 'ğŸ˜¥',
        'ğŸ˜®', 'ğŸ¤', 'ğŸ˜¯', 'ğŸ˜ª', 'ğŸ˜«', 'ğŸ¥±', 'ğŸ˜´', 'ğŸ˜Œ', 'ğŸ˜›', 'ğŸ˜œ',
        'ğŸ˜', 'ğŸ¤¤', 'ğŸ˜’', 'ğŸ˜“', 'ğŸ˜”', 'ğŸ˜•', 'ğŸ™ƒ', 'ğŸ¤‘', 'ğŸ˜²', 'â˜¹ï¸',
        'ğŸ™', 'ğŸ˜–', 'ğŸ˜', 'ğŸ˜Ÿ', 'ğŸ˜¤', 'ğŸ˜¢', 'ğŸ˜­', 'ğŸ˜¦', 'ğŸ˜§', 'ğŸ˜¨',
        'ğŸ˜©', 'ğŸ¤¯', 'ğŸ˜¬', 'ğŸ˜°', 'ğŸ˜±', 'ğŸ¥µ', 'ğŸ¥¶', 'ğŸ˜³', 'ğŸ¤ª', 'ğŸ˜µ',
        'ğŸ˜¡', 'ğŸ˜ ', 'ğŸ¤¬', 'ğŸ˜·', 'ğŸ¤’', 'ğŸ¤•', 'ğŸ¤¢', 'ğŸ¤®', 'ğŸ¤§', 'ğŸ˜‡',
        'ğŸ¥³', 'ğŸ¥º', 'ğŸ¤ ', 'ğŸ¤¡', 'ğŸ¤¥', 'ğŸ¤«', 'ğŸ¤­', 'ğŸ§', 'ğŸ¤“',
        
        # æ‰‹åŠ¿ç¬¦å·
        'ğŸ‘', 'ğŸ‘', 'ğŸ‘Œ', 'âœŒï¸', 'ğŸ¤', 'ğŸ¤Ÿ', 'ğŸ¤˜', 'ğŸ¤™', 'ğŸ‘ˆ', 'ğŸ‘‰',
        'ğŸ‘†', 'ğŸ‘‡', 'â˜ï¸', 'âœ‹', 'ğŸ¤š', 'ğŸ–ï¸', 'ğŸ––', 'ğŸ‘‹', 'ğŸ¤', 'ğŸ™',
        'ğŸ’ª', 'ğŸ¦¾', 'ğŸ¦¿', 'ğŸ¦µ', 'ğŸ¦¶', 'ğŸ‘‚', 'ğŸ¦»', 'ğŸ‘ƒ', 'ğŸ§ ', 'ğŸ¦·',
        
        # å¿ƒå½¢å’Œæƒ…æ„Ÿç¬¦å·
        'â¤ï¸', 'ğŸ§¡', 'ğŸ’›', 'ğŸ’š', 'ğŸ’™', 'ğŸ’œ', 'ğŸ–¤', 'ğŸ¤', 'ğŸ¤', 'ğŸ’”',
        'â£ï¸', 'ğŸ’•', 'ğŸ’', 'ğŸ’“', 'ğŸ’—', 'ğŸ’–', 'ğŸ’˜', 'ğŸ’', 'ğŸ’Ÿ',
        
        # å…¶ä»–å¸¸è§ç¬¦å·
        'âœ¨', 'â­', 'ğŸŒŸ', 'ğŸ’«', 'âœ…', 'âŒ', 'âš ï¸', 'ğŸ”¥', 'ğŸ’¯', 'ğŸ‰',
        'ğŸŠ', 'ğŸˆ', 'ğŸ', 'ğŸ†', 'ğŸ¥‡', 'ğŸ¥ˆ', 'ğŸ¥‰', 'ğŸ‘', 'ğŸ™Œ',
    ]
    
    for emoji in common_emojis:
        try:
            # ç¼–ç  emoji å¹¶è·å– token IDs
            token_ids = tokenizer.encode(emoji, add_special_tokens=False)
            emoji_token_ids.update(token_ids)
        except Exception:
            continue
    
    # æ‰«æ tokenizer çš„è¯æ±‡è¡¨ï¼ŒæŸ¥æ‰¾å¯èƒ½çš„ emoji tokens
    # è¿™ä¸ªè¿‡ç¨‹å¯èƒ½è¾ƒæ…¢ï¼Œä½†æ›´å…¨é¢
    try:
        vocab = tokenizer.get_vocab()
        for token, token_id in vocab.items():
            # è§£ç  token çœ‹æ˜¯å¦åŒ…å« emoji
            try:
                decoded = tokenizer.decode([token_id], skip_special_tokens=True)
                if contains_emoji(decoded):
                    emoji_token_ids.add(token_id)
            except Exception:
                continue
    except Exception as e:
        print(f"è­¦å‘Š: æ— æ³•æ‰«æå®Œæ•´è¯æ±‡è¡¨: {e}")
    
    return emoji_token_ids


def create_emoji_suppression_bias(
    tokenizer, 
    bias_value: float = -100.0
) -> dict:
    """
    åˆ›å»ºç”¨äºæ¨ç†æ—¶æŠ‘åˆ¶ emoji çš„ logit bias å­—å…¸
    
    Args:
        tokenizer: Hugging Face tokenizer (PreTrainedTokenizer)
        bias_value: è´Ÿå€¼è¶Šå¤§ï¼ŒæŠ‘åˆ¶è¶Šå¼ºï¼ˆæ¨è -10.0 åˆ° -100.0ï¼‰
        
    Returns:
        dict: {token_id: bias_value} çš„å­—å…¸ï¼Œå¯ç›´æ¥ä¼ ç»™ model.generate()
    """
    emoji_token_ids = get_emoji_token_ids(tokenizer)
    
    # åˆ›å»º bias å­—å…¸
    logit_bias = {token_id: bias_value for token_id in emoji_token_ids}
    
    print(f"âœ“ åˆ›å»º emoji æŠ‘åˆ¶ bias: {len(logit_bias)} ä¸ª token å°†è¢«æŠ‘åˆ¶ (bias={bias_value})")
    
    return logit_bias


# æµ‹è¯•å‡½æ•°
if __name__ == "__main__":
    # æµ‹è¯• emoji æ£€æµ‹
    test_cases = [
        ("è¿™æ˜¯ä¸€æ®µæ­£å¸¸æ–‡æœ¬", False),
        ("æˆ‘çˆ±ä½ â¤ï¸", True),
        ("å¤ªæ£’äº†ï¼ğŸ˜Š", True),
        ("è°¢è°¢è°¢è°¢è°¢è°¢â¤ï¸â¤ï¸â¤ï¸â¤ï¸â¤ï¸", True),
        ("Hello world", False),
        ("å¥½çš„ğŸ‘Œ", True),
        ("ğŸ‰ğŸ‰ğŸ‰", True),
    ]
    
    print("=" * 80)
    print("æµ‹è¯• Emoji æ£€æµ‹:")
    print("=" * 80)
    
    for text, expected in test_cases:
        result = contains_emoji(text)
        status = "âœ“" if result == expected else "âœ—"
        print(f"{status} '{text}' -> {result} (æœŸæœ›: {expected})")
    
    print("\n" + "=" * 80)
    print("æµ‹è¯•æ ·æœ¬è¿‡æ»¤:")
    print("=" * 80)
    
    test_samples = [
        {'continuation': 'å¥½çš„ï¼Œæˆ‘çŸ¥é“äº†'},
        {'continuation': 'è°¢è°¢â¤ï¸'},
        {'continuation': 'å¤ªæ£’äº†ï¼ğŸ˜ŠğŸ˜ŠğŸ˜Š'},
        {'continuation': 'è¿™æ˜¯æ­£å¸¸æ–‡æœ¬'},
        {'continuation': 'æ˜ç™½äº†ğŸ‘Œ'},
    ]
    
    filtered, stats = filter_samples_with_emoji(test_samples)
    
    print(f"æ€»æ ·æœ¬æ•°: {stats['total_samples']}")
    print(f"åŒ…å« emoji æ ·æœ¬æ•°: {stats['emoji_samples']}")
    print(f"è¿‡æ»¤åæ ·æœ¬æ•°: {stats['filtered_samples']}")
    print(f"Emoji æ¯”ä¾‹: {stats['emoji_ratio']:.2%}")
    print(f"ä¿ç•™æ¯”ä¾‹: {stats['kept_ratio']:.2%}")
    
    print("\nè¿‡æ»¤åçš„æ ·æœ¬:")
    for i, sample in enumerate(filtered, 1):
        print(f"  {i}. {sample['continuation']}")
