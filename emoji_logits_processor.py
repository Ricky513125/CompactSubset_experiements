"""
Emoji Logits Processor
ç”¨äºæ¨ç†æ—¶æŠ‘åˆ¶ emoji token çš„ç”Ÿæˆ
"""
import torch
from typing import List, Optional
from transformers import LogitsProcessor


class EmojiSuppressionLogitsProcessor(LogitsProcessor):
    """
    è‡ªå®šä¹‰ LogitsProcessorï¼Œç”¨äºæŠ‘åˆ¶ emoji token çš„ç”Ÿæˆ
    é€šè¿‡å¯¹ emoji token çš„ logits æ·»åŠ è´Ÿåç½®æ¥é™ä½å…¶ç”Ÿæˆæ¦‚ç‡
    """
    
    def __init__(self, emoji_token_ids: List[int], bias_value: float = -100.0):
        """
        Args:
            emoji_token_ids: éœ€è¦æŠ‘åˆ¶çš„ emoji token ID åˆ—è¡¨
            bias_value: è´Ÿåç½®å€¼ï¼Œè¶Šå°æŠ‘åˆ¶è¶Šå¼ºï¼ˆæ¨è -10.0 åˆ° -100.0ï¼‰
                       -10.0: è½»åº¦æŠ‘åˆ¶
                       -50.0: ä¸­åº¦æŠ‘åˆ¶
                       -100.0: å¼ºåŠ›æŠ‘åˆ¶ï¼ˆå‡ ä¹ä¸å¯èƒ½ç”Ÿæˆï¼‰
        """
        self.emoji_token_ids = set(emoji_token_ids)
        self.bias_value = bias_value
        
        if not self.emoji_token_ids:
            print("è­¦å‘Š: emoji_token_ids ä¸ºç©ºï¼ŒEmoji æŠ‘åˆ¶å°†ä¸èµ·ä½œç”¨")

    
    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:
        """
        å¯¹ emoji tokens åº”ç”¨è´Ÿåç½®
        
        Args:
            input_ids: å½“å‰å·²ç”Ÿæˆçš„ token IDs [batch_size, seq_len]
            scores: ä¸‹ä¸€ä¸ª token çš„ logits [batch_size, vocab_size]
            
        Returns:
            ä¿®æ”¹åçš„ logits
        """
        # å¯¹æ‰€æœ‰ emoji token IDs åº”ç”¨è´Ÿåç½®
        for token_id in self.emoji_token_ids:
            if token_id < scores.shape[-1]:  # ç¡®ä¿ token_id åœ¨æœ‰æ•ˆèŒƒå›´å†…
                scores[:, token_id] += self.bias_value
        
        return scores


class AdaptiveEmojiSuppressionLogitsProcessor(LogitsProcessor):
    """
    è‡ªé€‚åº” Emoji æŠ‘åˆ¶ Logits Processor
    æ ¹æ®å·²ç”Ÿæˆå†…å®¹ä¸­ emoji çš„æ•°é‡åŠ¨æ€è°ƒæ•´æŠ‘åˆ¶å¼ºåº¦
    """
    
    def __init__(
        self, 
        emoji_token_ids: List[int], 
        base_bias: float = -50.0,
        max_bias: float = -200.0,
        emoji_threshold: int = 2
    ):
        """
        Args:
            emoji_token_ids: éœ€è¦æŠ‘åˆ¶çš„ emoji token ID åˆ—è¡¨
            base_bias: åŸºç¡€è´Ÿåç½®å€¼ï¼ˆå½“æœªæ£€æµ‹åˆ°emojiæ—¶ï¼‰
            max_bias: æœ€å¤§è´Ÿåç½®å€¼ï¼ˆå½“æ£€æµ‹åˆ°å¤šä¸ªemojiæ—¶ï¼‰
            emoji_threshold: emoji æ•°é‡é˜ˆå€¼ï¼Œè¶…è¿‡æ­¤å€¼å°†åº”ç”¨æœ€å¤§æŠ‘åˆ¶
        """
        self.emoji_token_ids = set(emoji_token_ids)
        self.base_bias = base_bias
        self.max_bias = max_bias
        self.emoji_threshold = emoji_threshold
        
        print(f"âœ“ è‡ªé€‚åº” Emoji Logits Processor å·²åˆå§‹åŒ–:")
        print(f"  - æŠ‘åˆ¶ {len(self.emoji_token_ids)} ä¸ª emoji tokens")
        print(f"  - åŸºç¡€ bias: {self.base_bias}")
        print(f"  - æœ€å¤§ bias: {self.max_bias}")
        print(f"  - Emoji é˜ˆå€¼: {self.emoji_threshold}")
    
    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:
        """
        æ ¹æ®å·²ç”Ÿæˆçš„ emoji æ•°é‡åŠ¨æ€è°ƒæ•´æŠ‘åˆ¶å¼ºåº¦
        """
        batch_size = input_ids.shape[0]
        
        for i in range(batch_size):
            # ç»Ÿè®¡å½“å‰åºåˆ—ä¸­å·²ç”Ÿæˆçš„ emoji æ•°é‡
            emoji_count = sum(1 for token_id in input_ids[i].tolist() if token_id in self.emoji_token_ids)
            
            # æ ¹æ® emoji æ•°é‡è®¡ç®— bias
            if emoji_count == 0:
                bias = self.base_bias
            elif emoji_count < self.emoji_threshold:
                # çº¿æ€§æ’å€¼
                ratio = emoji_count / self.emoji_threshold
                bias = self.base_bias + (self.max_bias - self.base_bias) * ratio
            else:
                bias = self.max_bias
            
            # åº”ç”¨ bias
            for token_id in self.emoji_token_ids:
                if token_id < scores.shape[-1]:
                    scores[i, token_id] += bias
        
        return scores


def create_emoji_suppression_processor(
    tokenizer, 
    mode: str = "normal",
    bias_value: float = -100.0,
    **kwargs
) -> Optional[LogitsProcessor]:
    """
    åˆ›å»º Emoji æŠ‘åˆ¶ Logits Processor
    
    Args:
        tokenizer: Hugging Face tokenizer
        mode: æŠ‘åˆ¶æ¨¡å¼
            - "normal": æ ‡å‡†æŠ‘åˆ¶ï¼ˆå›ºå®š biasï¼‰
            - "adaptive": è‡ªé€‚åº”æŠ‘åˆ¶ï¼ˆæ ¹æ®å·²ç”Ÿæˆ emoji æ•°é‡è°ƒæ•´ï¼‰
            - "off": å…³é—­æŠ‘åˆ¶
        bias_value: è´Ÿåç½®å€¼ï¼ˆä»…ç”¨äº normal æ¨¡å¼ï¼‰
        **kwargs: å…¶ä»–å‚æ•°ï¼ˆç”¨äº adaptive æ¨¡å¼ï¼‰
            - base_bias: åŸºç¡€ biasï¼ˆé»˜è®¤ -50.0ï¼‰
            - max_bias: æœ€å¤§ biasï¼ˆé»˜è®¤ -200.0ï¼‰
            - emoji_threshold: emoji æ•°é‡é˜ˆå€¼ï¼ˆé»˜è®¤ 2ï¼‰
    
    Returns:
        LogitsProcessor å®ä¾‹ï¼Œå¦‚æœ mode="off" åˆ™è¿”å› None
    """
    if mode == "off":
        print("â„¹ï¸  Emoji æŠ‘åˆ¶å·²å…³é—­")
        return None
    
    # å¯¼å…¥ emoji_filter è·å– token IDs
    try:
        from emoji_filter import get_emoji_token_ids
    except ImportError:
        print("âŒ é”™è¯¯: æ— æ³•å¯¼å…¥ emoji_filterï¼ŒEmoji æŠ‘åˆ¶åŠŸèƒ½ä¸å¯ç”¨")
        return None
    
    # âœ… å…³é”®ä¿®å¤ï¼šè·å– emoji token IDs
    # print("ğŸ” æ­£åœ¨æ‰«æ tokenizer è¯æ±‡è¡¨ä»¥è·å– emoji token IDs...")
    emoji_token_ids = get_emoji_token_ids(tokenizer)
    
    if not emoji_token_ids:
        print("âš ï¸  è­¦å‘Š: æœªæ‰¾åˆ°ä»»ä½• emoji tokensï¼ŒæŠ‘åˆ¶åŠŸèƒ½å°†ä¸èµ·ä½œç”¨")
        return None
    
    # print(f"âœ“ æ‰¾åˆ° {len(emoji_token_ids)} ä¸ª emoji tokens")
    # print("=" * 80)
    
    # æ ¹æ®æ¨¡å¼åˆ›å»º processor
    if mode == "normal":
        return EmojiSuppressionLogitsProcessor(
            emoji_token_ids=list(emoji_token_ids),
            bias_value=bias_value
        )
    elif mode == "adaptive":
        return AdaptiveEmojiSuppressionLogitsProcessor(
            emoji_token_ids=list(emoji_token_ids),
            base_bias=kwargs.get('base_bias', -50.0),
            max_bias=kwargs.get('max_bias', -200.0),
            emoji_threshold=kwargs.get('emoji_threshold', 2)
        )
    else:
        print(f"âŒ é”™è¯¯: æœªçŸ¥çš„æŠ‘åˆ¶æ¨¡å¼ '{mode}'")
        return None


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    print("="*80)
    print("Emoji Logits Processor æµ‹è¯•")
    print("="*80)
    
    # æ¨¡æ‹Ÿæµ‹è¯•
    emoji_token_ids = [100, 200, 300]  # å‡è®¾çš„ emoji token IDs
    
    print("\n1. æµ‹è¯•æ ‡å‡†æŠ‘åˆ¶æ¨¡å¼:")
    processor = EmojiSuppressionLogitsProcessor(emoji_token_ids, bias_value=-50.0)
    
    # åˆ›å»ºæ¨¡æ‹Ÿ logits
    import torch
    batch_size = 2
    vocab_size = 500
    seq_len = 10
    
    input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))
    scores = torch.randn(batch_size, vocab_size)
    
    print(f"  åŸå§‹ scores[0, {emoji_token_ids[0]}]: {scores[0, emoji_token_ids[0]]:.4f}")
    
    modified_scores = processor(input_ids, scores)
    
    print(f"  ä¿®æ”¹å scores[0, {emoji_token_ids[0]}]: {modified_scores[0, emoji_token_ids[0]]:.4f}")
    print(f"  å·®å€¼: {modified_scores[0, emoji_token_ids[0]] - scores[0, emoji_token_ids[0]]:.4f}")
    
    print("\n2. æµ‹è¯•è‡ªé€‚åº”æŠ‘åˆ¶æ¨¡å¼:")
    adaptive_processor = AdaptiveEmojiSuppressionLogitsProcessor(
        emoji_token_ids, 
        base_bias=-50.0,
        max_bias=-200.0,
        emoji_threshold=2
    )
    
    # åˆ›å»ºåŒ…å« emoji çš„è¾“å…¥åºåˆ—
    input_with_emoji = torch.tensor([[100, 50, 200, 80, 150, 200, 90, 110, 120, 130]])  # åŒ…å«3ä¸ªemoji
    scores_new = torch.randn(1, vocab_size)
    
    print(f"  è¾“å…¥åºåˆ—ä¸­åŒ…å« {sum(1 for id in input_with_emoji[0].tolist() if id in emoji_token_ids)} ä¸ª emoji")
    print(f"  åŸå§‹ scores[0, {emoji_token_ids[0]}]: {scores_new[0, emoji_token_ids[0]]:.4f}")
    
    modified_scores_new = adaptive_processor(input_with_emoji, scores_new)
    
    print(f"  ä¿®æ”¹å scores[0, {emoji_token_ids[0]}]: {modified_scores_new[0, emoji_token_ids[0]]:.4f}")
    print(f"  å·®å€¼: {modified_scores_new[0, emoji_token_ids[0]] - scores_new[0, emoji_token_ids[0]]:.4f}")
    
    print("\n" + "="*80)
    print("âœ“ æµ‹è¯•å®Œæˆ")
    print("="*80)
