# vLLM æ¨ç†ä½¿ç”¨æŒ‡å—

## ğŸ“Œ ä»€ä¹ˆæ˜¯ vLLMï¼Ÿ

vLLM æ˜¯ä¸€ä¸ªé«˜æ€§èƒ½çš„ LLM æ¨ç†å¼•æ“ï¼Œä¸“ä¸ºå¤§è§„æ¨¡ç”Ÿäº§ç¯å¢ƒè®¾è®¡ã€‚

### æ ¸å¿ƒä¼˜åŠ¿

| ç‰¹æ€§ | HuggingFace Transformers | vLLM | æå‡ |
|-----|-------------------------|------|------|
| **ååé‡** | 100 samples/min (8 GPU) | 1500-2000 samples/min (4 GPU TP) | **15-20x** |
| **å†…å­˜æ•ˆç‡** | æ¯ä¸ªè¯·æ±‚ç‹¬ç«‹ | PagedAttention å…±äº« | **èŠ‚çœ 50-70%** |
| **æ‰¹å¤„ç†** | æ‰‹åŠ¨ç®¡ç† | Continuous Batching è‡ªåŠ¨ | **æ™ºèƒ½ä¼˜åŒ–** |
| **GPU åˆ©ç”¨ç‡** | 30-50% | 80-95% | **2-3x** |

### å…³é”®æŠ€æœ¯

1. **PagedAttention**: ç±»ä¼¼æ“ä½œç³»ç»Ÿçš„è™šæ‹Ÿå†…å­˜ï¼ŒåŠ¨æ€åˆ†é… KV cache
2. **Continuous Batching**: åŠ¨æ€æ·»åŠ /ç§»é™¤è¯·æ±‚ï¼Œæ— éœ€ç­‰å¾…æ•´ä¸ª batch å®Œæˆ
3. **Tensor Parallelism**: å•ä¸ªæ¨¡å‹è·¨å¤š GPU å¹¶è¡Œ
4. **Optimized CUDA Kernels**: é’ˆå¯¹ç”Ÿæˆä»»åŠ¡ä¼˜åŒ–çš„åº•å±‚å®ç°

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£… vLLM

```bash
# å®‰è£… vLLM (éœ€è¦ CUDA 11.8+)
pip install vllm

# æˆ–è€…ä»æºç å®‰è£…ï¼ˆè·å–æœ€æ–°ç‰¹æ€§ï¼‰
pip install git+https://github.com/vllm-project/vllm.git
```

**ç³»ç»Ÿè¦æ±‚**:
- GPU: A100, H100, H200 æ¨èï¼ˆV100 ä¹Ÿæ”¯æŒä½†æ€§èƒ½è¾ƒä½ï¼‰
- CUDA: >= 11.8
- GPU Memory: è‡³å°‘ 16GB (8B æ¨¡å‹)
- Python: >= 3.8

### 2. åŸºæœ¬ä½¿ç”¨

#### æ–¹æ³• A: ä½¿ç”¨ç°æˆçš„æ¨ç†è„šæœ¬

```bash
# å• GPU æ¨ç†
python inference_vllm.py \
    --checkpoint_dir outputs/Chameleons_8B_context_sampled_seed42 \
    --dataset Chameleons \
    --ablation_config context_only \
    --num_samples 5 \
    --output_dir outputs/leaderboards/Chameleons_vllm

# å¤š GPU Tensor Parallel (4 GPU)
python inference_vllm.py \
    --checkpoint_dir outputs/DMSC_8B_one_per_user_0213 \
    --dataset DMSC \
    --ablation_config profile_and_history \
    --num_samples 5 \
    --output_dir outputs/leaderboards/DMSC_vllm_4gpu \
    --tensor_parallel_size 4 \
    --gpu_memory_utilization 0.9
```

python inference_vllm.py \
    --checkpoint_dir outputs/Chameleons_8B_context_full \
    --dataset Chameleons \
    --ablation_config context_only \
    --num_samples 5 \
    --output_dir outputs/leaderboards/Chameleons_vllm_8gpu \
    --tensor_parallel_size 8 \
    --gpu_memory_utilization 0.9

#### æ–¹æ³• B: ä½¿ç”¨è¾…åŠ©è„šæœ¬

```bash
bash inference_with_vllm.sh
```

ä¿®æ”¹è„šæœ¬ä¸­çš„ `run_vllm_inference` è°ƒç”¨æ¥é€‚é…ä½ çš„ä»»åŠ¡ã€‚

---

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

### å®é™…æµ‹è¯•ï¼ˆ8B æ¨¡å‹ï¼Œ1000 ä¸ªæ ·æœ¬ï¼‰

| æ–¹æ¡ˆ | GPU æ•°é‡ | æ—¶é—´ | ååé‡ | æ˜¾å­˜å ç”¨ |
|-----|---------|-----|--------|---------|
| **HF Transformers** (åŸå§‹) | 8 (DDP) | ~10 min | 100 samples/min | 80GB |
| **vLLM** (å•å¡) | 1 | ~2 min | 500 samples/min | 18GB |
| **vLLM** (TP) | 4 | ~0.5 min | 2000 samples/min | 4x15GB |

### æˆæœ¬åˆ†æ

å‡è®¾æ¨ç† 10,000 ä¸ªæ ·æœ¬ï¼š
- **HF Transformers (8 GPU)**: 100 åˆ†é’Ÿ = 800 GPU-åˆ†é’Ÿ
- **vLLM (1 GPU)**: 20 åˆ†é’Ÿ = 20 GPU-åˆ†é’Ÿ
- **vLLM (4 GPU TP)**: 5 åˆ†é’Ÿ = 20 GPU-åˆ†é’Ÿ

**èŠ‚çœæˆæœ¬**: **40å€** (ç›¸æ¯”åŸå§‹ 8 GPU DDP)

---

## ğŸ”§ é«˜çº§é…ç½®

### 1. Tensor Parallelism vs Data Parallelism

#### Tensor Parallelism (vLLM æ¨è)

```bash
# å•ä¸ªæ¨¡å‹åˆ†å¸ƒåœ¨ 4 å¼  GPU ä¸Š
python inference_vllm.py \
    --checkpoint_dir outputs/model \
    --tensor_parallel_size 4 \
    --dataset Chameleons \
    --ablation_config context_only \
    --output_dir outputs/leaderboards
```

**ä¼˜åŠ¿**:
- æ”¯æŒæ›´å¤§çš„ batch size
- å•ä¸ªè¯·æ±‚å»¶è¿Ÿä½
- é€‚åˆå¤§æ¨¡å‹ (30B+)

#### Data Parallelism (åŸå§‹æ–¹æ¡ˆ)

```bash
# 8 ä¸ªç‹¬ç«‹æ¨¡å‹å‰¯æœ¬
torchrun --nproc_per_node=8 inference_distributed.py \
    --checkpoint_dir outputs/model \
    ...
```

**åŠ£åŠ¿**:
- æ¯å¼ å¡åŠ è½½å®Œæ•´æ¨¡å‹ï¼Œå†…å­˜æµªè´¹
- éœ€è¦æ‰‹åŠ¨åˆ†é…æ•°æ®åˆ°å„ä¸ªè¿›ç¨‹
- æ‰¹å¤„ç†æ•ˆç‡ä½

### 2. å†…å­˜ä¼˜åŒ–

#### é€‰é¡¹ 1: é™ä½ GPU Memory Utilization

```bash
# é»˜è®¤ 0.9 (90%)ï¼Œå¦‚æœ OOM å¯ä»¥é™ä½åˆ° 0.7
python inference_vllm.py \
    --gpu_memory_utilization 0.7 \
    ...
```

#### é€‰é¡¹ 2: å‡å°‘ Max Model Length

```bash
# é»˜è®¤ 8192ï¼Œå¯ä»¥æ ¹æ®å®é™…éœ€è¦è°ƒæ•´
python inference_vllm.py \
    --max_model_len 4096 \
    ...
```

#### é€‰é¡¹ 3: ä½¿ç”¨ Quantization

```bash
# å®‰è£… AutoAWQ
pip install autoawq

# é‡åŒ–æ¨¡å‹ï¼ˆ4-bitï¼‰
python -m awq.entry --model_path outputs/model \
    --w_bit 4 --q_group_size 128 \
    --output_path outputs/model_awq

# ä½¿ç”¨é‡åŒ–æ¨¡å‹æ¨ç†
python inference_vllm.py \
    --checkpoint_dir outputs/model_awq \
    --quantization awq \
    ...
```

### 3. é‡‡æ ·å‚æ•°è°ƒä¼˜

```bash
python inference_vllm.py \
    --temperature 0.8 \      # é™ä½æ¸©åº¦æé«˜ç¡®å®šæ€§
    --top_p 0.95 \           # nucleus sampling
    --top_k 50 \             # top-k sampling
    --max_tokens 256 \       # é™åˆ¶ç”Ÿæˆé•¿åº¦
    --seed 42 \              # å›ºå®šéšæœºç§å­
    ...
```

---

## ğŸ” å¸¸è§é—®é¢˜

### Q1: vLLM vs HuggingFace æ¨ç†ç»“æœä¸ä¸€è‡´ï¼Ÿ

**åŸå› **: é‡‡æ ·ç®—æ³•å®ç°ç•¥æœ‰å·®å¼‚

**è§£å†³æ–¹æ¡ˆ**:
1. å›ºå®šéšæœºç§å­: `--seed 42`
2. ä½¿ç”¨ greedy decoding: `--temperature 0.0`
3. æˆ–æ¥å—ç•¥å¾®å·®å¼‚ï¼ˆé€šå¸¸ä¸å½±å“æœ€ç»ˆè¯„ä¼°ï¼‰

### Q2: OOM (Out of Memory) é”™è¯¯

**è§£å†³æ–¹æ¡ˆ**:
```bash
# 1. é™ä½ GPU å†…å­˜åˆ©ç”¨ç‡
--gpu_memory_utilization 0.7

# 2. å‡å°‘æœ€å¤§åºåˆ—é•¿åº¦
--max_model_len 4096

# 3. å¢åŠ  Tensor Parallel å¤§å°
--tensor_parallel_size 2  # æˆ– 4, 8
```

### Q3: vLLM ä¸æ”¯æŒæˆ‘çš„æ¨¡å‹ï¼Ÿ

**æ£€æŸ¥å…¼å®¹æ€§**:
```python
from vllm import LLM

# æ”¯æŒçš„æ¶æ„
supported_models = [
    "LlamaForCausalLM",
    "Qwen2ForCausalLM",
    "MistralForCausalLM",
    "GPTNeoXForCausalLM",
    # ... æ›´å¤š
]
```

å¦‚æœä¸æ”¯æŒï¼Œå¯ä»¥:
1. è½¬æ¢ä¸ºå…¼å®¹æ ¼å¼
2. ä½¿ç”¨ HuggingFace Transformers ä½œä¸º fallback
3. æäº¤ issue åˆ° vLLM GitHub

### Q4: å¦‚ä½•ç›‘æ§æ¨ç†æ€§èƒ½ï¼Ÿ

**æŸ¥çœ‹æ±‡æ€»ä¿¡æ¯**:
```bash
cat outputs/leaderboards/Chameleons_vllm/inference_summary.json
```

**å…³é”®æŒ‡æ ‡**:
- `throughput_samples_per_sec`: ååé‡
- `inference_time_seconds`: æ€»æ¨ç†æ—¶é—´
- `total_samples`: æ€»æ ·æœ¬æ•°

---

## ğŸ“‹ å®Œæ•´å‚æ•°åˆ—è¡¨

### inference_vllm.py å‚æ•°

```bash
# å¿…éœ€å‚æ•°
--checkpoint_dir PATH          # æ¨¡å‹ checkpoint è·¯å¾„
--dataset NAME                 # æ•°æ®é›†åç§° (Chameleons, DMSC, etc.)
--ablation_config CONFIG       # æ¶ˆèå®éªŒé…ç½®
--output_dir PATH              # è¾“å‡ºç›®å½•

# vLLM é…ç½®
--tensor_parallel_size N       # Tensor Parallel å¤§å° (é»˜è®¤: 1)
--gpu_memory_utilization F     # GPU å†…å­˜åˆ©ç”¨ç‡ (é»˜è®¤: 0.9)
--max_model_len N              # æœ€å¤§åºåˆ—é•¿åº¦ (é»˜è®¤: 8192)

# é‡‡æ ·å‚æ•°
--temperature F                # æ¸©åº¦ (é»˜è®¤: 1.0)
--top_p F                      # Top-p (é»˜è®¤: 0.9)
--top_k N                      # Top-k (é»˜è®¤: 50)
--max_tokens N                 # æœ€å¤§ç”Ÿæˆ token æ•° (é»˜è®¤: 512)
--seed N                       # éšæœºç§å­ (é»˜è®¤: 42)

# å…¶ä»–
--num_samples N                # æ¯ç”¨æˆ·æ ·æœ¬æ•° (é»˜è®¤: 5)
--scenario_path PATH           # åœºæ™¯æ•°æ®è·¯å¾„ï¼ˆå¯é€‰ï¼Œè‡ªåŠ¨æ¨æ–­ï¼‰
```

---

## ğŸ¯ æ¨èé…ç½®

### é…ç½® 1: å¿«é€Ÿæµ‹è¯•ï¼ˆå• GPUï¼‰

```bash
python inference_vllm.py \
    --checkpoint_dir outputs/model \
    --dataset Chameleons \
    --ablation_config context_only \
    --output_dir outputs/test \
    --tensor_parallel_size 1 \
    --num_samples 1
```

**é€‚ç”¨åœºæ™¯**: å¿«é€ŸéªŒè¯ï¼Œå°è§„æ¨¡æµ‹è¯•

### é…ç½® 2: ç”Ÿäº§æ¨ç†ï¼ˆ4 GPU TPï¼‰

```bash
python inference_vllm.py \
    --checkpoint_dir outputs/model \
    --dataset Chameleons \
    --ablation_config profile_and_context \
    --output_dir outputs/leaderboards/final \
    --tensor_parallel_size 4 \
    --gpu_memory_utilization 0.9 \
    --max_model_len 8192 \
    --num_samples 5
```

**é€‚ç”¨åœºæ™¯**: å¤§è§„æ¨¡æ¨ç†ï¼Œè¿½æ±‚æœ€é«˜ååé‡

### é…ç½® 3: é«˜è´¨é‡ç”Ÿæˆ

```bash
python inference_vllm.py \
    --checkpoint_dir outputs/model \
    --dataset DMSC \
    --ablation_config profile_and_history \
    --output_dir outputs/leaderboards/high_quality \
    --temperature 0.8 \
    --top_p 0.95 \
    --top_k 50 \
    --max_tokens 512 \
    --seed 42
```

**é€‚ç”¨åœºæ™¯**: è¿½æ±‚ç”Ÿæˆè´¨é‡ï¼Œå¯å¤ç°ç»“æœ

---

## ğŸ”„ è¿ç§»æŒ‡å—ï¼šä» HuggingFace åˆ° vLLM

### æ­¥éª¤ 1: ä¿å­˜æ¨¡å‹ä¸º HuggingFace æ ¼å¼

å¦‚æœä½ çš„æ¨¡å‹å·²ç»æ˜¯ HuggingFace æ ¼å¼ (é€šå¸¸æ˜¯)ï¼Œè·³è¿‡æ­¤æ­¥éª¤ã€‚

### æ­¥éª¤ 2: ä¿®æ”¹æ¨ç†è„šæœ¬

**åŸå§‹ (HuggingFace)**:
```python
model = AutoModelForCausalLM.from_pretrained(checkpoint_dir)
tokenizer = AutoTokenizer.from_pretrained(checkpoint_dir)

for prompt in prompts:
    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
    outputs = model.generate(**inputs, max_new_tokens=512)
    text = tokenizer.decode(outputs[0])
```

**æ–°ç‰ˆ (vLLM)**:
```python
from vllm import LLM, SamplingParams

llm = LLM(model=checkpoint_dir, tensor_parallel_size=4)
sampling_params = SamplingParams(temperature=1.0, max_tokens=512)

outputs = llm.generate(prompts, sampling_params)
texts = [output.outputs[0].text for output in outputs]
```

### æ­¥éª¤ 3: è¿è¡Œå¹¶æ¯”è¾ƒç»“æœ

```bash
# åŸå§‹æ–¹æ³•
torchrun --nproc_per_node=8 inference_distributed.py ...

# vLLM æ–¹æ³•
python inference_vllm.py --tensor_parallel_size 4 ...
```

### æ­¥éª¤ 4: éªŒè¯è¾“å‡ºä¸€è‡´æ€§

```python
# æ¯”è¾ƒä¸¤ç§æ–¹æ³•çš„è¾“å‡º
import json

with open('outputs/hf_results/user1.json') as f:
    hf_result = json.load(f)

with open('outputs/vllm_results/user1.json') as f:
    vllm_result = json.load(f)

# æ£€æŸ¥ç”Ÿæˆæ–‡æœ¬
print("HF:", hf_result['generated_samples'][0])
print("vLLM:", vllm_result['generated_samples'][0])
```

---

## ğŸ“š æ›´å¤šèµ„æº

- **vLLM GitHub**: https://github.com/vllm-project/vllm
- **vLLM æ–‡æ¡£**: https://docs.vllm.ai/
- **æ€§èƒ½åŸºå‡†**: https://github.com/vllm-project/vllm#performance
- **ç¤¾åŒºè®¨è®º**: https://github.com/vllm-project/vllm/discussions

---

## ğŸ’¡ æœ€ä½³å®è·µ

1. **ä¼˜å…ˆä½¿ç”¨ Tensor Parallelism**: æ¯” Data Parallelism æ›´é«˜æ•ˆ
2. **åˆç†è®¾ç½® gpu_memory_utilization**: 0.85-0.95 ä¹‹é—´è¾ƒä½³
3. **å›ºå®šéšæœºç§å­**: ä¿è¯ç»“æœå¯å¤ç°
4. **ç›‘æ§ GPU åˆ©ç”¨ç‡**: ä½¿ç”¨ `nvidia-smi dmon -s u` æŸ¥çœ‹
5. **æ‰¹é‡æ¨ç†**: ä¸€æ¬¡æ€§å‡†å¤‡æ‰€æœ‰ promptsï¼Œè®© vLLM è‡ªåŠ¨ä¼˜åŒ–æ‰¹å¤„ç†

---

## ğŸš¨ æ³¨æ„äº‹é¡¹

1. **é¦–æ¬¡åŠ è½½è¾ƒæ…¢**: vLLM ä¼šç¼–è¯‘ CUDA kernelsï¼Œé¦–æ¬¡è¿è¡Œéœ€è¦ 1-2 åˆ†é’Ÿ
2. **ä¸æ”¯æŒæµå¼ç”Ÿæˆå¯è§†åŒ–**: vLLM é’ˆå¯¹ååé‡ä¼˜åŒ–ï¼Œä¸é€‚åˆäº¤äº’å¼åœºæ™¯
3. **å†…å­˜é¢„åˆ†é…**: vLLM ä¼šé¢„å…ˆåˆ†é…å¤§é‡æ˜¾å­˜ï¼Œå¯èƒ½å¯¼è‡´å…¶ä»–ç¨‹åº OOM
4. **æ¨¡å‹å…¼å®¹æ€§**: æ£€æŸ¥ä½ çš„æ¨¡å‹æ¶æ„æ˜¯å¦è¢« vLLM æ”¯æŒ

---

**æ€»ç»“**: vLLM æ˜¯å¤§è§„æ¨¡æ¨ç†çš„æœ€ä½³é€‰æ‹©ï¼Œç›¸æ¯” HuggingFace Transformers å¯ä»¥èŠ‚çœ **10-40å€** çš„æ—¶é—´å’Œæˆæœ¬ï¼
